{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALK4D5kjE2lj",
        "outputId": "58da7077-8ded-4738-e695-a04bd6faea41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts (−1, 0, +1): {-1: 718, 0: 239, 1: 791}\n",
            "Shapes: X_tr (1346, 52, 5) X_va (123, 52, 5) X_te (123, 52, 5)\n",
            "Epoch 1/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.4472 - loss: 1.0564 - val_accuracy: 0.4715 - val_loss: 0.9924\n",
            "Epoch 2/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.4775 - loss: 0.9836 - val_accuracy: 0.5285 - val_loss: 0.9848\n",
            "Epoch 3/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4698 - loss: 0.9794 - val_accuracy: 0.5285 - val_loss: 0.9799\n",
            "Epoch 4/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4744 - loss: 0.9779 - val_accuracy: 0.5285 - val_loss: 0.9756\n",
            "Epoch 5/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4833 - loss: 0.9775 - val_accuracy: 0.5447 - val_loss: 0.9727\n",
            "Epoch 6/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4792 - loss: 0.9760 - val_accuracy: 0.5366 - val_loss: 0.9690\n",
            "Epoch 7/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4882 - loss: 0.9752 - val_accuracy: 0.5285 - val_loss: 0.9660\n",
            "Epoch 8/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4798 - loss: 0.9744 - val_accuracy: 0.5203 - val_loss: 0.9648\n",
            "Epoch 9/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4820 - loss: 0.9732 - val_accuracy: 0.5041 - val_loss: 0.9645\n",
            "Epoch 10/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4848 - loss: 0.9723 - val_accuracy: 0.4878 - val_loss: 0.9634\n",
            "Epoch 11/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4737 - loss: 0.9715 - val_accuracy: 0.4878 - val_loss: 0.9631\n",
            "Epoch 12/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4807 - loss: 0.9707 - val_accuracy: 0.4878 - val_loss: 0.9646\n",
            "Epoch 13/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4892 - loss: 0.9704 - val_accuracy: 0.4878 - val_loss: 0.9650\n",
            "Epoch 14/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.4933 - loss: 0.9693 - val_accuracy: 0.4959 - val_loss: 0.9662\n",
            "Epoch 15/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4909 - loss: 0.9696 - val_accuracy: 0.4878 - val_loss: 0.9675\n",
            "Epoch 16/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4928 - loss: 0.9679 - val_accuracy: 0.4878 - val_loss: 0.9706\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\n",
            "Test accuracy: 0.45528455284552843\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.471     0.571     0.516        56\n",
            "           0      0.000     0.000     0.000        17\n",
            "           1      0.436     0.480     0.457        50\n",
            "\n",
            "    accuracy                          0.455       123\n",
            "   macro avg      0.302     0.350     0.324       123\n",
            "weighted avg      0.392     0.455     0.421       123\n",
            "\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred classes in order [-1,0,1]):\n",
            " [[32  0 24]\n",
            " [10  0  7]\n",
            " [26  0 24]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Minimal LSTM 3-class classifier for weekly returns (−1 / 0 / +1)\n",
        "# ---------------------------------------------------------------\n",
        "# Requirements:\n",
        "# pip install numpy pandas scikit-learn tensorflow==2.* (or compatible)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# -------------------\n",
        "# 1) Load your data\n",
        "# -------------------\n",
        "# Assume you already have a DataFrame `df` indexed by weekly dates like the sample you posted.\n",
        "# Columns used: 'Last Price','ret_1','p_state0_h1','p_state1_h1','p_state2_h1'\n",
        "# If your DataFrame variable is named differently, just replace `df` below.\n",
        "\n",
        "df = pd.read_csv(\"df.csv\")  # <-- your DataFrame here\n",
        "\n",
        "# Ensure time order\n",
        "df = df.sort_index()\n",
        "\n",
        "# -------------------\n",
        "# 2) Build/verify target\n",
        "# -------------------\n",
        "# Target definition:\n",
        "# next_ret = ret_1 shifted by -1 (next period return)\n",
        "# if next_ret > +0.5%  ->  +1\n",
        "# if next_ret < -0.5%  ->  -1\n",
        "# else                 ->   0\n",
        "\n",
        "NEUTRAL_BAND = 0.007  # 0.5%\n",
        "if \"target\" not in df.columns:\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    tgt = np.where(next_ret > NEUTRAL_BAND, 1,\n",
        "          np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "    df[\"target\"] = tgt\n",
        "\n",
        "# Drop the last row (no label after shift) and any rows with NaNs in features/label\n",
        "feature_cols = [ \"Last Price\", \"ret_1\", \"p_state0_h1\", \"p_state1_h1\", \"p_state2_h1\"]\n",
        "df = df.dropna(subset=feature_cols + [\"target\"]).copy()\n",
        "\n",
        "# Optional: quick class distribution check\n",
        "print(\"Class counts (−1, 0, +1):\", df[\"target\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# -------------------\n",
        "# 3) Train/Val/Test split (chronological 80/10/10)\n",
        "# -------------------\n",
        "n = len(df)\n",
        "i_train_end = int(0.8 * n)\n",
        "i_val_end   = int(0.9 * n)\n",
        "\n",
        "df_train = df.iloc[:i_train_end]\n",
        "df_val   = df.iloc[i_train_end:i_val_end]\n",
        "df_test  = df.iloc[i_val_end:]\n",
        "\n",
        "# -------------------\n",
        "# 4) Make sequences\n",
        "# -------------------\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    \"\"\"Return X (num_seq, lookback, num_feat), y (num_seq,) aligned for next-step label already in block_df['target'].\"\"\"\n",
        "    X_list, y_list = [], []\n",
        "    X_src = block_df[features].values\n",
        "    y_src = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X_src[t-lookback:t, :])\n",
        "        y_list.append(y_src[t])  # label for the last timestep\n",
        "    X = np.array(X_list, dtype=np.float32)\n",
        "    y = np.array(y_list, dtype=np.int64)\n",
        "    return X, y\n",
        "\n",
        "LOOKBACK = 52  # weeks; change later as you like\n",
        "\n",
        "# Fit scaler on TRAIN ONLY, then transform splits BEFORE building sequences (no leakage)\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_train[feature_cols])\n",
        "\n",
        "df_train_s = df_train.copy()\n",
        "df_val_s   = df_val.copy()\n",
        "df_test_s  = df_test.copy()\n",
        "\n",
        "df_train_s[feature_cols] = scaler.transform(df_train[feature_cols])\n",
        "df_val_s[feature_cols]   = scaler.transform(df_val[feature_cols])\n",
        "df_test_s[feature_cols]  = scaler.transform(df_test[feature_cols])\n",
        "\n",
        "X_tr, y_tr = make_sequences(df_train_s, feature_cols, LOOKBACK)\n",
        "X_va, y_va = make_sequences(df_val_s,   feature_cols, LOOKBACK)\n",
        "X_te, y_te = make_sequences(df_test_s,  feature_cols, LOOKBACK)\n",
        "\n",
        "print(\"Shapes:\", \"X_tr\", X_tr.shape, \"X_va\", X_va.shape, \"X_te\", X_te.shape)\n",
        "\n",
        "# -------------------\n",
        "# 5) Map targets {−1,0,1} -> {0,1,2} for SparseCategoricalCrossentropy\n",
        "# -------------------\n",
        "to_idx = {-1: 0, 0: 1, 1: 2}\n",
        "y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "y_te_i = np.vectorize(to_idx.get)(y_te)\n",
        "\n",
        "# -------------------\n",
        "# 6) Simple LSTM model\n",
        "# -------------------\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(LOOKBACK, len(feature_cols))),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_tr, y_tr_i,\n",
        "    validation_data=(X_va, y_va_i),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------\n",
        "# 7) Evaluate\n",
        "# -------------------\n",
        "probs = model.predict(X_te)\n",
        "y_hat_i = probs.argmax(axis=1)\n",
        "\n",
        "# Map back {0,1,2} -> {−1,0,1}\n",
        "from_idx = {0: -1, 1: 0, 2: 1}\n",
        "y_hat = np.vectorize(from_idx.get)(y_hat_i)\n",
        "y_true = y_te  # already in {−1,0,1}\n",
        "\n",
        "print(\"\\nTest accuracy:\", accuracy_score(y_true, y_hat))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_true, y_hat, digits=3))\n",
        "print(\"\\nConfusion matrix (rows=true, cols=pred classes in order [-1,0,1]):\\n\",\n",
        "      confusion_matrix(y_true, y_hat, labels=[-1,0,1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['target'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LiJhoYWHsvn",
        "outputId": "058d62e8-13c4-4a61-8014-d8b038fd6881"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target\n",
            " 1    791\n",
            "-1    718\n",
            " 0    239\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Use last row of features only (no sequence)\n",
        "X_last = df[[\"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\"]].iloc[:-1].values\n",
        "y_last = df[\"target\"].iloc[1:].values   # shift to next return\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000).fit(X_last, y_last)\n",
        "print(\"LogReg synthetic acc:\", logreg.score(X_last, y_last))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuJz5kTcQjEM",
        "outputId": "6a5c93d9-1a0c-4ed4-f8a4-4eae0da2acd7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg synthetic acc: 0.4642243846594161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with class weights (and optional oversampling for class 0)\n",
        "# ---------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# ====== 0) CONFIG ======\n",
        "FEATURES     = [ \"Last Price\", \"p_state0_h1\"]  #, \"p_state0_h1\", \"p_state1_h1\", \"p_state2_h1\"]\n",
        "LOOKBACK     = 52\n",
        "NEUTRAL_BAND = 0.005\n",
        "EPOCHS       = 60\n",
        "BATCH_SIZE   = 32\n",
        "USE_OVERSAMPLING = False   # set True to try oversampling class 0 sequences\n",
        "\n",
        "# ====== 1) PREP ======\n",
        "df = df.sort_index().copy()\n",
        "\n",
        "if \"target\" not in df.columns:\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    df[\"target\"] = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "                    np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "\n",
        "df = df.dropna(subset=FEATURES + [\"target\"]).copy()\n",
        "\n",
        "print(\"Class counts in FULL data:\", df[\"target\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# Chronological 80/10/10 split\n",
        "n = len(df)\n",
        "i_train_end = int(0.9 * n)\n",
        "i_val_end   = int(0.95 * n)\n",
        "df_train, df_val, df_test = df.iloc[:i_train_end], df.iloc[i_train_end:i_val_end], df.iloc[i_val_end:]\n",
        "\n",
        "# Scale on TRAIN only\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_train[FEATURES])\n",
        "for d in (df_train, df_val, df_test):\n",
        "    d[FEATURES] = scaler.transform(d[FEATURES])\n",
        "\n",
        "# ====== 2) SEQUENCING ======\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    X_list, y_list = [], []\n",
        "    X_src = block_df[features].values\n",
        "    y_src = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X_src[t-lookback:t, :])\n",
        "        y_list.append(y_src[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "X_tr, y_tr = make_sequences(df_train, FEATURES, LOOKBACK)\n",
        "X_va, y_va = make_sequences(df_val,   FEATURES, LOOKBACK)\n",
        "X_te, y_te = make_sequences(df_test,  FEATURES, LOOKBACK)\n",
        "\n",
        "print(\"Shapes:\", \"X_tr\", X_tr.shape, \"X_va\", X_va.shape, \"X_te\", X_te.shape)\n",
        "\n",
        "# Map {-1,0,1} -> {0,1,2}\n",
        "to_idx   = {-1:0, 0:1, 1:2}\n",
        "from_idx = {0:-1, 1:0, 2:1}\n",
        "y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "y_te_i = np.vectorize(to_idx.get)(y_te)\n",
        "\n",
        "# ====== 3) (Optional) Oversampling of class 0 sequences in TRAIN ======\n",
        "if USE_OVERSAMPLING:\n",
        "    # gather indices by class\n",
        "    idx_m1 = np.where(y_tr_i == 0)[0]\n",
        "    idx_0  = np.where(y_tr_i == 1)[0]\n",
        "    idx_p1 = np.where(y_tr_i == 2)[0]\n",
        "    # target count ~ min(max of the other classes, 0.8 * median) to avoid huge duplication\n",
        "    target_0 = max(len(idx_m1), len(idx_p1))\n",
        "    if len(idx_0) > 0 and target_0 > len(idx_0):\n",
        "        add = np.random.choice(idx_0, size=target_0 - len(idx_0), replace=True)\n",
        "        X_tr = np.concatenate([X_tr, X_tr[add]], axis=0)\n",
        "        y_tr_i = np.concatenate([y_tr_i, y_tr_i[add]], axis=0)\n",
        "        # shuffle after oversampling\n",
        "        perm = np.random.permutation(len(y_tr_i))\n",
        "        X_tr, y_tr_i = X_tr[perm], y_tr_i[perm]\n",
        "        print(f\"Oversampled class 0 sequences from {len(idx_0)} to {target_0}\")\n",
        "\n",
        "# ====== 4) Class weights (computed on TRAIN after optional oversampling) ======\n",
        "classes = np.array([0,1,2])\n",
        "class_weights_vec = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr_i)\n",
        "class_weights = {int(c): float(w) for c, w in zip(classes, class_weights_vec)}\n",
        "print(\"Class weights (for indices {0,1,2} ≡ {-1,0,1}):\", class_weights)\n",
        "\n",
        "# ====== 5) Model ======\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(LOOKBACK, len(FEATURES))),\n",
        "    layers.LSTM(32, dropout=0.1, recurrent_dropout=0.0),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_tr, y_tr_i,\n",
        "    validation_data=(X_va, y_va_i),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[es],\n",
        "    verbose=1,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# ====== 6) Evaluate ======\n",
        "probs   = model.predict(X_te)\n",
        "y_hat_i = probs.argmax(axis=1)\n",
        "y_hat   = np.vectorize(from_idx.get)(y_hat_i)\n",
        "\n",
        "print(\"\\nTest accuracy:\", accuracy_score(y_te, y_hat))\n",
        "print(\"Macro F1:\", f1_score(y_te, y_hat, labels=[-1,0,1], average=\"macro\"))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_te, y_hat, digits=3))\n",
        "print(\"Confusion matrix (rows=true, cols=pred [-1,0,1]):\\n\",\n",
        "      confusion_matrix(y_te, y_hat, labels=[-1,0,1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAudDaQUH7_2",
        "outputId": "3a29d0d1-06c3-4dd2-fe21-5f9eeedf9e69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts in FULL data: {-1: 718, 0: 239, 1: 791}\n",
            "Shapes: X_tr (1521, 52, 2) X_va (35, 52, 2) X_te (36, 52, 2)\n",
            "Class weights (for indices {0,1,2} ≡ {-1,0,1}): {0: 0.8203883495145631, 1: 2.485294117647059, 2: 0.7253218884120172}\n",
            "Epoch 1/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3311192286.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-3311192286.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-3311192286.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.3425 - loss: 1.1218 - val_accuracy: 0.1143 - val_loss: 1.2054\n",
            "Epoch 2/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3278 - loss: 1.1154 - val_accuracy: 0.1143 - val_loss: 1.1771\n",
            "Epoch 3/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3587 - loss: 1.1136 - val_accuracy: 0.1143 - val_loss: 1.1804\n",
            "Epoch 4/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3430 - loss: 1.1093 - val_accuracy: 0.1143 - val_loss: 1.1395\n",
            "Epoch 5/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3514 - loss: 1.1173 - val_accuracy: 0.1143 - val_loss: 1.1170\n",
            "Epoch 6/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3719 - loss: 1.1180 - val_accuracy: 0.1143 - val_loss: 1.1253\n",
            "Epoch 7/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.3636 - loss: 1.1122 - val_accuracy: 0.1143 - val_loss: 1.1296\n",
            "Epoch 8/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3743 - loss: 1.1137 - val_accuracy: 0.3143 - val_loss: 1.1200\n",
            "Epoch 9/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.3861 - loss: 1.1145 - val_accuracy: 0.4286 - val_loss: 1.1067\n",
            "Epoch 10/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4015 - loss: 1.1131 - val_accuracy: 0.4571 - val_loss: 1.0994\n",
            "Epoch 11/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3909 - loss: 1.1136 - val_accuracy: 0.4571 - val_loss: 1.1055\n",
            "Epoch 12/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3744 - loss: 1.1122 - val_accuracy: 0.4571 - val_loss: 1.0878\n",
            "Epoch 13/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4095 - loss: 1.1110 - val_accuracy: 0.4000 - val_loss: 1.1287\n",
            "Epoch 14/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3917 - loss: 1.1116 - val_accuracy: 0.3143 - val_loss: 1.1245\n",
            "Epoch 15/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4081 - loss: 1.1111 - val_accuracy: 0.3143 - val_loss: 1.1211\n",
            "Epoch 16/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3955 - loss: 1.1116 - val_accuracy: 0.2857 - val_loss: 1.1188\n",
            "Epoch 17/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.3948 - loss: 1.1106 - val_accuracy: 0.3143 - val_loss: 1.1236\n",
            "Epoch 18/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4084 - loss: 1.1064 - val_accuracy: 0.3714 - val_loss: 1.1211\n",
            "Epoch 19/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.3982 - loss: 1.1095 - val_accuracy: 0.2857 - val_loss: 1.1552\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
            "\n",
            "Test accuracy: 0.4166666666666667\n",
            "Macro F1: 0.3406015037593985\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.500     0.214     0.300        14\n",
            "           0      0.125     0.167     0.143         6\n",
            "           1      0.500     0.688     0.579        16\n",
            "\n",
            "    accuracy                          0.417        36\n",
            "   macro avg      0.375     0.356     0.341        36\n",
            "weighted avg      0.438     0.417     0.398        36\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[ 3  4  7]\n",
            " [ 1  1  4]\n",
            " [ 2  3 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Feature-subset search (require predicting ALL 3 classes on TEST)\n",
        "# ==============================================================\n",
        "\n",
        "import itertools\n",
        "from collections import OrderedDict\n",
        "\n",
        "REQUIRE_ALL_CLASSES = True\n",
        "ALL_CLASSES_SET = {-1, 0, 1}\n",
        "\n",
        "CANDIDATE_FEATURES = [\"Last Price\", \"ret_1\", \"p_state0_h1\", \"p_state1_h1\", \"p_state2_h1\"]\n",
        "\n",
        "# Ensure target exists and data sorted\n",
        "df = df.sort_index().copy()\n",
        "if \"target\" not in df.columns:\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    df[\"target\"] = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "                    np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "\n",
        "# Keep rows usable for all features (safe superset)\n",
        "df = df.dropna(subset=CANDIDATE_FEATURES + [\"target\"]).copy()\n",
        "\n",
        "# Chronological 80/10/10 split (fixed for all subsets)\n",
        "n = len(df)\n",
        "i_train_end = int(0.8 * n)\n",
        "i_val_end   = int(0.9 * n)\n",
        "df_train, df_val, df_test = df.iloc[:i_train_end].copy(), df.iloc[i_train_end:i_val_end].copy(), df.iloc[i_val_end:].copy()\n",
        "\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    X_list, y_list = [], []\n",
        "    X_src = block_df[features].values\n",
        "    y_src = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X_src[t-lookback:t, :])\n",
        "        y_list.append(y_src[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "to_idx   = {-1:0, 0:1, 1:2}\n",
        "from_idx = {0:-1, 1:0, 2:1}\n",
        "\n",
        "def build_model(input_dim):\n",
        "    tf.random.set_seed(42)\n",
        "    np.random.seed(42)\n",
        "    m = models.Sequential([\n",
        "        layers.Input(shape=(LOOKBACK, input_dim)),\n",
        "        layers.LSTM(32, dropout=0.1, recurrent_dropout=0.0),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(3, activation=\"softmax\")\n",
        "    ])\n",
        "    m.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return m\n",
        "\n",
        "def fit_eval_for_features(feat_subset):\n",
        "    # 1) Scale per subset (fit on TRAIN only)\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaler.fit(df_train[feat_subset])\n",
        "    dtr = df_train.copy(); dva = df_val.copy(); dte = df_test.copy()\n",
        "    dtr[feat_subset] = scaler.transform(dtr[feat_subset])\n",
        "    dva[feat_subset] = scaler.transform(dva[feat_subset])\n",
        "    dte[feat_subset] = scaler.transform(dte[feat_subset])\n",
        "\n",
        "    # 2) Make sequences\n",
        "    X_tr, y_tr = make_sequences(dtr, feat_subset, LOOKBACK)\n",
        "    X_va, y_va = make_sequences(dva, feat_subset, LOOKBACK)\n",
        "    X_te, y_te = make_sequences(dte, feat_subset, LOOKBACK)\n",
        "\n",
        "    # Map labels {-1,0,1} -> {0,1,2}\n",
        "    y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "    y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "    y_te_i = np.vectorize(to_idx.get)(y_te)\n",
        "\n",
        "    # 3) Optional oversampling (class 0 index = 1)\n",
        "    X_train_fin, y_train_fin = X_tr, y_tr_i\n",
        "    if USE_OVERSAMPLING:\n",
        "        idx_m1 = np.where(y_tr_i == 0)[0]\n",
        "        idx_0  = np.where(y_tr_i == 1)[0]\n",
        "        idx_p1 = np.where(y_tr_i == 2)[0]\n",
        "        target_0 = max(len(idx_m1), len(idx_p1))\n",
        "        if len(idx_0) > 0 and target_0 > len(idx_0):\n",
        "            add = np.random.choice(idx_0, size=target_0 - len(idx_0), replace=True)\n",
        "            X_train_fin = np.concatenate([X_tr, X_tr[add]], axis=0)\n",
        "            y_train_fin = np.concatenate([y_tr_i, y_tr_i[add]], axis=0)\n",
        "            perm = np.random.permutation(len(y_train_fin))\n",
        "            X_train_fin, y_train_fin = X_train_fin[perm], y_train_fin[perm]\n",
        "\n",
        "    # 4) Class weights from TRAIN\n",
        "    classes = np.array([0,1,2])\n",
        "    cw_vec = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_fin)\n",
        "    class_weights = {int(c): float(w) for c, w in zip(classes, cw_vec)}\n",
        "\n",
        "    # 5) Build, fit\n",
        "    model = build_model(input_dim=len(feat_subset))\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
        "    model.fit(\n",
        "        X_train_fin, y_train_fin,\n",
        "        validation_data=(X_va, y_va_i),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[es],\n",
        "        verbose=0,\n",
        "        class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    # 6) Evaluate on TEST\n",
        "    probs   = model.predict(X_te, verbose=0)\n",
        "    y_hat_i = probs.argmax(axis=1)\n",
        "    y_hat   = np.vectorize(from_idx.get)(y_hat_i)\n",
        "\n",
        "    acc  = accuracy_score(y_te, y_hat)\n",
        "    f1m  = f1_score(y_te, y_hat, labels=[-1,0,1], average=\"macro\")\n",
        "    rep  = classification_report(y_te, y_hat, digits=3)\n",
        "    cm   = confusion_matrix(y_te, y_hat, labels=[-1,0,1])\n",
        "\n",
        "    pred_classes = set(np.unique(y_hat))\n",
        "    passes = (pred_classes == ALL_CLASSES_SET) if REQUIRE_ALL_CLASSES else True\n",
        "\n",
        "    return {\n",
        "        \"features\": tuple(feat_subset),\n",
        "        \"n_features\": len(feat_subset),\n",
        "        \"test_acc\": acc,\n",
        "        \"macro_f1\": f1m,\n",
        "        \"report\": rep,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"pred_classes\": pred_classes,\n",
        "        \"passes_all_classes\": passes\n",
        "    }\n",
        "\n",
        "# Iterate all non-empty subsets\n",
        "all_subsets = []\n",
        "for r in range(1, len(CANDIDATE_FEATURES)+1):\n",
        "    all_subsets += list(itertools.combinations(CANDIDATE_FEATURES, r))\n",
        "\n",
        "print(f\"Evaluating {len(all_subsets)} feature combinations...\")\n",
        "results, skipped = [], 0\n",
        "for subset in all_subsets:\n",
        "    res = fit_eval_for_features(list(subset))\n",
        "    if REQUIRE_ALL_CLASSES and not res[\"passes_all_classes\"]:\n",
        "        skipped += 1\n",
        "        missing = ALL_CLASSES_SET - res[\"pred_classes\"]\n",
        "        print(f\"{subset} -> SKIPPED: missing classes {sorted(list(missing))}  \"\n",
        "              f\"(predicted: {sorted(list(res['pred_classes']))})\")\n",
        "        continue\n",
        "    results.append(res)\n",
        "    print(f\"{subset} -> acc={res['test_acc']:.3f}, f1={res['macro_f1']:.3f} | \"\n",
        "          f\"pred_classes={sorted(list(res['pred_classes']))}\")\n",
        "\n",
        "print(f\"\\nCombinations passing the 'all classes predicted' constraint: {len(results)} \"\n",
        "      f\"(skipped {skipped})\")\n",
        "\n",
        "if len(results) == 0:\n",
        "    print(\"\\nNo combinations satisfied the constraint. \"\n",
        "          \"Consider widening NEUTRAL_BAND, enabling USE_OVERSAMPLING, \"\n",
        "          \"or increasing EPOCHS/hidden units.\")\n",
        "else:\n",
        "    results_sorted = sorted(results, key=lambda d: (d[\"test_acc\"], d[\"macro_f1\"]), reverse=True)\n",
        "    best = results_sorted[0]\n",
        "    print(\"\\n================ BEST COMBINATION (with all classes predicted) ================\")\n",
        "    print(\"Features:\", best[\"features\"])\n",
        "    print(f\"Test accuracy: {best['test_acc']:.3f}\")\n",
        "    print(f\"Macro F1: {best['macro_f1']:.3f}\")\n",
        "    print(\"Predicted classes:\", sorted(list(best[\"pred_classes\"])))\n",
        "    print(\"\\nClassification report:\\n\", best[\"report\"])\n",
        "    print(\"Confusion matrix (rows=true, cols=pred [-1,0,1]):\\n\", best[\"confusion_matrix\"])\n",
        "\n",
        "    # Optional: show top-10 table\n",
        "    tbl = pd.DataFrame([{\n",
        "        \"features\": \", \".join(r[\"features\"]),\n",
        "        \"n_features\": r[\"n_features\"],\n",
        "        \"test_acc\": r[\"test_acc\"],\n",
        "        \"macro_f1\": r[\"macro_f1\"],\n",
        "        \"predicted_classes\": \"\".join(str(c) for c in sorted(list(r[\"pred_classes\"])))\n",
        "    } for r in results_sorted])\n",
        "    print(\"\\nTop 10 combinations (constraint-satisfying):\")\n",
        "    print(tbl.head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2UsxI32M0Sc",
        "outputId": "9a126872-9352-48c6-d35a-603885cf12f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 31 feature combinations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Last Price',) -> SKIPPED: missing classes [0]  (predicted: [np.int64(-1), np.int64(1)])\n",
            "('ret_1',) -> acc=0.431, f1=0.357 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('p_state0_h1',) -> acc=0.268, f1=0.264 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('p_state1_h1',) -> acc=0.309, f1=0.290 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('p_state2_h1',) -> SKIPPED: missing classes [-1, 1]  (predicted: [np.int64(0)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Last Price', 'ret_1') -> SKIPPED: missing classes [0]  (predicted: [np.int64(-1), np.int64(1)])\n",
            "('Last Price', 'p_state0_h1') -> acc=0.309, f1=0.309 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state1_h1') -> acc=0.463, f1=0.360 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state2_h1') -> acc=0.260, f1=0.260 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('ret_1', 'p_state0_h1') -> acc=0.488, f1=0.323 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('ret_1', 'p_state1_h1') -> acc=0.317, f1=0.292 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ret_1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n",
            "('p_state0_h1', 'p_state1_h1') -> acc=0.260, f1=0.235 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('p_state0_h1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('p_state1_h1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n",
            "('Last Price', 'ret_1', 'p_state0_h1') -> acc=0.333, f1=0.320 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'ret_1', 'p_state1_h1') -> acc=0.463, f1=0.333 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'ret_1', 'p_state2_h1') -> acc=0.350, f1=0.333 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state0_h1', 'p_state1_h1') -> acc=0.301, f1=0.287 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state0_h1', 'p_state2_h1') -> acc=0.398, f1=0.383 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state1_h1', 'p_state2_h1') -> acc=0.472, f1=0.401 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('ret_1', 'p_state0_h1', 'p_state1_h1') -> acc=0.268, f1=0.253 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('ret_1', 'p_state0_h1', 'p_state2_h1') -> acc=0.268, f1=0.251 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ret_1', 'p_state1_h1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('p_state0_h1', 'p_state1_h1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n",
            "('Last Price', 'ret_1', 'p_state0_h1', 'p_state1_h1') -> acc=0.504, f1=0.381 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'ret_1', 'p_state0_h1', 'p_state2_h1') -> acc=0.431, f1=0.392 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'ret_1', 'p_state1_h1', 'p_state2_h1') -> acc=0.382, f1=0.359 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state0_h1', 'p_state1_h1', 'p_state2_h1') -> acc=0.447, f1=0.321 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ret_1', 'p_state0_h1', 'p_state1_h1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n",
            "('Last Price', 'ret_1', 'p_state0_h1', 'p_state1_h1', 'p_state2_h1') -> acc=0.350, f1=0.338 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "\n",
            "Combinations passing the 'all classes predicted' constraint: 22 (skipped 9)\n",
            "\n",
            "================ BEST COMBINATION (with all classes predicted) ================\n",
            "Features: ('Last Price', 'ret_1', 'p_state0_h1', 'p_state1_h1')\n",
            "Test accuracy: 0.504\n",
            "Macro F1: 0.381\n",
            "Predicted classes: [np.int64(-1), np.int64(0), np.int64(1)]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.500     0.750     0.600        56\n",
            "           0      0.200     0.059     0.091        17\n",
            "           1      0.559     0.380     0.452        50\n",
            "\n",
            "    accuracy                          0.504       123\n",
            "   macro avg      0.420     0.396     0.381       123\n",
            "weighted avg      0.482     0.504     0.470       123\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[42  4 10]\n",
            " [11  1  5]\n",
            " [31  0 19]]\n",
            "\n",
            "Top 10 combinations (constraint-satisfying):\n",
            "                                         features  n_features  test_acc  macro_f1 predicted_classes\n",
            "      Last Price, ret_1, p_state0_h1, p_state1_h1           4  0.504065  0.381097              -101\n",
            "                               ret_1, p_state0_h1           2  0.487805  0.323257              -101\n",
            "             Last Price, p_state1_h1, p_state2_h1           3  0.471545  0.400560              -101\n",
            "                          Last Price, p_state1_h1           2  0.463415  0.359719              -101\n",
            "                   Last Price, ret_1, p_state1_h1           3  0.463415  0.332863              -101\n",
            "Last Price, p_state0_h1, p_state1_h1, p_state2_h1           4  0.447154  0.320910              -101\n",
            "      Last Price, ret_1, p_state0_h1, p_state2_h1           4  0.430894  0.391826              -101\n",
            "                                            ret_1           1  0.430894  0.356628              -101\n",
            "             Last Price, p_state0_h1, p_state2_h1           3  0.398374  0.383405              -101\n",
            "      Last Price, ret_1, p_state1_h1, p_state2_h1           4  0.382114  0.358690              -101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Synthetic 3-class LSTM test\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "np.random.seed(7)\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "# -------- Config knobs --------\n",
        "N              = 2500          # number of weekly points\n",
        "LOOKBACK       = 16\n",
        "NEUTRAL_BAND   = 0.005         # ±0.5% neutral\n",
        "# Regime return params (mean, std) weekly\n",
        "MU   = np.array([-0.012, 0.0, 0.012])   # down, neutral, up\n",
        "SIG  = np.array([ 0.02 , 0.005, 0.02 ])\n",
        "# Sticky transition matrix (rows=sum=1)\n",
        "Tmat = np.array([\n",
        "    [0.92, 0.06, 0.02],\n",
        "    [0.05, 0.90, 0.05],\n",
        "    [0.02, 0.06, 0.92]\n",
        "])\n",
        "# How informative the prob features are:\n",
        "PROB_NOISE_STD = 0.10  # 0.0 = perfect one-hot; higher = noisier/less reliable\n",
        "PRICE_START    = 3.00\n",
        "\n",
        "# -------- 1) Generate hidden states --------\n",
        "states = np.zeros(N, dtype=int)\n",
        "states[0] = np.random.choice([0,1,2])  # 0=down,1=neutral,2=up\n",
        "for t in range(1, N):\n",
        "    states[t] = np.random.choice([0,1,2], p=Tmat[states[t-1]])\n",
        "\n",
        "# -------- 2) Generate returns conditional on state --------\n",
        "# ret_t ~ Normal(mu[state_t], sigma[state_t])\n",
        "ret = np.random.normal(MU[states], SIG[states])\n",
        "# Build price (weekly)\n",
        "price = np.empty(N)\n",
        "price[0] = PRICE_START\n",
        "for t in range(1, N):\n",
        "    price[t] = price[t-1] * (1.0 + ret[t])\n",
        "\n",
        "# -------- 3) Create noisy \"predicted\" state probabilities (features) --------\n",
        "# Start with one-hot for true state and add Gaussian noise, then renormalize\n",
        "probs = np.zeros((N, 3))\n",
        "probs[np.arange(N), states] = 1.0\n",
        "probs = probs + np.random.normal(0, PROB_NOISE_STD, size=probs.shape)\n",
        "probs = np.clip(probs, 1e-6, None)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "# -------- 4) Assemble DataFrame & labels --------\n",
        "df_syn = pd.DataFrame({\n",
        "    \"Last Price\": price,\n",
        "    \"ret_1\": ret,\n",
        "    \"p_state0_h1\": probs[:,0],\n",
        "    \"p_state1_h1\": probs[:,1],\n",
        "    \"p_state2_h1\": probs[:,2],\n",
        "}, index=pd.date_range(\"1990-01-07\", periods=N, freq=\"W-SUN\"))\n",
        "\n",
        "# label uses NEXT period return\n",
        "next_ret = df_syn[\"ret_1\"].shift(-1)\n",
        "target = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "         np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "df_syn[\"target\"] = target\n",
        "df_syn = df_syn.dropna().copy()  # drop last row with no next_ret\n",
        "\n",
        "print(\"Synthetic class counts:\", df_syn[\"target\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# -------- 5) Chronological 80/10/10 split --------\n",
        "n = len(df_syn)\n",
        "i_train_end = int(0.8 * n)\n",
        "i_val_end   = int(0.9 * n)\n",
        "df_tr, df_va, df_te = df_syn.iloc[:i_train_end], df_syn.iloc[i_train_end:i_val_end], df_syn.iloc[i_val_end:]\n",
        "\n",
        "FEATURES = [\"Last Price\", \"ret_1\", \"p_state0_h1\", \"p_state1_h1\", \"p_state2_h1\"]\n",
        "\n",
        "# Scale on TRAIN only\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_tr[FEATURES])\n",
        "for d in (df_tr, df_va, df_te):\n",
        "    d[FEATURES] = scaler.transform(d[FEATURES])\n",
        "\n",
        "# -------- 6) Make sequences --------\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    X_list, y_list = [], []\n",
        "    X = block_df[features].values\n",
        "    y = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X[t-lookback:t, :])\n",
        "        y_list.append(y[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "X_tr, y_tr = make_sequences(df_tr, FEATURES, LOOKBACK)\n",
        "X_va, y_va = make_sequences(df_va, FEATURES, LOOKBACK)\n",
        "X_te, y_te = make_sequences(df_te, FEATURES, LOOKBACK)\n",
        "\n",
        "# Map {−1,0,1} ↦ {0,1,2}\n",
        "to_idx   = {-1:0, 0:1, 1:2}\n",
        "from_idx = {0:-1, 1:0, 2:1}\n",
        "y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "y_te_i = np.vectorize(to_idx.get)(y_te)\n",
        "\n",
        "# -------- 7) Simple LSTM model --------\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(LOOKBACK, len(FEATURES))),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[\"accuracy\"])\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_tr, y_tr_i,\n",
        "    validation_data=(X_va, y_va_i),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# -------- 8) Evaluate --------\n",
        "probs_hat = model.predict(X_te, verbose=0)\n",
        "y_hat_i = probs_hat.argmax(axis=1)\n",
        "y_hat = np.vectorize(from_idx.get)(y_hat_i)\n",
        "\n",
        "print(\"\\nSynthetic Test accuracy:\", accuracy_score(y_te, y_hat))\n",
        "print(\"Synthetic Macro F1:\", f1_score(y_te, y_hat, labels=[-1,0,1], average=\"macro\"))\n",
        "print(\"\\nClassification report (synthetic):\\n\", classification_report(y_te, y_hat, digits=3))\n",
        "print(\"Confusion matrix (synthetic, rows=true, cols=pred [-1,0,1]):\\n\",\n",
        "      confusion_matrix(y_te, y_hat, labels=[-1,0,1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amVhbKPDK97V",
        "outputId": "1536209e-e57a-4c8d-d4fb-3896d99c7394"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic class counts: {-1: 775, 0: 972, 1: 753}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3675114581.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-3675114581.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-3675114581.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f8e1513a020> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Synthetic Test accuracy: 0.5170940170940171\n",
            "Synthetic Macro F1: 0.5184062159895805\n",
            "\n",
            "Classification report (synthetic):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.529     0.561     0.544        66\n",
            "           0      0.529     0.500     0.514        90\n",
            "           1      0.494     0.500     0.497        78\n",
            "\n",
            "    accuracy                          0.517       234\n",
            "   macro avg      0.517     0.520     0.518       234\n",
            "weighted avg      0.517     0.517     0.517       234\n",
            "\n",
            "Confusion matrix (synthetic, rows=true, cols=pred [-1,0,1]):\n",
            " [[37 18 11]\n",
            " [16 45 29]\n",
            " [17 22 39]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 0) Imports & helpers\n",
        "# ======================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "np.random.seed(7)\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "def make_sequences(block_df, feature_cols, lookback=16, label_col=\"target\"):\n",
        "    X_list, y_list = [], []\n",
        "    X = block_df[feature_cols].values\n",
        "    y = block_df[label_col].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X[t-lookback:t, :])\n",
        "        y_list.append(y[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "def map_labels(y, to_idx={-1:0, 0:1, 1:2}):\n",
        "    return np.vectorize(to_idx.get)(y)\n",
        "\n",
        "def inv_map_labels(y_idx, from_idx={0:-1, 1:0, 2:1}):\n",
        "    return np.vectorize(from_idx.get)(y_idx)\n",
        "\n",
        "def build_lstm(input_dim, lookback=16, units=32):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(lookback, input_dim)),\n",
        "        layers.LSTM(units),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(3, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def fit_eval(X_tr, y_tr_i, X_va, y_va_i, X_te, y_te, use_class_weight=True, epochs=60, batch_size=32):\n",
        "    # Optional class weights\n",
        "    cw = None\n",
        "    if use_class_weight:\n",
        "        classes = np.array([0,1,2])\n",
        "        w = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr_i)\n",
        "        cw = {int(c): float(wi) for c, wi in zip(classes, w)}\n",
        "        print(\"Class weights:\", cw)\n",
        "\n",
        "    model = build_lstm(input_dim=X_tr.shape[2], lookback=X_tr.shape[1], units=32)\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
        "    model.fit(X_tr, y_tr_i,\n",
        "              validation_data=(X_va, y_va_i),\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              callbacks=[es],\n",
        "              verbose=0,\n",
        "              class_weight=cw)\n",
        "\n",
        "    probs = model.predict(X_te, verbose=0)\n",
        "    y_hat_i = probs.argmax(axis=1)\n",
        "    y_hat   = inv_map_labels(y_hat_i)\n",
        "\n",
        "    print(\"Test accuracy:\", accuracy_score(y_te, y_hat))\n",
        "    print(\"Macro F1:\", f1_score(y_te, y_hat, labels=[-1,0,1], average=\"macro\"))\n",
        "    print(\"\\nClassification report:\\n\", classification_report(y_te, y_hat, digits=3))\n",
        "    print(\"Confusion matrix (rows=true, cols=pred [-1,0,1]):\\n\",\n",
        "          confusion_matrix(y_te, y_hat, labels=[-1,0,1]))\n",
        "\n",
        "# ======================================================\n",
        "# 1) SYNTHETIC CHECK — PERFECT SIGNAL (noise = 0.0)\n",
        "# ======================================================\n",
        "def run_synthetic_perfect(NEUTRAL_BAND=0.005, LOOKBACK=16, N=2500):\n",
        "    # Hidden states (sticky)\n",
        "    Tmat = np.array([[0.92,0.06,0.02],\n",
        "                     [0.05,0.90,0.05],\n",
        "                     [0.02,0.06,0.92]])\n",
        "    MU  = np.array([-0.012, 0.0, 0.012])\n",
        "    SIG = np.array([ 0.02 , 0.005, 0.02 ])\n",
        "\n",
        "    states = np.zeros(N, dtype=int)\n",
        "    states[0] = np.random.choice([0,1,2])\n",
        "    for t in range(1, N):\n",
        "        states[t] = np.random.choice([0,1,2], p=Tmat[states[t-1]])\n",
        "\n",
        "    ret = np.random.normal(MU[states], SIG[states])\n",
        "    price = np.empty(N); price[0] = 3.00\n",
        "    for t in range(1, N):\n",
        "        price[t] = price[t-1] * (1.0 + ret[t])\n",
        "\n",
        "    # PERFECT probabilities (one-hot), i.e., PROB_NOISE_STD = 0.0\n",
        "    probs = np.zeros((N,3)); probs[np.arange(N), states] = 1.0\n",
        "\n",
        "    df_syn = pd.DataFrame({\n",
        "        \"Last Price\": price,\n",
        "        \"ret_1\": ret,\n",
        "        \"p_state0_h1\": probs[:,0],\n",
        "        \"p_state1_h1\": probs[:,1],\n",
        "        \"p_state2_h1\": probs[:,2],\n",
        "    }, index=pd.date_range(\"1990-01-07\", periods=N, freq=\"W-SUN\"))\n",
        "\n",
        "    next_ret = df_syn[\"ret_1\"].shift(-1)\n",
        "    target = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "             np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "    df_syn[\"target\"] = target\n",
        "    df_syn = df_syn.dropna()\n",
        "\n",
        "    FEATURES = [\"Last Price\",\"ret_1\",\"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\"]\n",
        "    n = len(df_syn); i_tr, i_va = int(0.8*n), int(0.9*n)\n",
        "    df_tr, df_va, df_te = df_syn.iloc[:i_tr], df_syn.iloc[i_tr:i_va], df_syn.iloc[i_va:]\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    scaler.fit(df_tr[FEATURES])\n",
        "    for d in (df_tr, df_va, df_te):\n",
        "        d[FEATURES] = scaler.transform(d[FEATURES])\n",
        "\n",
        "    X_tr, y_tr = make_sequences(df_tr, FEATURES, LOOKBACK)\n",
        "    X_va, y_va = make_sequences(df_va, FEATURES, LOOKBACK)\n",
        "    X_te, y_te = make_sequences(df_te, FEATURES, LOOKBACK)\n",
        "\n",
        "    y_tr_i, y_va_i = map_labels(y_tr), map_labels(y_va)\n",
        "    print(\"Synthetic (perfect signal) class counts:\", df_syn[\"target\"].value_counts().sort_index().to_dict())\n",
        "    fit_eval(X_tr, y_tr_i, X_va, y_va_i, X_te, y_te, use_class_weight=False, epochs=40)\n",
        "\n",
        "print(\"\\n=== Running synthetic perfect-signal check (expect very high accuracy) ===\")\n",
        "run_synthetic_perfect(NEUTRAL_BAND=0.005, LOOKBACK=16, N=2500)\n",
        "\n",
        "# ======================================================\n",
        "# 2) REAL DATA — widen neutral band to ±1% and add lags/roll stats\n",
        "# ======================================================\n",
        "# NOTE: You must provide your DataFrame `df` with at least:\n",
        "# ['Last Price','ret_1','p_state0_h1','p_state1_h1','p_state2_h1'], indexed by weekly date.\n",
        "# Example:\n",
        "# df = your_dataframe.sort_index()\n",
        "\n",
        "# ---- UNCOMMENT and point df to your data before running ----\n",
        "# df = df.sort_index().copy()\n",
        "\n",
        "def run_real_with_features(df,\n",
        "                           NEUTRAL_BAND=0.01,   # widened neutral band ±1%\n",
        "                           LOOKBACK=16,\n",
        "                           use_class_weight=True):\n",
        "    df = df.sort_index().copy()\n",
        "\n",
        "    # Rebuild target from NEXT return with wider neutral band\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    df[\"target\"] = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "                    np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "\n",
        "    # ========== Feature engineering (no leakage) ==========\n",
        "    # Simple lags of weekly return\n",
        "    df[\"ret_lag1\"] = df[\"ret_1\"].shift(1)\n",
        "    df[\"ret_lag2\"] = df[\"ret_1\"].shift(2)\n",
        "    df[\"ret_lag3\"] = df[\"ret_1\"].shift(3)\n",
        "    # Rolling mean/vol over last 4 and 8 weeks (uses only past values)\n",
        "    df[\"ret_ma_4\"]  = df[\"ret_1\"].rolling(4, min_periods=4).mean()\n",
        "    df[\"ret_vol_4\"] = df[\"ret_1\"].rolling(4, min_periods=4).std()\n",
        "    df[\"ret_ma_8\"]  = df[\"ret_1\"].rolling(8, min_periods=8).mean()\n",
        "    df[\"ret_vol_8\"] = df[\"ret_1\"].rolling(8, min_periods=8).std()\n",
        "\n",
        "    FEATURES = [\n",
        "        \"Last Price\",\"ret_1\", \"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\",\n",
        "        \"ret_lag1\",\"ret_lag2\",\"ret_lag3\",\n",
        "        \"ret_ma_4\",\"ret_vol_4\",\"ret_ma_8\",\"ret_vol_8\"\n",
        "    ]\n",
        "\n",
        "    df = df.dropna(subset=FEATURES + [\"target\"]).copy()\n",
        "\n",
        "    print(\"\\nReal data class counts with ±1% neutral band:\", df[\"target\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "    # Chronological 80/10/10\n",
        "    n = len(df); i_tr, i_va = int(0.8*n), int(0.9*n)\n",
        "    df_tr, df_va, df_te = df.iloc[:i_tr], df.iloc[i_tr:i_va], df.iloc[i_va:]\n",
        "\n",
        "    # Scale on train only\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    scaler.fit(df_tr[FEATURES])\n",
        "    for d in (df_tr, df_va, df_te):\n",
        "        d[FEATURES] = scaler.transform(d[FEATURES])\n",
        "\n",
        "    # Sequences\n",
        "    X_tr, y_tr = make_sequences(df_tr, FEATURES, LOOKBACK)\n",
        "    X_va, y_va = make_sequences(df_va, FEATURES, LOOKBACK)\n",
        "    X_te, y_te = make_sequences(df_te, FEATURES, LOOKBACK)\n",
        "\n",
        "    # Labels to indices\n",
        "    y_tr_i, y_va_i = map_labels(y_tr), map_labels(y_va)\n",
        "\n",
        "    # Train & evaluate\n",
        "    fit_eval(X_tr, y_tr_i, X_va, y_va_i, X_te, y_te,\n",
        "             use_class_weight=use_class_weight, epochs=60, batch_size=32)\n",
        "\n",
        "# ---- UNCOMMENT to run on your real DataFrame `df` ----\n",
        "print(\"\\n=== Running real-data model with wider neutral band + lag/roll features ===\")\n",
        "run_real_with_features(df, NEUTRAL_BAND=0.01, LOOKBACK=52, use_class_weight=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kvpAv1QLpRD",
        "outputId": "9c8a9e94-8110-4976-a30a-e1645074eb1f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running synthetic perfect-signal check (expect very high accuracy) ===\n",
            "Synthetic (perfect signal) class counts: {-1: 775, 0: 972, 1: 753}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2711619866.py:116: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-2711619866.py:116: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-2711619866.py:116: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.5170940170940171\n",
            "Macro F1: 0.5184062159895805\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.529     0.561     0.544        66\n",
            "           0      0.529     0.500     0.514        90\n",
            "           1      0.494     0.500     0.497        78\n",
            "\n",
            "    accuracy                          0.517       234\n",
            "   macro avg      0.517     0.520     0.518       234\n",
            "weighted avg      0.517     0.517     0.517       234\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[37 18 11]\n",
            " [16 45 29]\n",
            " [17 22 39]]\n",
            "\n",
            "=== Running real-data model with wider neutral band + lag/roll features ===\n",
            "\n",
            "Real data class counts with ±1% neutral band: {-1: 619, 0: 447, 1: 675}\n",
            "Class weights: {0: 0.9305555555555556, 1: 1.3576494427558257, 2: 0.8411801632140615}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2711619866.py:180: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-2711619866.py:180: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-2711619866.py:180: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.43089430894308944\n",
            "Macro F1: 0.43011377390462363\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.472     0.378     0.420        45\n",
            "           0      0.417     0.541     0.471        37\n",
            "           1      0.410     0.390     0.400        41\n",
            "\n",
            "    accuracy                          0.431       123\n",
            "   macro avg      0.433     0.436     0.430       123\n",
            "weighted avg      0.435     0.431     0.428       123\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[17 14 14]\n",
            " [ 8 20  9]\n",
            " [11 14 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Use last row of features only (no sequence)\n",
        "X_last = df_syn[[\"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\"]].iloc[:-1].values\n",
        "y_last = df_syn[\"target\"].iloc[1:].values   # shift to next return\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000).fit(X_last, y_last)\n",
        "print(\"LogReg synthetic acc:\", logreg.score(X_last, y_last))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8QKOyYEMkVW",
        "outputId": "a943ab01-a293-47aa-bca4-c422e54f6a2a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg synthetic acc: 0.5654261704681873\n"
          ]
        }
      ]
    }
  ]
}