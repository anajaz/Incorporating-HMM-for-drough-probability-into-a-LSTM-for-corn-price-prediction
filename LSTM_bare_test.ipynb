{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALK4D5kjE2lj",
        "outputId": "50b6b831-151d-4735-e857-67d888120f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts (−1, 0, +1): {-1: 718, 0: 239, 1: 791}\n",
            "Shapes: X_tr (1346, 52, 5) X_va (123, 52, 5) X_te (123, 52, 5)\n",
            "Epoch 1/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.4503 - loss: 1.0407 - val_accuracy: 0.4797 - val_loss: 1.0052\n",
            "Epoch 2/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4708 - loss: 0.9816 - val_accuracy: 0.4959 - val_loss: 1.0007\n",
            "Epoch 3/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4818 - loss: 0.9799 - val_accuracy: 0.5122 - val_loss: 0.9961\n",
            "Epoch 4/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.4820 - loss: 0.9789 - val_accuracy: 0.5041 - val_loss: 0.9906\n",
            "Epoch 5/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.4787 - loss: 0.9776 - val_accuracy: 0.4959 - val_loss: 0.9878\n",
            "Epoch 6/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4767 - loss: 0.9770 - val_accuracy: 0.4959 - val_loss: 0.9870\n",
            "Epoch 7/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4774 - loss: 0.9766 - val_accuracy: 0.4878 - val_loss: 0.9857\n",
            "Epoch 8/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4819 - loss: 0.9755 - val_accuracy: 0.4878 - val_loss: 0.9869\n",
            "Epoch 9/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4881 - loss: 0.9745 - val_accuracy: 0.4472 - val_loss: 0.9886\n",
            "Epoch 10/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4813 - loss: 0.9733 - val_accuracy: 0.4553 - val_loss: 0.9895\n",
            "Epoch 11/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4926 - loss: 0.9719 - val_accuracy: 0.4472 - val_loss: 0.9886\n",
            "Epoch 12/50\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4976 - loss: 0.9707 - val_accuracy: 0.4472 - val_loss: 0.9894\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\n",
            "Test accuracy: 0.44715447154471544\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.467     0.500     0.483        56\n",
            "           0      0.000     0.000     0.000        17\n",
            "           1      0.429     0.540     0.478        50\n",
            "\n",
            "    accuracy                          0.447       123\n",
            "   macro avg      0.298     0.347     0.320       123\n",
            "weighted avg      0.387     0.447     0.414       123\n",
            "\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred classes in order [-1,0,1]):\n",
            " [[28  0 28]\n",
            " [ 9  0  8]\n",
            " [23  0 27]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Minimal LSTM 3-class classifier for weekly returns (−1 / 0 / +1)\n",
        "# ---------------------------------------------------------------\n",
        "# Requirements:\n",
        "# pip install numpy pandas scikit-learn tensorflow==2.* (or compatible)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# -------------------\n",
        "# 1) Load your data\n",
        "# -------------------\n",
        "# Assume you already have a DataFrame `df` indexed by weekly dates like the sample you posted.\n",
        "# Columns used: 'Last Price','ret_1','p_state0_h1','p_state1_h1','p_state2_h1'\n",
        "# If your DataFrame variable is named differently, just replace `df` below.\n",
        "\n",
        "df = pd.read_csv(\"df.csv\")  # <-- your DataFrame here\n",
        "\n",
        "# Ensure time order\n",
        "df = df.sort_index()\n",
        "\n",
        "# -------------------\n",
        "# 2) Build/verify target\n",
        "# -------------------\n",
        "# Target definition:\n",
        "# next_ret = ret_1 shifted by -1 (next period return)\n",
        "# if next_ret > +0.5%  ->  +1\n",
        "# if next_ret < -0.5%  ->  -1\n",
        "# else                 ->   0\n",
        "\n",
        "NEUTRAL_BAND = 0.007  # 0.5%\n",
        "if \"target\" not in df.columns:\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    tgt = np.where(next_ret > NEUTRAL_BAND, 1,\n",
        "          np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "    df[\"target\"] = tgt\n",
        "\n",
        "# Drop the last row (no label after shift) and any rows with NaNs in features/label\n",
        "feature_cols = [ \"Last Price\", \"ret_1\", \"p_state0_h1\", \"p_state1_h1\", \"p_state2_h1\"]\n",
        "df = df.dropna(subset=feature_cols + [\"target\"]).copy()\n",
        "\n",
        "# Optional: quick class distribution check\n",
        "print(\"Class counts (−1, 0, +1):\", df[\"target\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# -------------------\n",
        "# 3) Train/Val/Test split (chronological 80/10/10)\n",
        "# -------------------\n",
        "n = len(df)\n",
        "i_train_end = int(0.8 * n)\n",
        "i_val_end   = int(0.9 * n)\n",
        "\n",
        "df_train = df.iloc[:i_train_end]\n",
        "df_val   = df.iloc[i_train_end:i_val_end]\n",
        "df_test  = df.iloc[i_val_end:]\n",
        "\n",
        "# -------------------\n",
        "# 4) Make sequences\n",
        "# -------------------\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    \"\"\"Return X (num_seq, lookback, num_feat), y (num_seq,) aligned for next-step label already in block_df['target'].\"\"\"\n",
        "    X_list, y_list = [], []\n",
        "    X_src = block_df[features].values\n",
        "    y_src = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X_src[t-lookback:t, :])\n",
        "        y_list.append(y_src[t])  # label for the last timestep\n",
        "    X = np.array(X_list, dtype=np.float32)\n",
        "    y = np.array(y_list, dtype=np.int64)\n",
        "    return X, y\n",
        "\n",
        "LOOKBACK = 52  # weeks; change later as you like\n",
        "\n",
        "# Fit scaler on TRAIN ONLY, then transform splits BEFORE building sequences (no leakage)\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_train[feature_cols])\n",
        "\n",
        "df_train_s = df_train.copy()\n",
        "df_val_s   = df_val.copy()\n",
        "df_test_s  = df_test.copy()\n",
        "\n",
        "df_train_s[feature_cols] = scaler.transform(df_train[feature_cols])\n",
        "df_val_s[feature_cols]   = scaler.transform(df_val[feature_cols])\n",
        "df_test_s[feature_cols]  = scaler.transform(df_test[feature_cols])\n",
        "\n",
        "X_tr, y_tr = make_sequences(df_train_s, feature_cols, LOOKBACK)\n",
        "X_va, y_va = make_sequences(df_val_s,   feature_cols, LOOKBACK)\n",
        "X_te, y_te = make_sequences(df_test_s,  feature_cols, LOOKBACK)\n",
        "\n",
        "print(\"Shapes:\", \"X_tr\", X_tr.shape, \"X_va\", X_va.shape, \"X_te\", X_te.shape)\n",
        "\n",
        "# -------------------\n",
        "# 5) Map targets {−1,0,1} -> {0,1,2} for SparseCategoricalCrossentropy\n",
        "# -------------------\n",
        "to_idx = {-1: 0, 0: 1, 1: 2}\n",
        "y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "y_te_i = np.vectorize(to_idx.get)(y_te)\n",
        "\n",
        "# -------------------\n",
        "# 6) Simple LSTM model\n",
        "# -------------------\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(LOOKBACK, len(feature_cols))),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_tr, y_tr_i,\n",
        "    validation_data=(X_va, y_va_i),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------\n",
        "# 7) Evaluate\n",
        "# -------------------\n",
        "probs = model.predict(X_te)\n",
        "y_hat_i = probs.argmax(axis=1)\n",
        "\n",
        "# Map back {0,1,2} -> {−1,0,1}\n",
        "from_idx = {0: -1, 1: 0, 2: 1}\n",
        "y_hat = np.vectorize(from_idx.get)(y_hat_i)\n",
        "y_true = y_te  # already in {−1,0,1}\n",
        "\n",
        "print(\"\\nTest accuracy:\", accuracy_score(y_true, y_hat))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_true, y_hat, digits=3))\n",
        "print(\"\\nConfusion matrix (rows=true, cols=pred classes in order [-1,0,1]):\\n\",\n",
        "      confusion_matrix(y_true, y_hat, labels=[-1,0,1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['target'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LiJhoYWHsvn",
        "outputId": "0625f69a-1102-4253-e036-580b85ea383a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target\n",
            " 1    791\n",
            "-1    718\n",
            " 0    239\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Use last row of features only (no sequence)\n",
        "X_last = df[[\"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\"]].iloc[:-1].values\n",
        "y_last = df[\"target\"].iloc[1:].values   # shift to next return\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000).fit(X_last, y_last)\n",
        "print(\"LogReg synthetic acc:\", logreg.score(X_last, y_last))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuJz5kTcQjEM",
        "outputId": "b27bb0fb-7cf3-4155-8a4b-a1d494e91f79"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg synthetic acc: 0.4642243846594161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with class weights (and optional oversampling for class 0)\n",
        "# ---------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# ====== 0) CONFIG ======\n",
        "FEATURES     = [ \"Last Price\", \"p_state0_h1\"]  #, \"p_state0_h1\", \"p_state1_h1\", \"p_state2_h1\"]\n",
        "LOOKBACK     = 52\n",
        "NEUTRAL_BAND = 0.005\n",
        "EPOCHS       = 60\n",
        "BATCH_SIZE   = 32\n",
        "USE_OVERSAMPLING = False   # set True to try oversampling class 0 sequences\n",
        "\n",
        "# ====== 1) PREP ======\n",
        "df = df.sort_index().copy()\n",
        "\n",
        "if \"target\" not in df.columns:\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    df[\"target\"] = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "                    np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "\n",
        "df = df.dropna(subset=FEATURES + [\"target\"]).copy()\n",
        "\n",
        "print(\"Class counts in FULL data:\", df[\"target\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# Chronological 80/10/10 split\n",
        "n = len(df)\n",
        "i_train_end = int(0.9 * n)\n",
        "i_val_end   = int(0.95 * n)\n",
        "df_train, df_val, df_test = df.iloc[:i_train_end], df.iloc[i_train_end:i_val_end], df.iloc[i_val_end:]\n",
        "\n",
        "# Scale on TRAIN only\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_train[FEATURES])\n",
        "for d in (df_train, df_val, df_test):\n",
        "    d[FEATURES] = scaler.transform(d[FEATURES])\n",
        "\n",
        "# ====== 2) SEQUENCING ======\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    X_list, y_list = [], []\n",
        "    X_src = block_df[features].values\n",
        "    y_src = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X_src[t-lookback:t, :])\n",
        "        y_list.append(y_src[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "X_tr, y_tr = make_sequences(df_train, FEATURES, LOOKBACK)\n",
        "X_va, y_va = make_sequences(df_val,   FEATURES, LOOKBACK)\n",
        "X_te, y_te = make_sequences(df_test,  FEATURES, LOOKBACK)\n",
        "\n",
        "print(\"Shapes:\", \"X_tr\", X_tr.shape, \"X_va\", X_va.shape, \"X_te\", X_te.shape)\n",
        "\n",
        "# Map {-1,0,1} -> {0,1,2}\n",
        "to_idx   = {-1:0, 0:1, 1:2}\n",
        "from_idx = {0:-1, 1:0, 2:1}\n",
        "y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "y_te_i = np.vectorize(to_idx.get)(y_te)\n",
        "\n",
        "# ====== 3) (Optional) Oversampling of class 0 sequences in TRAIN ======\n",
        "if USE_OVERSAMPLING:\n",
        "    # gather indices by class\n",
        "    idx_m1 = np.where(y_tr_i == 0)[0]\n",
        "    idx_0  = np.where(y_tr_i == 1)[0]\n",
        "    idx_p1 = np.where(y_tr_i == 2)[0]\n",
        "    # target count ~ min(max of the other classes, 0.8 * median) to avoid huge duplication\n",
        "    target_0 = max(len(idx_m1), len(idx_p1))\n",
        "    if len(idx_0) > 0 and target_0 > len(idx_0):\n",
        "        add = np.random.choice(idx_0, size=target_0 - len(idx_0), replace=True)\n",
        "        X_tr = np.concatenate([X_tr, X_tr[add]], axis=0)\n",
        "        y_tr_i = np.concatenate([y_tr_i, y_tr_i[add]], axis=0)\n",
        "        # shuffle after oversampling\n",
        "        perm = np.random.permutation(len(y_tr_i))\n",
        "        X_tr, y_tr_i = X_tr[perm], y_tr_i[perm]\n",
        "        print(f\"Oversampled class 0 sequences from {len(idx_0)} to {target_0}\")\n",
        "\n",
        "# ====== 4) Class weights (computed on TRAIN after optional oversampling) ======\n",
        "classes = np.array([0,1,2])\n",
        "class_weights_vec = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr_i)\n",
        "class_weights = {int(c): float(w) for c, w in zip(classes, class_weights_vec)}\n",
        "print(\"Class weights (for indices {0,1,2} ≡ {-1,0,1}):\", class_weights)\n",
        "\n",
        "# ====== 5) Model ======\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(LOOKBACK, len(FEATURES))),\n",
        "    layers.LSTM(32, dropout=0.1, recurrent_dropout=0.0),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_tr, y_tr_i,\n",
        "    validation_data=(X_va, y_va_i),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[es],\n",
        "    verbose=1,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# ====== 6) Evaluate ======\n",
        "probs   = model.predict(X_te)\n",
        "y_hat_i = probs.argmax(axis=1)\n",
        "y_hat   = np.vectorize(from_idx.get)(y_hat_i)\n",
        "\n",
        "print(\"\\nTest accuracy:\", accuracy_score(y_te, y_hat))\n",
        "print(\"Macro F1:\", f1_score(y_te, y_hat, labels=[-1,0,1], average=\"macro\"))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_te, y_hat, digits=3))\n",
        "print(\"Confusion matrix (rows=true, cols=pred [-1,0,1]):\\n\",\n",
        "      confusion_matrix(y_te, y_hat, labels=[-1,0,1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAudDaQUH7_2",
        "outputId": "2720beb3-df25-47a0-8e36-d6e89b8cd93b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts in FULL data: {-1: 718, 0: 239, 1: 791}\n",
            "Shapes: X_tr (1521, 52, 2) X_va (35, 52, 2) X_te (36, 52, 2)\n",
            "Class weights (for indices {0,1,2} ≡ {-1,0,1}): {0: 0.8203883495145631, 1: 2.485294117647059, 2: 0.7253218884120172}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3311192286.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-3311192286.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-3311192286.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.2981 - loss: 1.1245 - val_accuracy: 0.1429 - val_loss: 1.1057\n",
            "Epoch 2/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3917 - loss: 1.1220 - val_accuracy: 0.1429 - val_loss: 1.1100\n",
            "Epoch 3/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3844 - loss: 1.1192 - val_accuracy: 0.1143 - val_loss: 1.1257\n",
            "Epoch 4/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3665 - loss: 1.1200 - val_accuracy: 0.1143 - val_loss: 1.1198\n",
            "Epoch 5/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3612 - loss: 1.1188 - val_accuracy: 0.1143 - val_loss: 1.1191\n",
            "Epoch 6/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3829 - loss: 1.1151 - val_accuracy: 0.3143 - val_loss: 1.1089\n",
            "Epoch 7/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3795 - loss: 1.1163 - val_accuracy: 0.4857 - val_loss: 1.0961\n",
            "Epoch 8/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3724 - loss: 1.1165 - val_accuracy: 0.2857 - val_loss: 1.1247\n",
            "Epoch 9/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3738 - loss: 1.1169 - val_accuracy: 0.4000 - val_loss: 1.1101\n",
            "Epoch 10/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3873 - loss: 1.1152 - val_accuracy: 0.3143 - val_loss: 1.1279\n",
            "Epoch 11/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.4045 - loss: 1.1115 - val_accuracy: 0.1143 - val_loss: 1.1568\n",
            "Epoch 12/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.3852 - loss: 1.1125 - val_accuracy: 0.1143 - val_loss: 1.1647\n",
            "Epoch 13/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.3991 - loss: 1.1081 - val_accuracy: 0.1143 - val_loss: 1.1742\n",
            "Epoch 14/60\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.3931 - loss: 1.1103 - val_accuracy: 0.1143 - val_loss: 1.1917\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
            "\n",
            "Test accuracy: 0.4166666666666667\n",
            "Macro F1: 0.3904761904761904\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.571     0.286     0.381        14\n",
            "           0      0.133     0.333     0.190         6\n",
            "           1      0.643     0.562     0.600        16\n",
            "\n",
            "    accuracy                          0.417        36\n",
            "   macro avg      0.449     0.394     0.390        36\n",
            "weighted avg      0.530     0.417     0.447        36\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[4 8 2]\n",
            " [1 2 3]\n",
            " [2 5 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Feature-subset search (require predicting ALL 3 classes on TEST)\n",
        "# ==============================================================\n",
        "\n",
        "import itertools\n",
        "from collections import OrderedDict\n",
        "\n",
        "REQUIRE_ALL_CLASSES = True\n",
        "ALL_CLASSES_SET = {-1, 0, 1}\n",
        "\n",
        "CANDIDATE_FEATURES = [\"Last Price\", \"ret_1\", \"p_state0_h1\", \"p_state1_h1\", \"p_state2_h1\"]\n",
        "\n",
        "# Ensure target exists and data sorted\n",
        "df = df.sort_index().copy()\n",
        "if \"target\" not in df.columns:\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    df[\"target\"] = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "                    np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "\n",
        "# Keep rows usable for all features (safe superset)\n",
        "df = df.dropna(subset=CANDIDATE_FEATURES + [\"target\"]).copy()\n",
        "\n",
        "# Chronological 80/10/10 split (fixed for all subsets)\n",
        "n = len(df)\n",
        "i_train_end = int(0.8 * n)\n",
        "i_val_end   = int(0.9 * n)\n",
        "df_train, df_val, df_test = df.iloc[:i_train_end].copy(), df.iloc[i_train_end:i_val_end].copy(), df.iloc[i_val_end:].copy()\n",
        "\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    X_list, y_list = [], []\n",
        "    X_src = block_df[features].values\n",
        "    y_src = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X_src[t-lookback:t, :])\n",
        "        y_list.append(y_src[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "to_idx   = {-1:0, 0:1, 1:2}\n",
        "from_idx = {0:-1, 1:0, 2:1}\n",
        "\n",
        "def build_model(input_dim):\n",
        "    tf.random.set_seed(42)\n",
        "    np.random.seed(42)\n",
        "    m = models.Sequential([\n",
        "        layers.Input(shape=(LOOKBACK, input_dim)),\n",
        "        layers.LSTM(32, dropout=0.1, recurrent_dropout=0.0),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(3, activation=\"softmax\")\n",
        "    ])\n",
        "    m.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return m\n",
        "\n",
        "def fit_eval_for_features(feat_subset):\n",
        "    # 1) Scale per subset (fit on TRAIN only)\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaler.fit(df_train[feat_subset])\n",
        "    dtr = df_train.copy(); dva = df_val.copy(); dte = df_test.copy()\n",
        "    dtr[feat_subset] = scaler.transform(dtr[feat_subset])\n",
        "    dva[feat_subset] = scaler.transform(dva[feat_subset])\n",
        "    dte[feat_subset] = scaler.transform(dte[feat_subset])\n",
        "\n",
        "    # 2) Make sequences\n",
        "    X_tr, y_tr = make_sequences(dtr, feat_subset, LOOKBACK)\n",
        "    X_va, y_va = make_sequences(dva, feat_subset, LOOKBACK)\n",
        "    X_te, y_te = make_sequences(dte, feat_subset, LOOKBACK)\n",
        "\n",
        "    # Map labels {-1,0,1} -> {0,1,2}\n",
        "    y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "    y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "    y_te_i = np.vectorize(to_idx.get)(y_te)\n",
        "\n",
        "    # 3) Optional oversampling (class 0 index = 1)\n",
        "    X_train_fin, y_train_fin = X_tr, y_tr_i\n",
        "    if USE_OVERSAMPLING:\n",
        "        idx_m1 = np.where(y_tr_i == 0)[0]\n",
        "        idx_0  = np.where(y_tr_i == 1)[0]\n",
        "        idx_p1 = np.where(y_tr_i == 2)[0]\n",
        "        target_0 = max(len(idx_m1), len(idx_p1))\n",
        "        if len(idx_0) > 0 and target_0 > len(idx_0):\n",
        "            add = np.random.choice(idx_0, size=target_0 - len(idx_0), replace=True)\n",
        "            X_train_fin = np.concatenate([X_tr, X_tr[add]], axis=0)\n",
        "            y_train_fin = np.concatenate([y_tr_i, y_tr_i[add]], axis=0)\n",
        "            perm = np.random.permutation(len(y_train_fin))\n",
        "            X_train_fin, y_train_fin = X_train_fin[perm], y_train_fin[perm]\n",
        "\n",
        "    # 4) Class weights from TRAIN\n",
        "    classes = np.array([0,1,2])\n",
        "    cw_vec = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_fin)\n",
        "    class_weights = {int(c): float(w) for c, w in zip(classes, cw_vec)}\n",
        "\n",
        "    # 5) Build, fit\n",
        "    model = build_model(input_dim=len(feat_subset))\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
        "    model.fit(\n",
        "        X_train_fin, y_train_fin,\n",
        "        validation_data=(X_va, y_va_i),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[es],\n",
        "        verbose=0,\n",
        "        class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    # 6) Evaluate on TEST\n",
        "    probs   = model.predict(X_te, verbose=0)\n",
        "    y_hat_i = probs.argmax(axis=1)\n",
        "    y_hat   = np.vectorize(from_idx.get)(y_hat_i)\n",
        "\n",
        "    acc  = accuracy_score(y_te, y_hat)\n",
        "    f1m  = f1_score(y_te, y_hat, labels=[-1,0,1], average=\"macro\")\n",
        "    rep  = classification_report(y_te, y_hat, digits=3)\n",
        "    cm   = confusion_matrix(y_te, y_hat, labels=[-1,0,1])\n",
        "\n",
        "    pred_classes = set(np.unique(y_hat))\n",
        "    passes = (pred_classes == ALL_CLASSES_SET) if REQUIRE_ALL_CLASSES else True\n",
        "\n",
        "    return {\n",
        "        \"features\": tuple(feat_subset),\n",
        "        \"n_features\": len(feat_subset),\n",
        "        \"test_acc\": acc,\n",
        "        \"macro_f1\": f1m,\n",
        "        \"report\": rep,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"pred_classes\": pred_classes,\n",
        "        \"passes_all_classes\": passes\n",
        "    }\n",
        "\n",
        "# Iterate all non-empty subsets\n",
        "all_subsets = []\n",
        "for r in range(1, len(CANDIDATE_FEATURES)+1):\n",
        "    all_subsets += list(itertools.combinations(CANDIDATE_FEATURES, r))\n",
        "\n",
        "print(f\"Evaluating {len(all_subsets)} feature combinations...\")\n",
        "results, skipped = [], 0\n",
        "for subset in all_subsets:\n",
        "    res = fit_eval_for_features(list(subset))\n",
        "    if REQUIRE_ALL_CLASSES and not res[\"passes_all_classes\"]:\n",
        "        skipped += 1\n",
        "        missing = ALL_CLASSES_SET - res[\"pred_classes\"]\n",
        "        print(f\"{subset} -> SKIPPED: missing classes {sorted(list(missing))}  \"\n",
        "              f\"(predicted: {sorted(list(res['pred_classes']))})\")\n",
        "        continue\n",
        "    results.append(res)\n",
        "    print(f\"{subset} -> acc={res['test_acc']:.3f}, f1={res['macro_f1']:.3f} | \"\n",
        "          f\"pred_classes={sorted(list(res['pred_classes']))}\")\n",
        "\n",
        "print(f\"\\nCombinations passing the 'all classes predicted' constraint: {len(results)} \"\n",
        "      f\"(skipped {skipped})\")\n",
        "\n",
        "if len(results) == 0:\n",
        "    print(\"\\nNo combinations satisfied the constraint. \"\n",
        "          \"Consider widening NEUTRAL_BAND, enabling USE_OVERSAMPLING, \"\n",
        "          \"or increasing EPOCHS/hidden units.\")\n",
        "else:\n",
        "    results_sorted = sorted(results, key=lambda d: (d[\"test_acc\"], d[\"macro_f1\"]), reverse=True)\n",
        "    best = results_sorted[0]\n",
        "    print(\"\\n================ BEST COMBINATION (with all classes predicted) ================\")\n",
        "    print(\"Features:\", best[\"features\"])\n",
        "    print(f\"Test accuracy: {best['test_acc']:.3f}\")\n",
        "    print(f\"Macro F1: {best['macro_f1']:.3f}\")\n",
        "    print(\"Predicted classes:\", sorted(list(best[\"pred_classes\"])))\n",
        "    print(\"\\nClassification report:\\n\", best[\"report\"])\n",
        "    print(\"Confusion matrix (rows=true, cols=pred [-1,0,1]):\\n\", best[\"confusion_matrix\"])\n",
        "\n",
        "    # Optional: show top-10 table\n",
        "    tbl = pd.DataFrame([{\n",
        "        \"features\": \", \".join(r[\"features\"]),\n",
        "        \"n_features\": r[\"n_features\"],\n",
        "        \"test_acc\": r[\"test_acc\"],\n",
        "        \"macro_f1\": r[\"macro_f1\"],\n",
        "        \"predicted_classes\": \"\".join(str(c) for c in sorted(list(r[\"pred_classes\"])))\n",
        "    } for r in results_sorted])\n",
        "    print(\"\\nTop 10 combinations (constraint-satisfying):\")\n",
        "    print(tbl.head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2UsxI32M0Sc",
        "outputId": "9c8580fa-1d14-4404-83fb-fc4f6b7c148b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 31 feature combinations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Last Price',) -> SKIPPED: missing classes [0]  (predicted: [np.int64(-1), np.int64(1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ret_1',) -> SKIPPED: missing classes [0]  (predicted: [np.int64(-1), np.int64(1)])\n",
            "('p_state0_h1',) -> acc=0.309, f1=0.311 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('p_state1_h1',) -> acc=0.301, f1=0.270 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('p_state2_h1',) -> SKIPPED: missing classes [-1, 1]  (predicted: [np.int64(0)])\n",
            "('Last Price', 'ret_1') -> acc=0.455, f1=0.312 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state0_h1') -> acc=0.488, f1=0.378 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state1_h1') -> acc=0.472, f1=0.334 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Last Price', 'p_state2_h1') -> SKIPPED: missing classes [0]  (predicted: [np.int64(-1), np.int64(1)])\n",
            "('ret_1', 'p_state0_h1') -> acc=0.276, f1=0.277 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('ret_1', 'p_state1_h1') -> acc=0.317, f1=0.306 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ret_1', 'p_state2_h1') -> SKIPPED: missing classes [-1, 1]  (predicted: [np.int64(0)])\n",
            "('p_state0_h1', 'p_state1_h1') -> acc=0.260, f1=0.236 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('p_state0_h1', 'p_state2_h1') -> acc=0.268, f1=0.239 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('p_state1_h1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n",
            "('Last Price', 'ret_1', 'p_state0_h1') -> acc=0.301, f1=0.306 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'ret_1', 'p_state1_h1') -> acc=0.512, f1=0.407 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Last Price', 'ret_1', 'p_state2_h1') -> SKIPPED: missing classes [0]  (predicted: [np.int64(-1), np.int64(1)])\n",
            "('Last Price', 'p_state0_h1', 'p_state1_h1') -> acc=0.285, f1=0.267 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state0_h1', 'p_state2_h1') -> acc=0.439, f1=0.402 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state1_h1', 'p_state2_h1') -> acc=0.333, f1=0.323 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('ret_1', 'p_state0_h1', 'p_state1_h1') -> acc=0.293, f1=0.290 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ret_1', 'p_state0_h1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n",
            "('ret_1', 'p_state1_h1', 'p_state2_h1') -> acc=0.276, f1=0.242 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('p_state0_h1', 'p_state1_h1', 'p_state2_h1') -> acc=0.268, f1=0.242 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'ret_1', 'p_state0_h1', 'p_state1_h1') -> acc=0.333, f1=0.330 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'ret_1', 'p_state0_h1', 'p_state2_h1') -> acc=0.423, f1=0.383 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'ret_1', 'p_state1_h1', 'p_state2_h1') -> acc=0.455, f1=0.356 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "('Last Price', 'p_state0_h1', 'p_state1_h1', 'p_state2_h1') -> acc=0.252, f1=0.232 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ret_1', 'p_state0_h1', 'p_state1_h1', 'p_state2_h1') -> SKIPPED: missing classes [-1]  (predicted: [np.int64(0), np.int64(1)])\n",
            "('Last Price', 'ret_1', 'p_state0_h1', 'p_state1_h1', 'p_state2_h1') -> acc=0.276, f1=0.248 | pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "\n",
            "Combinations passing the 'all classes predicted' constraint: 22 (skipped 9)\n",
            "\n",
            "================ BEST COMBINATION (with all classes predicted) ================\n",
            "Features: ('Last Price', 'ret_1', 'p_state1_h1')\n",
            "Test accuracy: 0.512\n",
            "Macro F1: 0.407\n",
            "Predicted classes: [np.int64(-1), np.int64(0), np.int64(1)]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.524     0.768     0.623        56\n",
            "           0      0.333     0.118     0.174        17\n",
            "           1      0.514     0.360     0.424        50\n",
            "\n",
            "    accuracy                          0.512       123\n",
            "   macro avg      0.457     0.415     0.407       123\n",
            "weighted avg      0.494     0.512     0.480       123\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[43  1 12]\n",
            " [10  2  5]\n",
            " [29  3 18]]\n",
            "\n",
            "Top 10 combinations (constraint-satisfying):\n",
            "                                   features  n_features  test_acc  macro_f1 predicted_classes\n",
            "             Last Price, ret_1, p_state1_h1           3  0.512195  0.406877              -101\n",
            "                    Last Price, p_state0_h1           2  0.487805  0.377513              -101\n",
            "                    Last Price, p_state1_h1           2  0.471545  0.333982              -101\n",
            "Last Price, ret_1, p_state1_h1, p_state2_h1           4  0.455285  0.355911              -101\n",
            "                          Last Price, ret_1           2  0.455285  0.311697              -101\n",
            "       Last Price, p_state0_h1, p_state2_h1           3  0.439024  0.401822              -101\n",
            "Last Price, ret_1, p_state0_h1, p_state2_h1           4  0.422764  0.383220              -101\n",
            "Last Price, ret_1, p_state0_h1, p_state1_h1           4  0.333333  0.329982              -101\n",
            "       Last Price, p_state1_h1, p_state2_h1           3  0.333333  0.322759              -101\n",
            "                         ret_1, p_state1_h1           2  0.317073  0.305964              -101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Synthetic 3-class LSTM test\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "np.random.seed(7)\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "# -------- Config knobs --------\n",
        "N              = 2500          # number of weekly points\n",
        "LOOKBACK       = 16\n",
        "NEUTRAL_BAND   = 0.005         # ±0.5% neutral\n",
        "# Regime return params (mean, std) weekly\n",
        "MU   = np.array([-0.012, 0.0, 0.012])   # down, neutral, up\n",
        "SIG  = np.array([ 0.02 , 0.005, 0.02 ])\n",
        "# Sticky transition matrix (rows=sum=1)\n",
        "Tmat = np.array([\n",
        "    [0.92, 0.06, 0.02],\n",
        "    [0.05, 0.90, 0.05],\n",
        "    [0.02, 0.06, 0.92]\n",
        "])\n",
        "# How informative the prob features are:\n",
        "PROB_NOISE_STD = 0.10  # 0.0 = perfect one-hot; higher = noisier/less reliable\n",
        "PRICE_START    = 3.00\n",
        "\n",
        "# -------- 1) Generate hidden states --------\n",
        "states = np.zeros(N, dtype=int)\n",
        "states[0] = np.random.choice([0,1,2])  # 0=down,1=neutral,2=up\n",
        "for t in range(1, N):\n",
        "    states[t] = np.random.choice([0,1,2], p=Tmat[states[t-1]])\n",
        "\n",
        "# -------- 2) Generate returns conditional on state --------\n",
        "# ret_t ~ Normal(mu[state_t], sigma[state_t])\n",
        "ret = np.random.normal(MU[states], SIG[states])\n",
        "# Build price (weekly)\n",
        "price = np.empty(N)\n",
        "price[0] = PRICE_START\n",
        "for t in range(1, N):\n",
        "    price[t] = price[t-1] * (1.0 + ret[t])\n",
        "\n",
        "# -------- 3) Create noisy \"predicted\" state probabilities (features) --------\n",
        "# Start with one-hot for true state and add Gaussian noise, then renormalize\n",
        "probs = np.zeros((N, 3))\n",
        "probs[np.arange(N), states] = 1.0\n",
        "probs = probs + np.random.normal(0, PROB_NOISE_STD, size=probs.shape)\n",
        "probs = np.clip(probs, 1e-6, None)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "# -------- 4) Assemble DataFrame & labels --------\n",
        "df_syn = pd.DataFrame({\n",
        "    \"Last Price\": price,\n",
        "    \"ret_1\": ret,\n",
        "    \"p_state0_h1\": probs[:,0],\n",
        "    \"p_state1_h1\": probs[:,1],\n",
        "    \"p_state2_h1\": probs[:,2],\n",
        "}, index=pd.date_range(\"1990-01-07\", periods=N, freq=\"W-SUN\"))\n",
        "\n",
        "# label uses NEXT period return\n",
        "next_ret = df_syn[\"ret_1\"].shift(-1)\n",
        "target = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "         np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "df_syn[\"target\"] = target\n",
        "df_syn = df_syn.dropna().copy()  # drop last row with no next_ret\n",
        "\n",
        "print(\"Synthetic class counts:\", df_syn[\"target\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "# -------- 5) Chronological 80/10/10 split --------\n",
        "n = len(df_syn)\n",
        "i_train_end = int(0.8 * n)\n",
        "i_val_end   = int(0.9 * n)\n",
        "df_tr, df_va, df_te = df_syn.iloc[:i_train_end], df_syn.iloc[i_train_end:i_val_end], df_syn.iloc[i_val_end:]\n",
        "\n",
        "FEATURES = [\"Last Price\", \"ret_1\", \"p_state0_h1\", \"p_state1_h1\", \"p_state2_h1\"]\n",
        "\n",
        "# Scale on TRAIN only\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_tr[FEATURES])\n",
        "for d in (df_tr, df_va, df_te):\n",
        "    d[FEATURES] = scaler.transform(d[FEATURES])\n",
        "\n",
        "# -------- 6) Make sequences --------\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    X_list, y_list = [], []\n",
        "    X = block_df[features].values\n",
        "    y = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X[t-lookback:t, :])\n",
        "        y_list.append(y[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "X_tr, y_tr = make_sequences(df_tr, FEATURES, LOOKBACK)\n",
        "X_va, y_va = make_sequences(df_va, FEATURES, LOOKBACK)\n",
        "X_te, y_te = make_sequences(df_te, FEATURES, LOOKBACK)\n",
        "\n",
        "# Map {−1,0,1} ↦ {0,1,2}\n",
        "to_idx   = {-1:0, 0:1, 1:2}\n",
        "from_idx = {0:-1, 1:0, 2:1}\n",
        "y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "y_te_i = np.vectorize(to_idx.get)(y_te)\n",
        "\n",
        "# -------- 7) Simple LSTM model --------\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(LOOKBACK, len(FEATURES))),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[\"accuracy\"])\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_tr, y_tr_i,\n",
        "    validation_data=(X_va, y_va_i),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# -------- 8) Evaluate --------\n",
        "probs_hat = model.predict(X_te, verbose=0)\n",
        "y_hat_i = probs_hat.argmax(axis=1)\n",
        "y_hat = np.vectorize(from_idx.get)(y_hat_i)\n",
        "\n",
        "print(\"\\nSynthetic Test accuracy:\", accuracy_score(y_te, y_hat))\n",
        "print(\"Synthetic Macro F1:\", f1_score(y_te, y_hat, labels=[-1,0,1], average=\"macro\"))\n",
        "print(\"\\nClassification report (synthetic):\\n\", classification_report(y_te, y_hat, digits=3))\n",
        "print(\"Confusion matrix (synthetic, rows=true, cols=pred [-1,0,1]):\\n\",\n",
        "      confusion_matrix(y_te, y_hat, labels=[-1,0,1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amVhbKPDK97V",
        "outputId": "9ea370a8-c5ff-4b96-baf1-fc41bfa7f060"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic class counts: {-1: 775, 0: 972, 1: 753}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3675114581.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-3675114581.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-3675114581.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Synthetic Test accuracy: 0.5170940170940171\n",
            "Synthetic Macro F1: 0.5183433041483726\n",
            "\n",
            "Classification report (synthetic):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.529     0.561     0.544        66\n",
            "           0      0.536     0.500     0.517        90\n",
            "           1      0.487     0.500     0.494        78\n",
            "\n",
            "    accuracy                          0.517       234\n",
            "   macro avg      0.517     0.520     0.518       234\n",
            "weighted avg      0.518     0.517     0.517       234\n",
            "\n",
            "Confusion matrix (synthetic, rows=true, cols=pred [-1,0,1]):\n",
            " [[37 17 12]\n",
            " [16 45 29]\n",
            " [17 22 39]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 0) Imports & helpers\n",
        "# ======================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "np.random.seed(7)\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "def make_sequences(block_df, feature_cols, lookback=16, label_col=\"target\"):\n",
        "    X_list, y_list = [], []\n",
        "    X = block_df[feature_cols].values\n",
        "    y = block_df[label_col].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X[t-lookback:t, :])\n",
        "        y_list.append(y[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "def map_labels(y, to_idx={-1:0, 0:1, 1:2}):\n",
        "    return np.vectorize(to_idx.get)(y)\n",
        "\n",
        "def inv_map_labels(y_idx, from_idx={0:-1, 1:0, 2:1}):\n",
        "    return np.vectorize(from_idx.get)(y_idx)\n",
        "\n",
        "def build_lstm(input_dim, lookback=16, units=32):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(lookback, input_dim)),\n",
        "        layers.LSTM(units),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(3, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def fit_eval(X_tr, y_tr_i, X_va, y_va_i, X_te, y_te, use_class_weight=True, epochs=60, batch_size=32):\n",
        "    # Optional class weights\n",
        "    cw = None\n",
        "    if use_class_weight:\n",
        "        classes = np.array([0,1,2])\n",
        "        w = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr_i)\n",
        "        cw = {int(c): float(wi) for c, wi in zip(classes, w)}\n",
        "        print(\"Class weights:\", cw)\n",
        "\n",
        "    model = build_lstm(input_dim=X_tr.shape[2], lookback=X_tr.shape[1], units=32)\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
        "    model.fit(X_tr, y_tr_i,\n",
        "              validation_data=(X_va, y_va_i),\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              callbacks=[es],\n",
        "              verbose=0,\n",
        "              class_weight=cw)\n",
        "\n",
        "    probs = model.predict(X_te, verbose=0)\n",
        "    y_hat_i = probs.argmax(axis=1)\n",
        "    y_hat   = inv_map_labels(y_hat_i)\n",
        "\n",
        "    print(\"Test accuracy:\", accuracy_score(y_te, y_hat))\n",
        "    print(\"Macro F1:\", f1_score(y_te, y_hat, labels=[-1,0,1], average=\"macro\"))\n",
        "    print(\"\\nClassification report:\\n\", classification_report(y_te, y_hat, digits=3))\n",
        "    print(\"Confusion matrix (rows=true, cols=pred [-1,0,1]):\\n\",\n",
        "          confusion_matrix(y_te, y_hat, labels=[-1,0,1]))\n",
        "\n",
        "# ======================================================\n",
        "# 1) SYNTHETIC CHECK — PERFECT SIGNAL (noise = 0.0)\n",
        "# ======================================================\n",
        "def run_synthetic_perfect(NEUTRAL_BAND=0.005, LOOKBACK=16, N=2500):\n",
        "    # Hidden states (sticky)\n",
        "    Tmat = np.array([[0.92,0.06,0.02],\n",
        "                     [0.05,0.90,0.05],\n",
        "                     [0.02,0.06,0.92]])\n",
        "    MU  = np.array([-0.012, 0.0, 0.012])\n",
        "    SIG = np.array([ 0.02 , 0.005, 0.02 ])\n",
        "\n",
        "    states = np.zeros(N, dtype=int)\n",
        "    states[0] = np.random.choice([0,1,2])\n",
        "    for t in range(1, N):\n",
        "        states[t] = np.random.choice([0,1,2], p=Tmat[states[t-1]])\n",
        "\n",
        "    ret = np.random.normal(MU[states], SIG[states])\n",
        "    price = np.empty(N); price[0] = 3.00\n",
        "    for t in range(1, N):\n",
        "        price[t] = price[t-1] * (1.0 + ret[t])\n",
        "\n",
        "    # PERFECT probabilities (one-hot), i.e., PROB_NOISE_STD = 0.0\n",
        "    probs = np.zeros((N,3)); probs[np.arange(N), states] = 1.0\n",
        "\n",
        "    df_syn = pd.DataFrame({\n",
        "        \"Last Price\": price,\n",
        "        \"ret_1\": ret,\n",
        "        \"p_state0_h1\": probs[:,0],\n",
        "        \"p_state1_h1\": probs[:,1],\n",
        "        \"p_state2_h1\": probs[:,2],\n",
        "    }, index=pd.date_range(\"1990-01-07\", periods=N, freq=\"W-SUN\"))\n",
        "\n",
        "    next_ret = df_syn[\"ret_1\"].shift(-1)\n",
        "    target = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "             np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "    df_syn[\"target\"] = target\n",
        "    df_syn = df_syn.dropna()\n",
        "\n",
        "    FEATURES = [\"Last Price\",\"ret_1\",\"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\"]\n",
        "    n = len(df_syn); i_tr, i_va = int(0.8*n), int(0.9*n)\n",
        "    df_tr, df_va, df_te = df_syn.iloc[:i_tr], df_syn.iloc[i_tr:i_va], df_syn.iloc[i_va:]\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    scaler.fit(df_tr[FEATURES])\n",
        "    for d in (df_tr, df_va, df_te):\n",
        "        d[FEATURES] = scaler.transform(d[FEATURES])\n",
        "\n",
        "    X_tr, y_tr = make_sequences(df_tr, FEATURES, LOOKBACK)\n",
        "    X_va, y_va = make_sequences(df_va, FEATURES, LOOKBACK)\n",
        "    X_te, y_te = make_sequences(df_te, FEATURES, LOOKBACK)\n",
        "\n",
        "    y_tr_i, y_va_i = map_labels(y_tr), map_labels(y_va)\n",
        "    print(\"Synthetic (perfect signal) class counts:\", df_syn[\"target\"].value_counts().sort_index().to_dict())\n",
        "    fit_eval(X_tr, y_tr_i, X_va, y_va_i, X_te, y_te, use_class_weight=False, epochs=40)\n",
        "\n",
        "print(\"\\n=== Running synthetic perfect-signal check (expect very high accuracy) ===\")\n",
        "run_synthetic_perfect(NEUTRAL_BAND=0.005, LOOKBACK=16, N=2500)\n",
        "\n",
        "# ======================================================\n",
        "# 2) REAL DATA — widen neutral band to ±1% and add lags/roll stats\n",
        "# ======================================================\n",
        "# NOTE: You must provide your DataFrame `df` with at least:\n",
        "# ['Last Price','ret_1','p_state0_h1','p_state1_h1','p_state2_h1'], indexed by weekly date.\n",
        "# Example:\n",
        "# df = your_dataframe.sort_index()\n",
        "\n",
        "# ---- UNCOMMENT and point df to your data before running ----\n",
        "# df = df.sort_index().copy()\n",
        "\n",
        "def run_real_with_features(df,\n",
        "                           NEUTRAL_BAND=0.01,   # widened neutral band ±1%\n",
        "                           LOOKBACK=16,\n",
        "                           use_class_weight=True):\n",
        "    df = df.sort_index().copy()\n",
        "\n",
        "    # Rebuild target from NEXT return with wider neutral band\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    df[\"target\"] = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "                    np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "\n",
        "    # ========== Feature engineering (no leakage) ==========\n",
        "    # Simple lags of weekly return\n",
        "    df[\"ret_lag1\"] = df[\"ret_1\"].shift(1)\n",
        "    df[\"ret_lag2\"] = df[\"ret_1\"].shift(2)\n",
        "    df[\"ret_lag3\"] = df[\"ret_1\"].shift(3)\n",
        "    # Rolling mean/vol over last 4 and 8 weeks (uses only past values)\n",
        "    df[\"ret_ma_4\"]  = df[\"ret_1\"].rolling(4, min_periods=4).mean()\n",
        "    df[\"ret_vol_4\"] = df[\"ret_1\"].rolling(4, min_periods=4).std()\n",
        "    df[\"ret_ma_8\"]  = df[\"ret_1\"].rolling(8, min_periods=8).mean()\n",
        "    df[\"ret_vol_8\"] = df[\"ret_1\"].rolling(8, min_periods=8).std()\n",
        "\n",
        "    FEATURES = [\n",
        "        \"Last Price\",\"ret_1\", \"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\",\n",
        "        \"ret_lag1\",\"ret_lag2\",\"ret_lag3\",\n",
        "        \"ret_ma_4\",\"ret_vol_4\",\"ret_ma_8\",\"ret_vol_8\"\n",
        "    ]\n",
        "\n",
        "    df = df.dropna(subset=FEATURES + [\"target\"]).copy()\n",
        "\n",
        "    print(\"\\nReal data class counts with ±1% neutral band:\", df[\"target\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "    # Chronological 80/10/10\n",
        "    n = len(df); i_tr, i_va = int(0.8*n), int(0.9*n)\n",
        "    df_tr, df_va, df_te = df.iloc[:i_tr], df.iloc[i_tr:i_va], df.iloc[i_va:]\n",
        "\n",
        "    # Scale on train only\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    scaler.fit(df_tr[FEATURES])\n",
        "    for d in (df_tr, df_va, df_te):\n",
        "        d[FEATURES] = scaler.transform(d[FEATURES])\n",
        "\n",
        "    # Sequences\n",
        "    X_tr, y_tr = make_sequences(df_tr, FEATURES, LOOKBACK)\n",
        "    X_va, y_va = make_sequences(df_va, FEATURES, LOOKBACK)\n",
        "    X_te, y_te = make_sequences(df_te, FEATURES, LOOKBACK)\n",
        "\n",
        "    # Labels to indices\n",
        "    y_tr_i, y_va_i = map_labels(y_tr), map_labels(y_va)\n",
        "\n",
        "    # Train & evaluate\n",
        "    fit_eval(X_tr, y_tr_i, X_va, y_va_i, X_te, y_te,\n",
        "             use_class_weight=use_class_weight, epochs=60, batch_size=32)\n",
        "\n",
        "# ---- UNCOMMENT to run on your real DataFrame `df` ----\n",
        "print(\"\\n=== Running real-data model with wider neutral band + lag/roll features ===\")\n",
        "run_real_with_features(df, NEUTRAL_BAND=0.01, LOOKBACK=52, use_class_weight=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kvpAv1QLpRD",
        "outputId": "433d04ff-c1df-479b-c34a-dab9778bfc7c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running synthetic perfect-signal check (expect very high accuracy) ===\n",
            "Synthetic (perfect signal) class counts: {-1: 775, 0: 972, 1: 753}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2711619866.py:116: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-2711619866.py:116: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-2711619866.py:116: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.5170940170940171\n",
            "Macro F1: 0.5184062159895805\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.529     0.561     0.544        66\n",
            "           0      0.529     0.500     0.514        90\n",
            "           1      0.494     0.500     0.497        78\n",
            "\n",
            "    accuracy                          0.517       234\n",
            "   macro avg      0.517     0.520     0.518       234\n",
            "weighted avg      0.517     0.517     0.517       234\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[37 18 11]\n",
            " [16 45 29]\n",
            " [17 22 39]]\n",
            "\n",
            "=== Running real-data model with wider neutral band + lag/roll features ===\n",
            "\n",
            "Real data class counts with ±1% neutral band: {-1: 619, 0: 447, 1: 675}\n",
            "Class weights: {0: 0.9305555555555556, 1: 1.3576494427558257, 2: 0.8411801632140615}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2711619866.py:180: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-2711619866.py:180: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n",
            "/tmp/ipython-input-2711619866.py:180: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  d[FEATURES] = scaler.transform(d[FEATURES])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.34959349593495936\n",
            "Macro F1: 0.34084967320261433\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.421     0.178     0.250        45\n",
            "           0      0.292     0.514     0.373        37\n",
            "           1      0.410     0.390     0.400        41\n",
            "\n",
            "    accuracy                          0.350       123\n",
            "   macro avg      0.375     0.361     0.341       123\n",
            "weighted avg      0.379     0.350     0.337       123\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[ 8 25 12]\n",
            " [ 7 19 11]\n",
            " [ 4 21 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Use last row of features only (no sequence)\n",
        "X_last = df_syn[[\"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\"]].iloc[:-1].values\n",
        "y_last = df_syn[\"target\"].iloc[1:].values   # shift to next return\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000).fit(X_last, y_last)\n",
        "print(\"LogReg synthetic acc:\", logreg.score(X_last, y_last))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8QKOyYEMkVW",
        "outputId": "75faa01e-9a9b-4c15-dd02-eaea5ec47416"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg synthetic acc: 0.5654261704681873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "SK4qcmOUVDIU",
        "outputId": "0e75983f-14f2-48d8-e884-5df3a258aa90"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            date  Last Price     ret_1  p_state0_h1  p_state1_h1  p_state2_h1  \\\n",
              "1     1992-01-12        2.43  0.016736     0.949237     0.050318     0.000445   \n",
              "2     1992-01-19        2.48  0.020576     0.922941     0.075764     0.001295   \n",
              "3     1992-01-26        2.55  0.028226     0.781818     0.212314     0.005868   \n",
              "4     1992-02-02        2.57  0.007843     0.400583     0.581123     0.018294   \n",
              "5     1992-02-09        2.57  0.000000     0.222312     0.753668     0.024020   \n",
              "...          ...         ...       ...          ...          ...          ...   \n",
              "1746  2025-06-22        4.06 -0.044706     0.842409     0.154882     0.002709   \n",
              "1747  2025-06-29        4.00 -0.014778     0.938669     0.060752     0.000579   \n",
              "1748  2025-07-06        4.10  0.025000     0.955329     0.044461     0.000210   \n",
              "1749  2025-07-13        3.87 -0.056098     0.933028     0.066269     0.000704   \n",
              "1750  2025-07-20        3.95  0.020672     0.849863     0.147592     0.002545   \n",
              "\n",
              "      target  \n",
              "1          1  \n",
              "2          1  \n",
              "3          1  \n",
              "4          0  \n",
              "5         -1  \n",
              "...      ...  \n",
              "1746      -1  \n",
              "1747       1  \n",
              "1748      -1  \n",
              "1749       1  \n",
              "1750      -1  \n",
              "\n",
              "[1748 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-134b25d3-4014-4803-8016-616490a52c00\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Last Price</th>\n",
              "      <th>ret_1</th>\n",
              "      <th>p_state0_h1</th>\n",
              "      <th>p_state1_h1</th>\n",
              "      <th>p_state2_h1</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1992-01-12</td>\n",
              "      <td>2.43</td>\n",
              "      <td>0.016736</td>\n",
              "      <td>0.949237</td>\n",
              "      <td>0.050318</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1992-01-19</td>\n",
              "      <td>2.48</td>\n",
              "      <td>0.020576</td>\n",
              "      <td>0.922941</td>\n",
              "      <td>0.075764</td>\n",
              "      <td>0.001295</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1992-01-26</td>\n",
              "      <td>2.55</td>\n",
              "      <td>0.028226</td>\n",
              "      <td>0.781818</td>\n",
              "      <td>0.212314</td>\n",
              "      <td>0.005868</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1992-02-02</td>\n",
              "      <td>2.57</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>0.400583</td>\n",
              "      <td>0.581123</td>\n",
              "      <td>0.018294</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1992-02-09</td>\n",
              "      <td>2.57</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.222312</td>\n",
              "      <td>0.753668</td>\n",
              "      <td>0.024020</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>2025-06-22</td>\n",
              "      <td>4.06</td>\n",
              "      <td>-0.044706</td>\n",
              "      <td>0.842409</td>\n",
              "      <td>0.154882</td>\n",
              "      <td>0.002709</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>2025-06-29</td>\n",
              "      <td>4.00</td>\n",
              "      <td>-0.014778</td>\n",
              "      <td>0.938669</td>\n",
              "      <td>0.060752</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>2025-07-06</td>\n",
              "      <td>4.10</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.955329</td>\n",
              "      <td>0.044461</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1749</th>\n",
              "      <td>2025-07-13</td>\n",
              "      <td>3.87</td>\n",
              "      <td>-0.056098</td>\n",
              "      <td>0.933028</td>\n",
              "      <td>0.066269</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1750</th>\n",
              "      <td>2025-07-20</td>\n",
              "      <td>3.95</td>\n",
              "      <td>0.020672</td>\n",
              "      <td>0.849863</td>\n",
              "      <td>0.147592</td>\n",
              "      <td>0.002545</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1748 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-134b25d3-4014-4803-8016-616490a52c00')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-134b25d3-4014-4803-8016-616490a52c00 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-134b25d3-4014-4803-8016-616490a52c00');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f1f0792a-e5e8-4ea9-a578-875f2dc6e4f2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1f0792a-e5e8-4ea9-a578-875f2dc6e4f2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f1f0792a-e5e8-4ea9-a578-875f2dc6e4f2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_25e3e558-70dd-4d4f-8b39-51371d3ad42f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_25e3e558-70dd-4d4f-8b39-51371d3ad42f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Expanding CV + Final Test\n",
        "# =========================\n",
        "\n",
        "# ---- Config for CV ----\n",
        "CV_FIRST_TRAIN = 520          # first train span\n",
        "CV_VAL_SPAN    = 150          # each validation span\n",
        "CV_STEP        = 150          # slide by 150 each fold\n",
        "TRAINVAL_END   = pd.Timestamp(\"2022-12-31\")  # CV/trainval cutoff\n",
        "TEST_START     = pd.Timestamp(\"2023-01-01\")  # test period start\n",
        "\n",
        "# Require that the model predicts ALL classes in each validation fold (optional)\n",
        "REQUIRE_ALL_CLASSES_VAL = False   # set True if you want this constraint on VAL\n",
        "ALL_CLASSES_SET = {-1, 0, 1}\n",
        "\n",
        "# Small hyperparameter grid for model selection via CV\n",
        "HPARAM_GRID = [\n",
        "    {\"units\": 32, \"dense\": 16, \"dropout\": 0.10, \"lr\": 1e-3},\n",
        "    {\"units\": 64, \"dense\": 16, \"dropout\": 0.10, \"lr\": 1e-3},\n",
        "    {\"units\": 64, \"dense\": 32, \"dropout\": 0.20, \"lr\": 1e-3},\n",
        "]\n",
        "\n",
        "# -----------------------\n",
        "# 0) Prep & Target label\n",
        "# -----------------------\n",
        "# Pick ONE of the two options below.\n",
        "\n",
        "# --- Option 1: you already have a 'date' column ---\n",
        "if \"date\" in df.columns:\n",
        "    df = df.sort_values(\"date\").copy()\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    df = df.set_index(\"date\")\n",
        "else:\n",
        "    # --- Option 2: no date column, you know the start date and weekly freq ---\n",
        "    # Adjust the start date to your dataset’s true first week\n",
        "    start = pd.Timestamp(\"1970-01-04\")  # e.g., first Sunday of 1970; change if needed\n",
        "    df = df.sort_index().copy()\n",
        "    df.index = pd.date_range(start=start, periods=len(df), freq=\"W-SUN\")\n",
        "\n",
        "print(\"Index dtype:\", df.index.dtype)\n",
        "print(\"Range:\", df.index.min(), \"→\", df.index.max())\n",
        "if not np.issubdtype(df.index.dtype, np.datetime64):\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "# Your original features (adjust if you wish)\n",
        "FEATURES = [\"Last Price\",\"ret_1\",\"p_state0_h1\",\"p_state1_h1\",\"p_state2_h1\"]\n",
        "\n",
        "if \"target\" not in df.columns:\n",
        "    next_ret = df[\"ret_1\"].shift(-1)\n",
        "    df[\"target\"] = np.where(next_ret >  NEUTRAL_BAND,  1,\n",
        "                    np.where(next_ret < -NEUTRAL_BAND, -1, 0))\n",
        "\n",
        "# Keep rows usable (for stability across folds we drop NaNs upfront)\n",
        "df = df.dropna(subset=FEATURES + [\"target\"]).copy()\n",
        "\n",
        "# Split into trainval (<=2022-12-31) and test (>=2023-01-01)\n",
        "mask_trainval = df.index <= TRAINVAL_END\n",
        "mask_test     = df.index >= TEST_START\n",
        "\n",
        "df_trainval = df.loc[mask_trainval].copy()\n",
        "df_test     = df.loc[mask_test].copy()\n",
        "\n",
        "print(f\"Train/Val span: {df_trainval.index.min().date()} .. {df_trainval.index.max().date()} | n={len(df_trainval)}\")\n",
        "print(f\"Test span:      {df_test.index.min().date() if not df_test.empty else 'N/A'} .. {df_test.index.max().date() if not df_test.empty else 'N/A'} | n={len(df_test)}\")\n",
        "\n",
        "# -----------------------\n",
        "# 1) Helpers\n",
        "# -----------------------\n",
        "to_idx   = {-1:0, 0:1, 1:2}\n",
        "from_idx = {0:-1, 1:0, 2:1}\n",
        "\n",
        "def make_sequences(block_df, features, lookback=16):\n",
        "    X_list, y_list = [], []\n",
        "    X_src = block_df[features].values\n",
        "    y_src = block_df[\"target\"].values\n",
        "    for t in range(lookback, len(block_df)):\n",
        "        X_list.append(X_src[t-lookback:t, :])\n",
        "        y_list.append(y_src[t])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n",
        "\n",
        "def build_model(input_dim, units=32, dense=16, dropout=0.1, lr=1e-3):\n",
        "    tf.random.set_seed(42); np.random.seed(42)\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(LOOKBACK, input_dim)),\n",
        "        layers.LSTM(units, dropout=dropout, recurrent_dropout=0.0),\n",
        "        layers.Dense(dense, activation=\"relu\"),\n",
        "        layers.Dense(3, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def fit_one_split(d_tr, d_va, features, hparams):\n",
        "    # Scale on TRAIN only\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaler.fit(d_tr[features])\n",
        "    dtr = d_tr.copy(); dva = d_va.copy()\n",
        "    dtr[features] = scaler.transform(dtr[features])\n",
        "    dva[features] = scaler.transform(dva[features])\n",
        "\n",
        "    # Sequences\n",
        "    X_tr, y_tr = make_sequences(dtr, features, LOOKBACK)\n",
        "    X_va, y_va = make_sequences(dva, features, LOOKBACK)\n",
        "\n",
        "    # Label mapping\n",
        "    y_tr_i = np.vectorize(to_idx.get)(y_tr)\n",
        "    y_va_i = np.vectorize(to_idx.get)(y_va)\n",
        "\n",
        "    # Optional oversampling of neutral (index=1)\n",
        "    X_train_fin, y_train_fin = X_tr, y_tr_i\n",
        "    if USE_OVERSAMPLING:\n",
        "        idx_m1 = np.where(y_tr_i == 0)[0]\n",
        "        idx_0  = np.where(y_tr_i == 1)[0]\n",
        "        idx_p1 = np.where(y_tr_i == 2)[0]\n",
        "        target_0 = max(len(idx_m1), len(idx_p1))\n",
        "        if len(idx_0) > 0 and target_0 > len(idx_0):\n",
        "            add = np.random.choice(idx_0, size=target_0 - len(idx_0), replace=True)\n",
        "            X_train_fin = np.concatenate([X_tr, X_tr[add]], axis=0)\n",
        "            y_train_fin = np.concatenate([y_tr_i, y_tr_i[add]], axis=0)\n",
        "            perm = np.random.permutation(len(y_train_fin))\n",
        "            X_train_fin, y_train_fin = X_train_fin[perm], y_train_fin[perm]\n",
        "\n",
        "    # Class weights\n",
        "    classes = np.array([0,1,2])\n",
        "    cw_vec = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_fin)\n",
        "    class_weights = {int(c): float(w) for c, w in zip(classes, cw_vec)}\n",
        "\n",
        "    # Build & fit\n",
        "    model = build_model(len(features), **hparams)\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
        "    model.fit(\n",
        "        X_train_fin, y_train_fin,\n",
        "        validation_data=(X_va, y_va_i),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[es],\n",
        "        verbose=0,\n",
        "        class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    probs = model.predict(X_va, verbose=0)\n",
        "    yhat_i = probs.argmax(axis=1)\n",
        "    yhat   = np.vectorize(from_idx.get)(yhat_i)\n",
        "\n",
        "    acc = accuracy_score(y_va, yhat)\n",
        "    f1m = f1_score(y_va, yhat, labels=[-1,0,1], average=\"macro\")\n",
        "    rep = classification_report(y_va, yhat, digits=3)\n",
        "    cm  = confusion_matrix(y_va, yhat, labels=[-1,0,1])\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"scaler\": scaler,\n",
        "        \"val_acc\": acc,\n",
        "        \"val_f1m\": f1m,\n",
        "        \"val_report\": rep,\n",
        "        \"val_cm\": cm,\n",
        "        \"pred_classes\": set(np.unique(yhat))\n",
        "    }\n",
        "\n",
        "def expanding_cv(df_tv, features, hparams):\n",
        "    \"\"\"Return per-fold metrics and mean scores for these hparams.\"\"\"\n",
        "    n = len(df_tv)\n",
        "    idx = np.arange(n)\n",
        "    # fold k: train [0:train_end), val [train_end:val_end)\n",
        "    train_end = CV_FIRST_TRAIN\n",
        "    folds = []\n",
        "    while True:\n",
        "        val_end = train_end + CV_VAL_SPAN\n",
        "        if val_end > n:\n",
        "            break\n",
        "        d_tr = df_tv.iloc[:train_end]\n",
        "        d_va = df_tv.iloc[train_end:val_end]\n",
        "        res  = fit_one_split(d_tr, d_va, features, hparams)\n",
        "\n",
        "        if REQUIRE_ALL_CLASSES_VAL and res[\"pred_classes\"] != ALL_CLASSES_SET:\n",
        "            folds.append({**res, \"skip_reason\": f\"missing classes {sorted(list(ALL_CLASSES_SET - res['pred_classes']))}\"})\n",
        "        else:\n",
        "            folds.append(res)\n",
        "\n",
        "        # expand train, slide val\n",
        "        train_end = val_end if CV_STEP is None else (train_end + CV_STEP)\n",
        "\n",
        "        # Stop if next val_end would exceed\n",
        "        if train_end + CV_VAL_SPAN > n:\n",
        "            break\n",
        "\n",
        "    # Aggregate (only non-skipped folds)\n",
        "    valid_folds = [f for f in folds if \"skip_reason\" not in f]\n",
        "    if len(valid_folds) == 0:\n",
        "        return {\"folds\": folds, \"mean_val_f1m\": -np.inf, \"mean_val_acc\": -np.inf}\n",
        "\n",
        "    mean_f1 = float(np.mean([f[\"val_f1m\"] for f in valid_folds]))\n",
        "    mean_acc = float(np.mean([f[\"val_acc\"] for f in valid_folds]))\n",
        "    return {\"folds\": folds, \"mean_val_f1m\": mean_f1, \"mean_val_acc\": mean_acc}\n",
        "\n",
        "# -----------------------\n",
        "# 2) Run CV over grid\n",
        "# -----------------------\n",
        "cv_results = []\n",
        "for hp in HPARAM_GRID:\n",
        "    cv = expanding_cv(df_trainval, FEATURES, hp)\n",
        "    cv_results.append({\"hparams\": hp, **cv})\n",
        "    print(f\"HP {hp} -> mean Val F1={cv['mean_val_f1m']:.3f}, mean Val Acc={cv['mean_val_acc']:.3f} | folds={len(cv['folds'])}\")\n",
        "\n",
        "# Pick best by mean F1 (then Acc)\n",
        "cv_sorted = sorted(cv_results, key=lambda d: (d[\"mean_val_f1m\"], d[\"mean_val_acc\"]), reverse=True)\n",
        "best_cv = cv_sorted[0]\n",
        "print(\"\\n=== BEST HPARAMS FROM CV ===\")\n",
        "print(best_cv[\"hparams\"])\n",
        "print(f\"Mean Val F1={best_cv['mean_val_f1m']:.3f}, Mean Val Acc={best_cv['mean_val_acc']:.3f}\")\n",
        "\n",
        "# Optional: show per-fold summary for best\n",
        "for i, f in enumerate(best_cv[\"folds\"], 1):\n",
        "    if \"skip_reason\" in f:\n",
        "        print(f\"Fold {i}: SKIPPED ({f['skip_reason']})\")\n",
        "    else:\n",
        "        print(f\"Fold {i}: Val F1={f['val_f1m']:.3f}, Val Acc={f['val_acc']:.3f}, pred_classes={sorted(list(f['pred_classes']))}\")\n",
        "\n",
        "# -----------------------\n",
        "# 3) Retrain on ALL train+val (<=2022-12-31) with best hparams\n",
        "# -----------------------\n",
        "best_hp = best_cv[\"hparams\"]\n",
        "# Scale on full trainval\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_trainval[FEATURES])\n",
        "dtrv = df_trainval.copy()\n",
        "dte  = df_test.copy()\n",
        "dtrv[FEATURES] = scaler.transform(dtrv[FEATURES])\n",
        "if not dte.empty:\n",
        "    dte[FEATURES] = scaler.transform(dte[FEATURES])\n",
        "\n",
        "# Sequences\n",
        "X_trv, y_trv = make_sequences(dtrv, FEATURES, LOOKBACK)\n",
        "X_te,  y_te  = (np.empty((0, LOOKBACK, len(FEATURES)), dtype=np.float32), np.array([], dtype=np.int64))\n",
        "if not df_test.empty:\n",
        "    X_te, y_te = make_sequences(dte, FEATURES, LOOKBACK)\n",
        "\n",
        "# Label mapping\n",
        "y_trv_i = np.vectorize(to_idx.get)(y_trv)\n",
        "y_te_i  = np.vectorize(to_idx.get)(y_te) if len(y_te) else y_te\n",
        "\n",
        "# Optional oversampling on trainval\n",
        "X_train_fin, y_train_fin = X_trv, y_trv_i\n",
        "if USE_OVERSAMPLING:\n",
        "    idx_m1 = np.where(y_trv_i == 0)[0]\n",
        "    idx_0  = np.where(y_trv_i == 1)[0]\n",
        "    idx_p1 = np.where(y_trv_i == 2)[0]\n",
        "    target_0 = max(len(idx_m1), len(idx_p1))\n",
        "    if len(idx_0) > 0 and target_0 > len(idx_0):\n",
        "        add = np.random.choice(idx_0, size=target_0 - len(idx_0), replace=True)\n",
        "        X_train_fin = np.concatenate([X_trv, X_trv[add]], axis=0)\n",
        "        y_train_fin = np.concatenate([y_trv_i, y_trv_i[add]], axis=0)\n",
        "        perm = np.random.permutation(len(y_train_fin))\n",
        "        X_train_fin, y_train_fin = X_train_fin[perm], y_train_fin[perm]\n",
        "\n",
        "# Class weights on full trainval\n",
        "classes = np.array([0,1,2])\n",
        "cw_vec = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_fin)\n",
        "class_weights = {int(c): float(w) for c, w in zip(classes, cw_vec)}\n",
        "\n",
        "# Train final model\n",
        "final_model = build_model(len(FEATURES), **best_hp)\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
        "\n",
        "# Hold out a tiny slice of trainval tail for monitoring to keep ES meaningful\n",
        "val_tail = max(LOOKBACK + 50, 300)  # heuristic\n",
        "split = max(len(X_train_fin) - val_tail, LOOKBACK + 10)\n",
        "X_tr_final, y_tr_final = X_train_fin[:split], y_train_fin[:split]\n",
        "X_va_final, y_va_final = X_train_fin[split:], y_train_fin[split:]\n",
        "\n",
        "final_model.fit(\n",
        "    X_tr_final, y_tr_final,\n",
        "    validation_data=(X_va_final, y_va_final) if len(X_va_final) else None,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[es],\n",
        "    verbose=0,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 4) Test evaluation (2023-01-01 .. end)\n",
        "# -----------------------\n",
        "if len(y_te):\n",
        "    probs = final_model.predict(X_te, verbose=0)\n",
        "    yhat_i = probs.argmax(axis=1)\n",
        "    yhat   = np.vectorize(from_idx.get)(yhat_i)\n",
        "\n",
        "    test_acc = accuracy_score(y_te, yhat)\n",
        "    test_f1m = f1_score(y_te, yhat, labels=[-1,0,1], average=\"macro\")\n",
        "    rep      = classification_report(y_te, yhat, digits=3)\n",
        "    cm       = confusion_matrix(y_te, yhat, labels=[-1,0,1])\n",
        "    predc    = sorted(list(np.unique(yhat)))\n",
        "\n",
        "    print(\"\\n=========== FINAL TEST RESULTS (2023–) ===========\")\n",
        "    print(f\"Acc={test_acc:.3f}, Macro-F1={test_f1m:.3f}, Predicted classes={predc}\")\n",
        "    print(\"\\nClassification report:\\n\", rep)\n",
        "    print(\"Confusion matrix (rows=true, cols=pred [-1,0,1]):\\n\", cm)\n",
        "else:\n",
        "    print(\"\\nNo test samples available after sequencing. Check TEST_START or data length.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ghDdhdtTbF0",
        "outputId": "28bfbb0b-e6b5-4033-b4f2-2b767ef8f0ed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index dtype: datetime64[ns]\n",
            "Range: 1992-01-12 00:00:00 → 2025-07-20 00:00:00\n",
            "Train/Val span: 1992-01-12 .. 2022-12-25 | n=1614\n",
            "Test span:      2023-01-01 .. 2025-07-20 | n=134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HP {'units': 32, 'dense': 16, 'dropout': 0.1, 'lr': 0.001} -> mean Val F1=0.334, mean Val Acc=0.412 | folds=7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HP {'units': 64, 'dense': 16, 'dropout': 0.1, 'lr': 0.001} -> mean Val F1=0.303, mean Val Acc=0.407 | folds=7\n",
            "HP {'units': 64, 'dense': 32, 'dropout': 0.2, 'lr': 0.001} -> mean Val F1=0.330, mean Val Acc=0.391 | folds=7\n",
            "\n",
            "=== BEST HPARAMS FROM CV ===\n",
            "{'units': 32, 'dense': 16, 'dropout': 0.1, 'lr': 0.001}\n",
            "Mean Val F1=0.334, Mean Val Acc=0.412\n",
            "Fold 1: Val F1=0.293, Val Acc=0.313, pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "Fold 2: Val F1=0.405, Val Acc=0.455, pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "Fold 3: Val F1=0.411, Val Acc=0.493, pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "Fold 4: Val F1=0.281, Val Acc=0.448, pred_classes=[np.int64(-1), np.int64(1)]\n",
            "Fold 5: Val F1=0.250, Val Acc=0.291, pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "Fold 6: Val F1=0.366, Val Acc=0.433, pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "Fold 7: Val F1=0.329, Val Acc=0.448, pred_classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "\n",
            "=========== FINAL TEST RESULTS (2023–) ===========\n",
            "Acc=0.314, Macro-F1=0.283, Predicted classes=[np.int64(-1), np.int64(0), np.int64(1)]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1      0.667     0.074     0.133        54\n",
            "           0      0.153     0.529     0.237        17\n",
            "           1      0.453     0.511     0.480        47\n",
            "\n",
            "    accuracy                          0.314       118\n",
            "   macro avg      0.424     0.371     0.283       118\n",
            "weighted avg      0.507     0.314     0.286       118\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred [-1,0,1]):\n",
            " [[ 4 28 22]\n",
            " [ 1  9  7]\n",
            " [ 1 22 24]]\n"
          ]
        }
      ]
    }
  ]
}