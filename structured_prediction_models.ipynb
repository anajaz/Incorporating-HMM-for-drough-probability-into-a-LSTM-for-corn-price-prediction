{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j5xOs1lINIms"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"data.csv\", dtype={\"date\": \"string\"})\n",
        "data.rename(columns={\"y\": \"target\"}, inplace=True)\n",
        "data.rename(columns={\"p_state0_h1\": \"p_state0\"}, inplace=True)\n",
        "data.rename(columns={\"p_state1_h1\": \"p_state1\"}, inplace=True)\n",
        "data.rename(columns={\"p_state2_h1\": \"p_state2\"}, inplace=True)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3FBlO1gi9h7",
        "outputId": "c46084f3-ec6d-48bf-e074-8a4f76979291"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            date     spi_6  p_state0  p_state1  p_state2  Last Price  \\\n",
            "0     1992-01-12 -0.676211  0.188472  0.778493  0.033035        2.43   \n",
            "1     1992-01-19 -0.408163  0.042357  0.918067  0.039576        2.48   \n",
            "2     1992-01-26 -0.149762  0.031910  0.927547  0.040544        2.55   \n",
            "3     1992-02-02 -0.012210  0.031731  0.926709  0.041559        2.57   \n",
            "4     1992-02-09 -0.154111  0.031869  0.927580  0.040551        2.57   \n",
            "...          ...       ...       ...       ...       ...         ...   \n",
            "1744  2025-06-29 -0.317970  0.047144  0.919293  0.033563        4.00   \n",
            "1745  2025-07-06 -0.346593  0.053727  0.912975  0.033298        4.10   \n",
            "1746  2025-07-13 -0.113659  0.039652  0.926234  0.034114        3.87   \n",
            "1747  2025-07-20  0.110532  0.035815  0.928811  0.035374        3.95   \n",
            "1748  2025-07-27  0.294891  0.035153  0.926273  0.038575        3.87   \n",
            "\n",
            "     price_date_used  fallback_days     ret_1  top_state  next_ret  target  \n",
            "0         1992-01-10            2.0  0.016736          1  0.020576       1  \n",
            "1         1992-01-17            2.0  0.020576          1  0.028226       1  \n",
            "2         1992-01-24            2.0  0.028226          1  0.007843       1  \n",
            "3         1992-01-31            2.0  0.007843          1  0.000000       0  \n",
            "4         1992-02-07            2.0  0.000000          1 -0.007782       0  \n",
            "...              ...            ...       ...        ...       ...     ...  \n",
            "1744      2025-06-27            2.0 -0.014778          1  0.025000       1  \n",
            "1745      2025-07-03            3.0  0.025000          1 -0.056098       0  \n",
            "1746      2025-07-11            2.0 -0.056098          1  0.020672       1  \n",
            "1747      2025-07-18            2.0  0.020672          1 -0.020253       0  \n",
            "1748      2025-07-25            2.0 -0.020253          1       NaN       0  \n",
            "\n",
            "[1749 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This notebook builds different modesl for predicting corn prices\n",
        "\n",
        "The models are always a simple logistic regression and a LSTM, but the inputs change:\n",
        "First we segment the training, val and test sets. Then we build different models:\n",
        "1.   Only price data\n",
        "2.   Price and prcp data\n",
        "3.   Price and SPI data\n",
        "4.   Price, SPI and HMM data\n"
      ],
      "metadata": {
        "id": "MG48muHWNOsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Only price data"
      ],
      "metadata": {
        "id": "ydaX_QYUNRfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1 Logistic regression"
      ],
      "metadata": {
        "id": "PfvZwxmcNV5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2 LSTM"
      ],
      "metadata": {
        "id": "9hUBppWX79RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "xP2iy0kq9059"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LSTM-only pipeline for binary next-period price direction\n",
        "# Assumes your DataFrame ALREADY contains the binary target column.\n",
        "#\n",
        "# Expected columns at minimum:\n",
        "#   [\"date\",\"return\",\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi\",\"target\"]\n",
        "# Set a datetime index and sort ascending before running:\n",
        "#   df.index = pd.to_datetime(df[\"date\"]); df = df.sort_index()\n",
        "# ============================================================\n",
        "\n",
        "# ----------------------------\n",
        "# Config and splitting helpers\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class CVConfig:\n",
        "    val_span: int        # e.g., 52 (weeks)\n",
        "    step: int            # e.g., 26\n",
        "    min_train_span: int  # e.g., 156\n",
        "\n",
        "FEATURES = [\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi_6\"]\n",
        "\n",
        "def make_static_splits(df: pd.DataFrame, test_start: str | pd.Timestamp):\n",
        "    \"\"\"\n",
        "    Chronological hold-out split: all dates >= test_start go to test.\n",
        "    The remainder will later be split into train/val.\n",
        "    \"\"\"\n",
        "    mask_test = df.index >= pd.to_datetime(test_start)\n",
        "    return df.loc[~mask_test].copy(), df.loc[mask_test].copy()\n",
        "\n",
        "def expanding_cv_folds(trainval_df: pd.DataFrame, cfg: CVConfig):\n",
        "    \"\"\"\n",
        "    Walk-forward (expanding-window) CV generator over train/val pairs.\n",
        "    \"\"\"\n",
        "    n = len(trainval_df); idx = np.arange(n)\n",
        "    train_end = cfg.min_train_span\n",
        "    while True:\n",
        "        val_end = train_end + cfg.val_span\n",
        "        if val_end > n: break\n",
        "        train_mask = idx < train_end\n",
        "        val_mask   = (idx >= train_end) & (idx < val_end)\n",
        "        yield trainval_df.loc[train_mask].copy(), trainval_df.loc[val_mask].copy()\n",
        "        train_end += cfg.step\n",
        "\n",
        "# ----------------------------\n",
        "# Sequences, model, and metrics\n",
        "# ----------------------------\n",
        "def make_sequences(df: pd.DataFrame, lookback: int, feature_cols, target_col: str = \"target\"):\n",
        "    \"\"\"\n",
        "    Build (X, y) where each X[i] is a window of length `lookback` ending at t-1,\n",
        "    and y[i] is the target at time t.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    V = df[feature_cols].values\n",
        "    t = df[target_col].values\n",
        "    for i in range(lookback, len(df)):        # predict y at i using window [i-L, ..., i-1]\n",
        "        X.append(V[i-lookback:i])\n",
        "        y.append(t[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_lstm(input_dim, lookback, units=64, dropout=0.2, lr=1e-3):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(lookback, input_dim)),\n",
        "        layers.LSTM(units, return_sequences=False),\n",
        "        layers.Dropout(dropout),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Root Mean Squared Error. For classification, pass probabilities for âˆšBrier.\"\"\"\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "\n",
        "def fit_eval_lstm(\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    lookback=26,\n",
        "    class_weight: dict | None = None,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Scale on train only, make sequences, train LSTM, evaluate on val.\n",
        "    Returns (model, metrics_dict, (pred, proba), scaler).\n",
        "    \"\"\"\n",
        "    # Clean and scale\n",
        "    train_df = train_df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    val_df   = val_df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    scaler = StandardScaler().fit(train_df[feature_cols])\n",
        "    tr = train_df.copy(); va = val_df.copy()\n",
        "    tr[feature_cols] = scaler.transform(tr[feature_cols])\n",
        "    va[feature_cols] = scaler.transform(va[feature_cols])\n",
        "\n",
        "    # Sequences\n",
        "    X_tr, y_tr = make_sequences(tr, lookback, feature_cols, target_col)\n",
        "    X_va, y_va = make_sequences(va, lookback, feature_cols, target_col)\n",
        "\n",
        "    # Guard: not enough data to form sequences\n",
        "    if len(X_tr) == 0 or len(X_va) == 0:\n",
        "        raise ValueError(\"Not enough rows to form sequences. Reduce lookback or provide more data.\")\n",
        "\n",
        "    # Model\n",
        "    model = build_lstm(input_dim=X_tr.shape[2], lookback=lookback)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=(X_va, y_va),\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=class_weight\n",
        "    )\n",
        "\n",
        "    proba = model.predict(X_va, verbose=0).ravel()\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": float(accuracy_score(y_va, pred)),\n",
        "        \"f1_macro\": float(f1_score(y_va, pred, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_va, proba)) if len(np.unique(y_va)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_va, proba),                 # âˆšBrier on probabilities\n",
        "        \"rmse_hard\": rmse(y_va, pred.astype(float)),    # sqrt(error rate) on hard labels\n",
        "        \"classification_report\": classification_report(y_va, pred, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_va, pred).tolist()\n",
        "    }\n",
        "    return model, metrics, (pred, proba), scaler\n",
        "\n",
        "# ----------------------------\n",
        "# End-to-end runners (LSTM only)\n",
        "# ----------------------------\n",
        "def run_holdout_lstm(\n",
        "    df: pd.DataFrame,\n",
        "    test_start: str | pd.Timestamp,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    lookback=26,\n",
        "    class_weight: dict | None = None,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Split Train/Val/Test chronologically (hold-out; df already has 'target')\n",
        "    2) Train on Train, tune on Val\n",
        "    3) Retrain on Train+Val and evaluate on Test (no peeking at Test)\n",
        "    Returns dict with validation metrics, test metrics, and split boundaries.\n",
        "    \"\"\"\n",
        "    # Sort and basic cleaning (drop rows missing features or target)\n",
        "    df = df.sort_index()\n",
        "    df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    # Train/Val/Test chronological split\n",
        "    trainval, test = make_static_splits(df, test_start)\n",
        "\n",
        "    # carve validation from tail of trainval\n",
        "    VAL_SPAN = min(52, max(26, len(trainval)//10))  # ~10% bounded into weekly range\n",
        "    val = trainval.iloc[-VAL_SPAN:].copy()\n",
        "    train = trainval.iloc[:-VAL_SPAN].copy()\n",
        "\n",
        "    # ---- validation round (select hyperparams externally if needed)\n",
        "    _, val_metrics, _, _ = fit_eval_lstm(\n",
        "        train, val,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        lookback=lookback,\n",
        "        class_weight=class_weight,\n",
        "        epochs=epochs, batch_size=batch_size, patience=patience\n",
        "    )\n",
        "\n",
        "    # ---- final training on Train+Val, evaluate on Test (no test peeking)\n",
        "    # Scale on Train+Val\n",
        "    trv = pd.concat([train, val]).copy()\n",
        "    trv = trv.dropna(subset=feature_cols + [target_col])\n",
        "    test = test.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "    scaler = StandardScaler().fit(trv[feature_cols])\n",
        "    trv_scaled  = trv.copy();  trv_scaled[feature_cols]  = scaler.transform(trv_scaled[feature_cols])\n",
        "    test_scaled = test.copy(); test_scaled[feature_cols] = scaler.transform(test_scaled[feature_cols])\n",
        "\n",
        "    # Make sequences\n",
        "    X_trv, y_trv = make_sequences(trv_scaled,  lookback, feature_cols, target_col)\n",
        "    X_te,  y_te  = make_sequences(test_scaled, lookback, feature_cols, target_col)\n",
        "\n",
        "    if len(X_trv) == 0 or len(X_te) == 0:\n",
        "        raise ValueError(\"Not enough sequence data in Train+Val or Test. Adjust lookback or splits.\")\n",
        "\n",
        "    # Set aside a small tail of Train+Val sequences for early-stopping monitor (still not using Test)\n",
        "    monitor_k = max( min( max(len(X_trv)//10, 32), 256 ), 16 )  # between 16 and 256, ~10% of trv\n",
        "    monitor_k = min(monitor_k, len(X_trv)//5) if len(X_trv) >= 5 else 0\n",
        "    if monitor_k == 0:\n",
        "        X_trv_train, y_trv_train = X_trv, y_trv\n",
        "        val_monitor = None\n",
        "    else:\n",
        "        X_trv_train, y_trv_train = X_trv[:-monitor_k], y_trv[:-monitor_k]\n",
        "        X_trv_valm, y_trv_valm   = X_trv[-monitor_k:], y_trv[-monitor_k:]\n",
        "        val_monitor = (X_trv_valm, y_trv_valm)\n",
        "\n",
        "    model = build_lstm(input_dim=X_trv.shape[2], lookback=lookback)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "\n",
        "    model.fit(\n",
        "        X_trv_train, y_trv_train,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=val_monitor,\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=class_weight\n",
        "    )\n",
        "\n",
        "    # Evaluate on the entire Test set\n",
        "    proba_te = model.predict(X_te, verbose=0).ravel()\n",
        "    pred_te  = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    test_metrics = {\n",
        "        \"acc\": float(accuracy_score(y_te, pred_te)),\n",
        "        \"f1_macro\": float(f1_score(y_te, pred_te, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_te, proba_te)) if len(np.unique(y_te)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_te, proba_te),                 # âˆšBrier on probabilities\n",
        "        \"rmse_hard\": rmse(y_te, pred_te.astype(float)),    # sqrt(error rate) on hard labels\n",
        "        \"classification_report\": classification_report(y_te, pred_te, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_te, pred_te).tolist()\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"splits\": {\n",
        "            \"train\": (train.index.min(), train.index.max()),\n",
        "            \"val\":   (val.index.min(),   val.index.max()),\n",
        "            \"test\":  (test.index.min(),  test.index.max())\n",
        "        },\n",
        "        \"lookback\": lookback\n",
        "    }\n",
        "\n",
        "def run_walkforward_lstm(\n",
        "    df: pd.DataFrame,\n",
        "    cfg: CVConfig = CVConfig(val_span=52, step=26, min_train_span=156),\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    lookback=26,\n",
        "    class_weight: dict | None = None,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Expanding-window CV for LSTM. Assumes df already has 'target'.\n",
        "    Returns average metrics across folds and per-fold metrics list.\n",
        "    \"\"\"\n",
        "    df = df.sort_index()\n",
        "    df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    fold_metrics = []\n",
        "    for df_tr, df_va in expanding_cv_folds(df, cfg):\n",
        "        # Safety: dropna per fold\n",
        "        df_tr = df_tr.dropna(subset=feature_cols + [target_col])\n",
        "        df_va = df_va.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "        _, m, _, _ = fit_eval_lstm(\n",
        "            df_tr, df_va,\n",
        "            feature_cols=feature_cols, target_col=target_col,\n",
        "            lookback=lookback,\n",
        "            class_weight=class_weight,\n",
        "            epochs=epochs, batch_size=batch_size, patience=patience\n",
        "        )\n",
        "        fold_metrics.append(m)\n",
        "\n",
        "    def avg(key):\n",
        "        vals = [fm.get(key, np.nan) for fm in fold_metrics if not np.isnan(fm.get(key, np.nan))]\n",
        "        return float(np.mean(vals)) if vals else np.nan\n",
        "\n",
        "    return {\n",
        "        \"avg_metrics\": {\n",
        "            \"acc\": avg(\"acc\"),\n",
        "            \"f1_macro\": avg(\"f1_macro\"),\n",
        "            \"roc_auc\": avg(\"roc_auc\"),\n",
        "            \"rmse_prob\": avg(\"rmse_prob\"),\n",
        "            \"rmse_hard\": avg(\"rmse_hard\"),\n",
        "        },\n",
        "        \"folds\": fold_metrics,\n",
        "        \"n_folds\": len(fold_metrics),\n",
        "        \"cfg\": cfg,\n",
        "        \"lookback\": lookback\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXIsVSFyrKjE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Example usage (uncomment)\n",
        "# ----------------------------\n",
        "selected_features = [\"Last Price\"]\n",
        "data.index = pd.to_datetime(data[\"date\"])\n",
        "data = data.sort_index()\n",
        "#\n",
        "# # Hold-out\n",
        "result_holdout = run_holdout_lstm(\n",
        "     data, test_start=\"2020-01-01\",\n",
        "     feature_cols=selected_features, target_col=\"target\",\n",
        "     lookback=26, epochs=50, batch_size=64\n",
        " )\n",
        "print(result_holdout[\"val_metrics\"])\n",
        "print(result_holdout[\"test_metrics\"])\n",
        "\n",
        "# # Walk-forward CV\n",
        "result_wf = run_walkforward_lstm(\n",
        "    data, cfg=CVConfig(val_span=52, step=26, min_train_span=156),\n",
        "    feature_cols=selected_features, target_col=\"target\",\n",
        "    lookback=26, epochs=50, batch_size=64\n",
        ")\n",
        "print(result_wf[\"avg_metrics\"], \"folds:\", result_wf[\"n_folds\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7u2n14VrWRI",
        "outputId": "bbf1e5d4-58a3-4332-8f2a-2e2305e13b8c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc': 0.46153846153846156, 'f1_macro': 0.3680555555555556, 'roc_auc': 0.4166666666666667, 'rmse_prob': 0.5006786232965725, 'rmse_hard': 0.7337993857053428, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.500     0.786     0.611        14\\n           1      0.250     0.083     0.125        12\\n\\n    accuracy                          0.462        26\\n   macro avg      0.375     0.435     0.368        26\\nweighted avg      0.385     0.462     0.387        26\\n', 'confusion_matrix': [[11, 3], [11, 1]]}\n",
            "{'acc': 0.47924528301886793, 'f1_macro': 0.3239795918367347, 'roc_auc': 0.5665867853474837, 'rmse_prob': 0.5003197024704186, 'rmse_hard': 0.721633367424991, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.479     1.000     0.648       127\\n           1      0.000     0.000     0.000       138\\n\\n    accuracy                          0.479       265\\n   macro avg      0.240     0.500     0.324       265\\nweighted avg      0.230     0.479     0.311       265\\n', 'confusion_matrix': [[127, 0], [138, 0]]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7dfaa238da80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7dfaa10e3b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc': 0.55, 'f1_macro': 0.4073415313577481, 'roc_auc': 0.5281454501507639, 'rmse_prob': 0.49775749241007233, 'rmse_hard': 0.6674536854020207} folds: 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_synthetic_lstm_dataset(\n",
        "    n=1500,\n",
        "    seed=7,\n",
        "    start_date=\"1990-01-05\",\n",
        "    freq=\"W-FRI\",\n",
        "    p_stick=0.97,                 # state persistence (higher = easier)\n",
        "    mu=(-0.02, 0.0, 0.02),        # per-state drift applied to NEXT return\n",
        "    sigma=0.004,                  # noise on returns (lower = easier)\n",
        "    prob_noise=0.02               # noise on p_state probabilities (lower = easier)\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with columns:\n",
        "      ['ret_1','Last Price','p_state0','p_state1','p_state2','spi_6','target']\n",
        "    Index is a DatetimeIndex. 'target' = 1 if next-period return > 0, else 0.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    dates = pd.date_range(start_date, periods=n, freq=freq)\n",
        "    K = 3  # dry/neutral/wet\n",
        "\n",
        "    # Highly persistent Markov chain\n",
        "    trans = np.full((K, K), (1 - p_stick) / (K - 1))\n",
        "    np.fill_diagonal(trans, p_stick)\n",
        "\n",
        "    s = np.zeros(n, dtype=int)\n",
        "    for t in range(1, n):\n",
        "        s[t] = rng.choice(K, p=trans[s[t-1]])\n",
        "\n",
        "    # Near one-hot state probabilities with tiny noise\n",
        "    p_states = np.full((n, K), prob_noise / (K - 1))\n",
        "    p_states[np.arange(n), s] = 1 - prob_noise\n",
        "\n",
        "    # SPI correlated with state\n",
        "    spi_means = np.array([-1.0, 0.0, 1.0])\n",
        "    spi = spi_means[s] + rng.normal(0, 0.2, size=n)\n",
        "\n",
        "    # Returns: r_t depends on PREVIOUS state s_{t-1}  -> avoids leakage in your windowing\n",
        "    r = np.zeros(n)\n",
        "    for t in range(1, n):\n",
        "        r[t] = mu[s[t-1]] + rng.normal(0, sigma)\n",
        "\n",
        "    # Price path from log-returns (approx)\n",
        "    price = 100 * np.exp(np.cumsum(r))\n",
        "\n",
        "    # target_t = 1[ r_{t+1} > 0 ]  (binary next-period direction)\n",
        "    target = (pd.Series(r).shift(-1) > 0).astype(\"float\").to_numpy()\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"date\": dates,\n",
        "        \"return\": r,\n",
        "        \"Last Price\": price,\n",
        "        \"p_state0\": p_states[:, 0],\n",
        "        \"p_state1\": p_states[:, 1],\n",
        "        \"p_state2\": p_states[:, 2],\n",
        "        \"spi\": spi,\n",
        "        \"target\": target\n",
        "    }).set_index(\"date\")\n",
        "\n",
        "    # Drop last row where target is NaN (no next period)\n",
        "    df = df.dropna(subset=[\"target\"]).copy()\n",
        "    df[\"target\"] = df[\"target\"].astype(int)\n",
        "    return df\n",
        "\n",
        "# 1) Make data\n",
        "df_syn = generate_synthetic_lstm_dataset()\n",
        "\n",
        "# 2) (Optional) only certain features? e.g., use all default ones:\n",
        "FEATURES_SYN = [\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi\"]\n",
        "\n",
        "# 3) Run your existing functions:\n",
        "# df_syn.index already set; it's sorted\n",
        "res = run_holdout_lstm(\n",
        "    df_syn,\n",
        "    test_start=\"2018-01-05\",      # pick a split date inside the range\n",
        "    feature_cols=FEATURES_SYN,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    epochs=40,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "print(\"VAL:\", res[\"val_metrics\"])\n",
        "print(\"TEST:\", res[\"test_metrics\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buUOoXYbwCUU",
        "outputId": "dc640280-81b9-4b51-99b8-aa7619530136"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL: {'acc': 0.8846153846153846, 'f1_macro': 0.46938775510204084, 'roc_auc': 0.8405797101449275, 'rmse_prob': 0.28764915650110395, 'rmse_hard': 0.3396831102433787, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.000     0.000     0.000         3\\n           1      0.885     1.000     0.939        23\\n\\n    accuracy                          0.885        26\\n   macro avg      0.442     0.500     0.469        26\\nweighted avg      0.783     0.885     0.830        26\\n', 'confusion_matrix': [[0, 3], [0, 23]]}\n",
            "TEST: {'acc': 0.9230769230769231, 'f1_macro': 0.48, 'roc_auc': 0.5833333333333334, 'rmse_prob': 0.27237596932814634, 'rmse_hard': 0.2773500981126146, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.000     0.000     0.000         1\\n           1      0.923     1.000     0.960        12\\n\\n    accuracy                          0.923        13\\n   macro avg      0.462     0.500     0.480        13\\nweighted avg      0.852     0.923     0.886        13\\n', 'confusion_matrix': [[0, 1], [0, 12]]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Logistic-Regression-only pipeline for binary next-period direction\n",
        "# Assumes your DataFrame ALREADY contains the binary target column.\n",
        "#\n",
        "# Expected columns at minimum:\n",
        "#   [\"date\",\"return\",\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi\",\"target\"]\n",
        "# Set a datetime index and sort ascending before running:\n",
        "#   df.index = pd.to_datetime(df[\"date\"]); df = df.sort_index()\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ----------------------------\n",
        "# Config and splitting helpers\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class CVConfig:\n",
        "    val_span: int        # e.g., 52 (weeks)\n",
        "    step: int            # e.g., 26\n",
        "    min_train_span: int  # e.g., 156\n",
        "\n",
        "FEATURES = [\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi_6\"]\n",
        "\n",
        "def make_static_splits(df: pd.DataFrame, test_start: str | pd.Timestamp):\n",
        "    \"\"\"\n",
        "    Chronological hold-out split: all dates >= test_start go to test.\n",
        "    The remainder will later be split into train/val.\n",
        "    \"\"\"\n",
        "    mask_test = df.index >= pd.to_datetime(test_start)\n",
        "    return df.loc[~mask_test].copy(), df.loc[mask_test].copy()\n",
        "\n",
        "def expanding_cv_folds(trainval_df: pd.DataFrame, cfg: CVConfig):\n",
        "    \"\"\"\n",
        "    Walk-forward (expanding-window) CV generator over train/val pairs.\n",
        "    \"\"\"\n",
        "    n = len(trainval_df); idx = np.arange(n)\n",
        "    train_end = cfg.min_train_span\n",
        "    while True:\n",
        "        val_end = train_end + cfg.val_span\n",
        "        if val_end > n: break\n",
        "        train_mask = idx < train_end\n",
        "        val_mask   = (idx >= train_end) & (idx < val_end)\n",
        "        yield trainval_df.loc[train_mask].copy(), trainval_df.loc[val_mask].copy()\n",
        "        train_end += cfg.step\n",
        "\n",
        "# ----------------------------\n",
        "# Metrics\n",
        "# ----------------------------\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Root Mean Squared Error. For classification, pass probabilities for âˆšBrier.\"\"\"\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "\n",
        "# ----------------------------\n",
        "# Logistic Regression model\n",
        "# ----------------------------\n",
        "def fit_eval_logreg(\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    class_weight=\"balanced\",           # or dict, or None\n",
        "    C=1.0,                             # inverse regularization strength\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Scale on train only, fit Logistic Regression, evaluate on val.\n",
        "    Returns (model, metrics_dict, (pred, proba), scaler).\n",
        "    \"\"\"\n",
        "    # Clean\n",
        "    train_df = train_df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    val_df   = val_df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    y_tr = train_df[target_col].astype(int).values\n",
        "    y_va = val_df[target_col].astype(int).values\n",
        "    if len(np.unique(y_tr)) < 2:\n",
        "        raise ValueError(\"Training split contains only one class; LogisticRegression needs at least 2 classes.\")\n",
        "\n",
        "    # Scale on train only\n",
        "    scaler = StandardScaler().fit(train_df[feature_cols])\n",
        "    X_tr = scaler.transform(train_df[feature_cols])\n",
        "    X_va = scaler.transform(val_df[feature_cols])\n",
        "\n",
        "    # Model\n",
        "    clf = LogisticRegression(\n",
        "        class_weight=class_weight,\n",
        "        C=C,\n",
        "        penalty=penalty,\n",
        "        solver=solver,\n",
        "        max_iter=max_iter\n",
        "    )\n",
        "    clf.fit(X_tr, y_tr)\n",
        "\n",
        "    # Probabilities (index of positive class=1)\n",
        "    if 1 in clf.classes_:\n",
        "        pos_idx = list(clf.classes_).index(1)\n",
        "        proba = clf.predict_proba(X_va)[:, pos_idx]\n",
        "    else:\n",
        "        # Shouldn't happen if y_tr had both classes, but guard anyway\n",
        "        proba = np.zeros_like(y_va, dtype=float)\n",
        "\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": float(accuracy_score(y_va, pred)),\n",
        "        \"f1_macro\": float(f1_score(y_va, pred, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_va, proba)) if len(np.unique(y_va)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_va, proba),                 # âˆšBrier on probabilities\n",
        "        \"rmse_hard\": rmse(y_va, pred.astype(float)),    # sqrt(error rate) on hard labels\n",
        "        \"classification_report\": classification_report(y_va, pred, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_va, pred).tolist()\n",
        "    }\n",
        "    return clf, metrics, (pred, proba), scaler\n",
        "\n",
        "# ----------------------------\n",
        "# End-to-end runners (LogReg only)\n",
        "# ----------------------------\n",
        "def run_holdout_logreg(\n",
        "    df: pd.DataFrame,\n",
        "    test_start: str | pd.Timestamp,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    class_weight=\"balanced\",\n",
        "    C=1.0,\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000,\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Split Train/Val/Test chronologically (hold-out; df already has 'target')\n",
        "    2) Train on Train, tune on Val\n",
        "    3) Retrain on Train+Val and evaluate on Test\n",
        "    Returns dict with validation metrics, test metrics, and split boundaries.\n",
        "    \"\"\"\n",
        "    # Sort and basic cleaning\n",
        "    df = df.sort_index()\n",
        "    df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    # Train/Val/Test split\n",
        "    trainval, test = make_static_splits(df, test_start)\n",
        "\n",
        "    # carve validation from tail of trainval\n",
        "    VAL_SPAN = min(52, max(26, len(trainval)//10))  # ~10% bounded into weekly-ish range\n",
        "    val = trainval.iloc[-VAL_SPAN:].copy()\n",
        "    train = trainval.iloc[:-VAL_SPAN].copy()\n",
        "\n",
        "    # ---- validation round\n",
        "    _, val_metrics, _, _ = fit_eval_logreg(\n",
        "        train, val,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        class_weight=class_weight, C=C, penalty=penalty, solver=solver, max_iter=max_iter\n",
        "    )\n",
        "\n",
        "    # ---- final training on Train+Val, evaluate on Test\n",
        "    trv = pd.concat([train, val]).copy()\n",
        "    trv = trv.dropna(subset=feature_cols + [target_col])\n",
        "    test = test.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "    y_trv = trv[target_col].astype(int).values\n",
        "    if len(np.unique(y_trv)) < 2:\n",
        "        raise ValueError(\"Train+Val contains only one class; cannot fit LogisticRegression.\")\n",
        "\n",
        "    scaler = StandardScaler().fit(trv[feature_cols])\n",
        "    X_trv = scaler.transform(trv[feature_cols]); y_trv = trv[target_col].astype(int).values\n",
        "    X_te  = scaler.transform(test[feature_cols]); y_te  = test[target_col].astype(int).values\n",
        "\n",
        "    clf = LogisticRegression(\n",
        "        class_weight=class_weight, C=C, penalty=penalty, solver=solver, max_iter=max_iter\n",
        "    ).fit(X_trv, y_trv)\n",
        "\n",
        "    if 1 in clf.classes_:\n",
        "        pos_idx = list(clf.classes_).index(1)\n",
        "        proba_te = clf.predict_proba(X_te)[:, pos_idx]\n",
        "    else:\n",
        "        proba_te = np.zeros_like(y_te, dtype=float)\n",
        "\n",
        "    pred_te  = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    test_metrics = {\n",
        "        \"acc\": float(accuracy_score(y_te, pred_te)),\n",
        "        \"f1_macro\": float(f1_score(y_te, pred_te, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_te, proba_te)) if len(np.unique(y_te)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_te, proba_te),                 # âˆšBrier on probabilities\n",
        "        \"rmse_hard\": rmse(y_te, pred_te.astype(float)),    # sqrt(error rate) on hard labels\n",
        "        \"classification_report\": classification_report(y_te, pred_te, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_te, pred_te).tolist()\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"splits\": {\n",
        "            \"train\": (train.index.min(), train.index.max()),\n",
        "            \"val\":   (val.index.min(),   val.index.max()),\n",
        "            \"test\":  (test.index.min(),  test.index.max())\n",
        "        }\n",
        "    }\n",
        "\n",
        "def run_walkforward_logreg(\n",
        "    df: pd.DataFrame,\n",
        "    cfg: CVConfig = CVConfig(val_span=52, step=26, min_train_span=156),\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    class_weight=\"balanced\",\n",
        "    C=1.0,\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Expanding-window CV for Logistic Regression. Assumes df already has 'target'.\n",
        "    Returns average metrics across folds and per-fold metrics list.\n",
        "    \"\"\"\n",
        "    df = df.sort_index()\n",
        "    df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    fold_metrics = []\n",
        "    for df_tr, df_va in expanding_cv_folds(df, cfg):\n",
        "        df_tr = df_tr.dropna(subset=feature_cols + [target_col])\n",
        "        df_va = df_va.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "        # skip fold if training has <2 classes\n",
        "        if len(np.unique(df_tr[target_col].astype(int).values)) < 2:\n",
        "            continue\n",
        "\n",
        "        _, m, _, _ = fit_eval_logreg(\n",
        "            df_tr, df_va,\n",
        "            feature_cols=feature_cols, target_col=target_col,\n",
        "            class_weight=class_weight, C=C, penalty=penalty, solver=solver, max_iter=max_iter\n",
        "        )\n",
        "        fold_metrics.append(m)\n",
        "\n",
        "    def avg(key):\n",
        "        vals = [fm.get(key, np.nan) for fm in fold_metrics if not np.isnan(fm.get(key, np.nan))]\n",
        "        return float(np.mean(vals)) if vals else np.nan\n",
        "\n",
        "    return {\n",
        "        \"avg_metrics\": {\n",
        "            \"acc\": avg(\"acc\"),\n",
        "            \"f1_macro\": avg(\"f1_macro\"),\n",
        "            \"roc_auc\": avg(\"roc_auc\"),\n",
        "            \"rmse_prob\": avg(\"rmse_prob\"),\n",
        "            \"rmse_hard\": avg(\"rmse_hard\"),\n",
        "        },\n",
        "        \"folds\": fold_metrics,\n",
        "        \"n_folds\": len(fold_metrics),\n",
        "        \"cfg\": cfg\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (uncomment)\n",
        "# ----------------------------\n",
        "data.index = pd.to_datetime(data[\"date\"])\n",
        "data = data.sort_index()\n",
        "#\n",
        "selected_features = [\"Last Price\"]\n",
        "# # Hold-out\n",
        "result_holdout = run_holdout_logreg(\n",
        "    data, test_start=\"2020-01-01\",\n",
        "    feature_cols=[\"Last Price\", \"ret_1\", \"spi_6\"],\n",
        "    target_col=\"target\",\n",
        "    class_weight=\"balanced\", C=1.0, penalty=\"l2\", solver=\"lbfgs\"\n",
        ")\n",
        "print(result_holdout[\"val_metrics\"])\n",
        "print(result_holdout[\"test_metrics\"])\n",
        "#\n",
        "# # Walk-forward CV\n",
        "# result_wf = run_walkforward_logreg(\n",
        "#     df, cfg=CVConfig(val_span=52, step=26, min_train_span=156),\n",
        "#     feature_cols=[\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi\"],\n",
        "#     target_col=\"target\",\n",
        "#     class_weight=\"balanced\", C=1.0, penalty=\"l2\", solver=\"lbfgs\"\n",
        "# )\n",
        "# print(result_wf[\"avg_metrics\"], \"folds:\", result_wf[\"n_folds\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnmSFe6Z2ExO",
        "outputId": "7dd28add-122e-4016-a21f-6009d59147e0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc': 0.4807692307692308, 'f1_macro': 0.47592385218365063, 'roc_auc': 0.4851190476190476, 'rmse_prob': 0.5002640895967102, 'rmse_hard': 0.7205766921228921, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.526     0.357     0.426        28\\n           1      0.455     0.625     0.526        24\\n\\n    accuracy                          0.481        52\\n   macro avg      0.490     0.491     0.476        52\\nweighted avg      0.493     0.481     0.472        52\\n', 'confusion_matrix': [[10, 18], [9, 15]]}\n",
            "{'acc': 0.5017182130584192, 'f1_macro': 0.47959447959447954, 'roc_auc': 0.4743904743904744, 'rmse_prob': 0.5007000969151711, 'rmse_hard': 0.7058907755039591, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.495     0.720     0.587       143\\n           1      0.518     0.291     0.372       148\\n\\n    accuracy                          0.502       291\\n   macro avg      0.507     0.505     0.480       291\\nweighted avg      0.507     0.502     0.478       291\\n', 'confusion_matrix': [[103, 40], [105, 43]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LSTM (single hold-out: Train -> Val -> retrain on Train+Val -> Test)\n",
        "# Assumes df already has a binary 'target' column and a datetime index.\n",
        "#\n",
        "# - Train on full Train, validate on Val (choose threshold on Val)\n",
        "# - Retrain on Train+Val (no test peeking), evaluate once on Test\n",
        "# - Includes RMSE metrics (âˆšBrier on probabilities and RMSE on hard labels)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# --------- Configurable features ---------\n",
        "FEATURES = [\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi_6\"]\n",
        "\n",
        "# --------- Split & metrics helpers ---------\n",
        "def make_static_splits(df: pd.DataFrame, test_start: str|pd.Timestamp):\n",
        "    m = df.index >= pd.to_datetime(test_start)\n",
        "    return df.loc[~m].copy(), df.loc[m].copy()\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
        "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "\n",
        "def best_threshold(y_true, proba, metric=\"f1_macro\"):\n",
        "    grid = np.linspace(0.05, 0.95, 181)\n",
        "    best_t, best = 0.5, -1.0\n",
        "    for t in grid:\n",
        "        pred = (proba >= t).astype(int)\n",
        "        score = f1_score(y_true, pred, average=\"macro\") if metric==\"f1_macro\" else f1_score(y_true, pred)\n",
        "        if score > best:\n",
        "            best, best_t = score, t\n",
        "    return float(best_t)\n",
        "\n",
        "def safe_logloss(y_true, proba):\n",
        "    # clip to avoid log(0) under any version\n",
        "    p = np.clip(np.asarray(proba, dtype=float), 1e-7, 1-1e-7)\n",
        "    return float(log_loss(y_true, p, labels=[0, 1]))\n",
        "\n",
        "def _class_weight_from_labels(y):\n",
        "    # inverse-frequency weights normalized s.t. average weight â‰ˆ 1\n",
        "    y = np.asarray(y, int)\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    K = len(classes); total = counts.sum()\n",
        "    return {int(c): float(total/(K*cnt)) for c, cnt in zip(classes, counts)}\n",
        "\n",
        "# --------- Sequences & model ---------\n",
        "def make_sequences(df: pd.DataFrame, lookback: int, feature_cols, target_col=\"target\"):\n",
        "    \"\"\"\n",
        "    Build (X, y) where each X[i] is a window of length `lookback` ending at t-1,\n",
        "    and y[i] is the target at time t (no look-ahead).\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    V = df[feature_cols].values\n",
        "    t = df[target_col].values\n",
        "    for i in range(lookback, len(df)):\n",
        "        X.append(V[i-lookback:i])\n",
        "        y.append(t[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_lstm(input_dim, lookback, units=64, dropout=0.2, lr=1e-3):\n",
        "    m = keras.Sequential([\n",
        "        layers.Input(shape=(lookback, input_dim)),\n",
        "        layers.LSTM(units, return_sequences=False),\n",
        "        layers.Dropout(dropout),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    m.compile(optimizer=keras.optimizers.Adam(lr),\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "    return m\n",
        "\n",
        "# --------- Core training / evaluation blocks ---------\n",
        "def fit_lstm_train_val(\n",
        "    df_train: pd.DataFrame,\n",
        "    df_val: pd.DataFrame,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    class_weight=\"balanced\",  # \"balanced\" -> auto inverse-freq; or pass a dict {0:w0,1:w1}; or None\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Fit scaler on TRAIN, train LSTM on TRAIN sequences with early-stopping on VAL sequences.\n",
        "    Return: model, scaler, y_val_seq, proba_val, val_metrics_at_0p5\n",
        "    \"\"\"\n",
        "    # Clean\n",
        "    df_train = df_train.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    df_val   = df_val.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    # Scale on TRAIN only\n",
        "    scaler = StandardScaler().fit(df_train[feature_cols])\n",
        "    tr = df_train.copy(); va = df_val.copy()\n",
        "    tr[feature_cols] = scaler.transform(tr[feature_cols])\n",
        "    va[feature_cols] = scaler.transform(va[feature_cols])\n",
        "\n",
        "    # Sequences\n",
        "    X_tr, y_tr = make_sequences(tr, lookback, feature_cols, target_col)\n",
        "    X_va, y_va = make_sequences(va, lookback, feature_cols, target_col)\n",
        "    if len(X_tr) == 0 or len(X_va) == 0:\n",
        "        raise ValueError(\"Not enough data to form sequences. Reduce lookback or provide more rows.\")\n",
        "\n",
        "    # Class weights\n",
        "    cw = None\n",
        "    if class_weight == \"balanced\":\n",
        "        cw = _class_weight_from_labels(y_tr)\n",
        "    elif isinstance(class_weight, dict):\n",
        "        cw = class_weight\n",
        "\n",
        "    # Train\n",
        "    model = build_lstm(input_dim=X_tr.shape[2], lookback=lookback)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=(X_va, y_va),\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=cw\n",
        "    )\n",
        "\n",
        "    # Validation outputs & metrics at threshold 0.5\n",
        "    proba_va = model.predict(X_va, verbose=0).ravel()\n",
        "    pred_va  = (proba_va >= 0.5).astype(int)\n",
        "    val_metrics_0p5 = {\n",
        "        \"acc\": float(accuracy_score(y_va, pred_va)),\n",
        "        \"f1_macro\": float(f1_score(y_va, pred_va, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_va, proba_va)) if len(np.unique(y_va)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_va, proba_va),\n",
        "        \"rmse_hard\": rmse(y_va, pred_va.astype(float)),\n",
        "        \"classification_report\": classification_report(y_va, pred_va, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_va, pred_va).tolist()\n",
        "    }\n",
        "    return model, scaler, y_va, proba_va, val_metrics_0p5\n",
        "\n",
        "def fit_lstm_final_on_trv(\n",
        "    df_trv: pd.DataFrame,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    class_weight=\"balanced\",\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrain LSTM on TRAIN+VAL with an internal holdout (from the tail of trv sequences) for early-stopping.\n",
        "    \"\"\"\n",
        "    df_trv = df_trv.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    scaler = StandardScaler().fit(df_trv[feature_cols])\n",
        "    trv = df_trv.copy()\n",
        "    trv[feature_cols] = scaler.transform(trv[feature_cols])\n",
        "\n",
        "    X_trv, y_trv = make_sequences(trv, lookback, feature_cols, target_col)\n",
        "    if len(X_trv) == 0:\n",
        "        raise ValueError(\"Not enough data in Train+Val to form sequences. Reduce lookback.\")\n",
        "\n",
        "    # small internal validation from tail of Train+Val (no Test peeking)\n",
        "    monitor_k = max( min( max(len(X_trv)//10, 32), 256 ), 16 )\n",
        "    monitor_k = min(monitor_k, len(X_trv)//5) if len(X_trv) >= 5 else 0\n",
        "    if monitor_k > 0:\n",
        "        X_trv_train, y_trv_train = X_trv[:-monitor_k], y_trv[:-monitor_k]\n",
        "        X_trv_valm,  y_trv_valm  = X_trv[-monitor_k:], y_trv[-monitor_k:]\n",
        "        val_monitor = (X_trv_valm, y_trv_valm)\n",
        "    else:\n",
        "        X_trv_train, y_trv_train = X_trv, y_trv\n",
        "        val_monitor = None\n",
        "\n",
        "    cw = None\n",
        "    if class_weight == \"balanced\":\n",
        "        cw = _class_weight_from_labels(y_trv_train)\n",
        "    elif isinstance(class_weight, dict):\n",
        "        cw = class_weight\n",
        "\n",
        "    model = build_lstm(input_dim=X_trv.shape[2], lookback=lookback)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    model.fit(\n",
        "        X_trv_train, y_trv_train,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=val_monitor,\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=cw\n",
        "    )\n",
        "    return model, scaler\n",
        "\n",
        "def eval_lstm_on_split(model, scaler, df_split, feature_cols=FEATURES, target_col=\"target\", lookback=26, threshold=0.5):\n",
        "    df_split = df_split.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    df_split[feature_cols] = scaler.transform(df_split[feature_cols])\n",
        "\n",
        "    X, y = make_sequences(df_split, lookback, feature_cols, target_col)\n",
        "    if len(X) == 0:\n",
        "        raise ValueError(\"Not enough data in this split to form sequences at the chosen lookback.\")\n",
        "    proba = model.predict(X, verbose=0).ravel()\n",
        "    pred  = (proba >= threshold).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": float(accuracy_score(y, pred)),\n",
        "        \"f1_macro\": float(f1_score(y, pred, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y, proba)) if len(np.unique(y)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y, proba),\n",
        "        \"rmse_hard\": rmse(y, pred.astype(float)),\n",
        "        \"classification_report\": classification_report(y, pred, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y, pred).tolist()\n",
        "    }\n",
        "    return metrics, y, proba, pred\n",
        "\n",
        "# --------- End-to-end (no CV; Train -> Val -> retrain -> Test) ---------\n",
        "def run_holdout_lstm_single(\n",
        "    df: pd.DataFrame,\n",
        "    test_start: str|pd.Timestamp,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    val_span: int|None = None,      # if None -> ~10% bounded in [26, 52]\n",
        "    class_weight=\"balanced\",        # \"balanced\", dict, or None\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5,\n",
        "    tune_threshold: bool = True,\n",
        "    tune_metric: str = \"f1_macro\"   # which metric to optimize when picking threshold\n",
        "):\n",
        "    # 0) Sort; basic cleaning deferred to child funcs to avoid accidental leakage\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # 1) Train/Val/Test split\n",
        "    trainval, test = make_static_splits(df, test_start)\n",
        "    if val_span is None:\n",
        "        val_span = min(52, max(26, len(trainval)//10))\n",
        "    val   = trainval.iloc[-val_span:].copy()\n",
        "    train = trainval.iloc[:-val_span].copy()\n",
        "\n",
        "    # 2) Train on full Train, validate on Val (get raw proba for threshold tuning)\n",
        "    model_tr, scaler_tr, y_va, proba_va, val_metrics_0p5 = fit_lstm_train_val(\n",
        "        train, val,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        lookback=lookback,\n",
        "        class_weight=class_weight,\n",
        "        epochs=epochs, batch_size=batch_size, patience=patience\n",
        "    )\n",
        "\n",
        "    # 3) Choose threshold on Val (optional)\n",
        "    t_star = 0.5\n",
        "    if tune_threshold:\n",
        "        t_star = best_threshold(y_va, proba_va, metric=tune_metric)\n",
        "\n",
        "    # Recompute Val metrics at tuned threshold for reporting\n",
        "    # (reuse trained model_tr + scaler_tr to avoid re-fitting)\n",
        "    df_val_scaled = val.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    df_val_scaled[feature_cols] = scaler_tr.transform(df_val_scaled[feature_cols])\n",
        "    X_va2, y_va2 = make_sequences(df_val_scaled, lookback, feature_cols, target_col)\n",
        "    proba_va2 = model_tr.predict(X_va2, verbose=0).ravel()\n",
        "    pred_va2  = (proba_va2 >= t_star).astype(int)\n",
        "    val_metrics_tuned = {\n",
        "        \"acc\": float(accuracy_score(y_va2, pred_va2)),\n",
        "        \"f1_macro\": float(f1_score(y_va2, pred_va2, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_va2, proba_va2)) if len(np.unique(y_va2)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_va2, proba_va2),\n",
        "        \"rmse_hard\": rmse(y_va2, pred_va2.astype(float)),\n",
        "        \"classification_report\": classification_report(y_va2, pred_va2, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_va2, pred_va2).tolist()\n",
        "    }\n",
        "\n",
        "    # 4) Retrain on Train+Val, evaluate once on Test using t_star (no test peeking)\n",
        "    trv = pd.concat([train, val]).copy()\n",
        "    model_final, scaler_final = fit_lstm_final_on_trv(\n",
        "        trv,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        lookback=lookback,\n",
        "        class_weight=class_weight,\n",
        "        epochs=epochs, batch_size=batch_size, patience=patience\n",
        "    )\n",
        "    test_metrics, *_ = eval_lstm_on_split(\n",
        "        model_final, scaler_final, test,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        lookback=lookback, threshold=t_star\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"val_metrics_tuned\": val_metrics_tuned,\n",
        "        \"val_metrics_at_0p5\": val_metrics_0p5,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"threshold_used\": float(t_star),\n",
        "        \"splits\": {\n",
        "            \"train\": (train.index.min(), train.index.max()),\n",
        "            \"val\":   (val.index.min(),   val.index.max()),\n",
        "            \"test\":  (test.index.min(),  test.index.max())\n",
        "        },\n",
        "        \"feature_cols\": list(feature_cols),\n",
        "        \"lookback\": lookback\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (uncomment)\n",
        "# ----------------------------\n",
        "data.index = pd.to_datetime(data[\"date\"])\n",
        "data = data.sort_index()\n",
        "out = run_holdout_lstm_single(\n",
        "    data,\n",
        "    test_start=\"2018-01-01\",\n",
        "    feature_cols=[\"spi_6\"],\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    val_span=None,            # or set e.g., 52\n",
        "    class_weight=\"balanced\",  # or pass dict {0: w0, 1: w1} or None\n",
        "    epochs=50, batch_size=64, patience=5,\n",
        "    tune_threshold=True, tune_metric=\"f1_macro\"\n",
        ")\n",
        "print(\"VAL (tuned):\", out[\"val_metrics_tuned\"])\n",
        "print(\"VAL (0.5):  \", out[\"val_metrics_at_0p5\"])\n",
        "print(\"TEST:       \", out[\"test_metrics\"])\n",
        "print(\"Threshold:  \", out[\"threshold_used\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36xY7zum69T_",
        "outputId": "6f129430-6494-43bd-ad25-3f4869fa8048"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL (tuned): {'acc': 0.7307692307692307, 'f1_macro': 0.7097288676236044, 'roc_auc': 0.6190476190476191, 'rmse_prob': 0.49491058350034967, 'rmse_hard': 0.5188745216627708, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.857     0.500     0.632        12\\n           1      0.684     0.929     0.788        14\\n\\n    accuracy                          0.731        26\\n   macro avg      0.771     0.714     0.710        26\\nweighted avg      0.764     0.731     0.716        26\\n', 'confusion_matrix': [[6, 6], [1, 13]]}\n",
            "VAL (0.5):   {'acc': 0.5769230769230769, 'f1_macro': 0.48468468468468473, 'roc_auc': 0.6190476190476191, 'rmse_prob': 0.49491058350034967, 'rmse_hard': 0.6504436355879909, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.667     0.167     0.267        12\\n           1      0.565     0.929     0.703        14\\n\\n    accuracy                          0.577        26\\n   macro avg      0.616     0.548     0.485        26\\nweighted avg      0.612     0.577     0.501        26\\n', 'confusion_matrix': [[2, 10], [1, 13]]}\n",
            "TEST:        {'acc': 0.5230352303523035, 'f1_macro': 0.5087145969498911, 'roc_auc': 0.5315823491391973, 'rmse_prob': 0.5010209052267155, 'rmse_hard': 0.6906263603770829, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.528     0.355     0.425       183\\n           1      0.520     0.688     0.593       186\\n\\n    accuracy                          0.523       369\\n   macro avg      0.524     0.522     0.509       369\\nweighted avg      0.524     0.523     0.509       369\\n', 'confusion_matrix': [[65, 118], [58, 128]]}\n",
            "Threshold:   0.5099999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LSTM feature-subset search (single hold-out; pick subset by LOG LOSS)\n",
        "# - Enumerates ALL non-empty feature combinations from CANDIDATE_FEATURES\n",
        "# - Trains on Train, validates on Val (choose subset with lowest log loss)\n",
        "# - Optionally retrains on Train+Val and reports Test log loss for top-K subsets\n",
        "#\n",
        "# Assumes:\n",
        "#   - df already has a binary 'target' column\n",
        "#   - df has a datetime index (or set df.index = pd.to_datetime(df[\"date\"]))\n",
        "# ============================================================\n",
        "\n",
        "import itertools, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, f1_score\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ---------- basic helpers ----------\n",
        "def make_static_splits(df: pd.DataFrame, test_start: str|pd.Timestamp):\n",
        "    m = df.index >= pd.to_datetime(test_start)\n",
        "    return df.loc[~m].copy(), df.loc[m].copy()\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
        "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "\n",
        "def _class_weight_from_labels(y):\n",
        "    y = np.asarray(y, int)\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    K = len(classes); total = counts.sum()\n",
        "    return {int(c): float(total/(K*cnt)) for c, cnt in zip(classes, counts)}\n",
        "\n",
        "def best_threshold(y_true, proba, metric=\"f1_macro\"):\n",
        "    # not used for log loss selection, but handy to keep\n",
        "    grid = np.linspace(0.05, 0.95, 181)\n",
        "    best_t, best = 0.5, -1.0\n",
        "    for t in grid:\n",
        "        pred = (proba >= t).astype(int)\n",
        "        score = f1_score(y_true, pred, average=\"macro\") if metric==\"f1_macro\" else f1_score(y_true, pred)\n",
        "        if score > best:\n",
        "            best, best_t = score, t\n",
        "    return float(best_t)\n",
        "\n",
        "# ---------- sequences & model ----------\n",
        "def make_sequences(df: pd.DataFrame, lookback: int, feature_cols, target_col=\"target\"):\n",
        "    X, y = [], []\n",
        "    V = df[feature_cols].values\n",
        "    t = df[target_col].values\n",
        "    for i in range(lookback, len(df)):\n",
        "        X.append(V[i-lookback:i])\n",
        "        y.append(t[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_lstm(input_dim, lookback, units=64, dropout=0.2, lr=1e-3):\n",
        "    m = keras.Sequential([\n",
        "        layers.Input(shape=(lookback, input_dim)),\n",
        "        layers.LSTM(units, return_sequences=False),\n",
        "        layers.Dropout(dropout),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    m.compile(optimizer=keras.optimizers.Adam(lr),\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "    return m\n",
        "\n",
        "# ---------- scaling utilities ----------\n",
        "def _split_scale_cols(feature_cols, keep_prefixes=(\"p_state\",)):\n",
        "    to_scale = [c for c in feature_cols if not any(c.startswith(p) for p in keep_prefixes)]\n",
        "    return to_scale\n",
        "\n",
        "def _fit_transform(train_df, other_df, feature_cols, keep_prefixes=(\"p_state\",)):\n",
        "    to_scale = _split_scale_cols(feature_cols, keep_prefixes)\n",
        "    scaler = StandardScaler().fit(train_df[to_scale]) if len(to_scale) else None\n",
        "    tr = train_df.copy(); ot = other_df.copy()\n",
        "    if scaler is not None:\n",
        "        tr[to_scale] = scaler.transform(tr[to_scale])\n",
        "        ot[to_scale] = scaler.transform(ot[to_scale])\n",
        "    return tr, ot, scaler, to_scale\n",
        "\n",
        "def _transform_with(scaler, df, to_scale):\n",
        "    if scaler is None or not to_scale:\n",
        "        return df.copy()\n",
        "    out = df.copy()\n",
        "    out[to_scale] = scaler.transform(out[to_scale])\n",
        "    return out\n",
        "def safe_logloss(y_true, proba):\n",
        "    \"\"\"Version-agnostic log loss: clips probs and always passes labels=[0,1].\"\"\"\n",
        "    p = np.clip(np.asarray(proba, dtype=float), 1e-7, 1-1e-7)\n",
        "    y = np.asarray(y_true, dtype=int)\n",
        "    return float(log_loss(y, p, labels=[0, 1]))\n",
        "\n",
        "# ---------- core training on a given subset ----------\n",
        "def _train_eval_subset(train_df, val_df, feature_cols, target_col=\"target\",\n",
        "                       lookback=26, units=64, dropout=0.2, lr=1e-3,\n",
        "                       patience=5, batch_size=64, epochs=50,\n",
        "                       class_weight_mode=\"balanced\", keep_prefixes=(\"p_state\",)):\n",
        "    # clean\n",
        "    cols_needed = feature_cols + [target_col]\n",
        "    train_df = train_df.dropna(subset=cols_needed).copy()\n",
        "    val_df   = val_df.dropna(subset=cols_needed).copy()\n",
        "    if train_df.empty or val_df.empty:\n",
        "        return None  # not enough data\n",
        "\n",
        "    # scale (price/SPI by default; keep p_state* raw)\n",
        "    tr, va, scaler, to_scale = _fit_transform(train_df, val_df, feature_cols, keep_prefixes)\n",
        "\n",
        "    # sequences\n",
        "    X_tr, y_tr = make_sequences(tr, lookback, feature_cols, target_col)\n",
        "    X_va, y_va = make_sequences(va, lookback, feature_cols, target_col)\n",
        "    if len(X_tr) == 0 or len(X_va) == 0:\n",
        "        return None\n",
        "\n",
        "    # class weights\n",
        "    cw = None\n",
        "    if class_weight_mode == \"balanced\":\n",
        "        cw = _class_weight_from_labels(y_tr)\n",
        "    elif isinstance(class_weight_mode, dict):\n",
        "        cw = class_weight_mode\n",
        "\n",
        "    # model\n",
        "    model = build_lstm(input_dim=X_tr.shape[2], lookback=lookback, units=units, dropout=dropout, lr=lr)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=(X_va, y_va),\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=cw\n",
        "    )\n",
        "    # validation probabilities/metrics\n",
        "    proba_va = model.predict(X_va, verbose=0).ravel()\n",
        "    # robust log loss (uses labels=[0,1] even if one class absent)\n",
        "    val_logloss = safe_logloss(y_va, proba_va)\n",
        "    # some extra metrics for info\n",
        "    val_auc = float(roc_auc_score(y_va, proba_va)) if len(np.unique(y_va)) == 2 else np.nan\n",
        "    val_acc = float(accuracy_score(y_va, (proba_va >= 0.5).astype(int)))\n",
        "    val_f1  = float(f1_score(y_va, (proba_va >= 0.5).astype(int), average=\"macro\"))\n",
        "    val_rmse_prob = rmse(y_va, proba_va)\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"scaler\": scaler,\n",
        "        \"to_scale\": to_scale,\n",
        "        \"y_va\": y_va,\n",
        "        \"proba_va\": proba_va,\n",
        "        \"val_logloss\": val_logloss,\n",
        "        \"val_auc\": val_auc,\n",
        "        \"val_acc\": val_acc,\n",
        "        \"val_f1_macro\": val_f1,\n",
        "        \"val_rmse_prob\": val_rmse_prob\n",
        "    }\n",
        "\n",
        "# ---------- final retrain on Train+Val and test ----------\n",
        "def _retrain_and_test(df_trv, df_test, feature_cols, target_col=\"target\",\n",
        "                      lookback=26, units=64, dropout=0.2, lr=1e-3,\n",
        "                      patience=5, batch_size=64, epochs=50,\n",
        "                      class_weight_mode=\"balanced\", keep_prefixes=(\"p_state\",)):\n",
        "    # clean\n",
        "    cols_needed = feature_cols + [target_col]\n",
        "    df_trv  = df_trv.dropna(subset=cols_needed).copy()\n",
        "    df_test = df_test.dropna(subset=cols_needed).copy()\n",
        "    if df_trv.empty or df_test.empty:\n",
        "        return None\n",
        "\n",
        "    # scale on Train+Val\n",
        "    to_scale = _split_scale_cols(feature_cols, keep_prefixes)\n",
        "    scaler = StandardScaler().fit(df_trv[to_scale]) if len(to_scale) else None\n",
        "    trv  = _transform_with(scaler, df_trv, to_scale)\n",
        "    test = _transform_with(scaler, df_test, to_scale)\n",
        "\n",
        "    # sequences\n",
        "    X_trv, y_trv = make_sequences(trv,  lookback, feature_cols, target_col)\n",
        "    X_te,  y_te  = make_sequences(test, lookback, feature_cols, target_col)\n",
        "    if len(X_trv) == 0 or len(X_te) == 0:\n",
        "        return None\n",
        "\n",
        "    # small internal val from trv tail for early stopping\n",
        "    monitor_k = max(min(max(len(X_trv)//10, 32), 256), 16)\n",
        "    monitor_k = min(monitor_k, len(X_trv)//5) if len(X_trv) >= 5 else 0\n",
        "    if monitor_k > 0:\n",
        "        X_tr, y_tr = X_trv[:-monitor_k], y_trv[:-monitor_k]\n",
        "        X_vm, y_vm = X_trv[-monitor_k:], y_trv[-monitor_k:]\n",
        "        val_monitor = (X_vm, y_vm)\n",
        "    else:\n",
        "        X_tr, y_tr = X_trv, y_trv\n",
        "        val_monitor = None\n",
        "\n",
        "    cw = None\n",
        "    if class_weight_mode == \"balanced\":\n",
        "        cw = _class_weight_from_labels(y_tr)\n",
        "    elif isinstance(class_weight_mode, dict):\n",
        "        cw = class_weight_mode\n",
        "\n",
        "    # model\n",
        "    model = build_lstm(input_dim=X_trv.shape[2], lookback=lookback, units=units, dropout=dropout, lr=lr)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=epochs, batch_size=64,\n",
        "        validation_data=val_monitor,\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=cw\n",
        "    )\n",
        "\n",
        "    proba_te = model.predict(X_te, verbose=0).ravel()\n",
        "    test_logloss = safe_logloss(y_te, proba_te)\n",
        "    test_auc = float(roc_auc_score(y_te, proba_te)) if len(np.unique(y_te)) == 2 else np.nan\n",
        "    test_acc = float(accuracy_score(y_te, (proba_te >= 0.5).astype(int)))\n",
        "    test_f1  = float(f1_score(y_te, (proba_te >= 0.5).astype(int), average=\"macro\"))\n",
        "    test_rmse_prob = rmse(y_te, proba_te)\n",
        "\n",
        "    return {\n",
        "        \"test_logloss\": test_logloss,\n",
        "        \"test_auc\": test_auc,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"test_f1_macro\": test_f1,\n",
        "        \"test_rmse_prob\": test_rmse_prob\n",
        "    }\n",
        "\n",
        "# ---------- search over ALL feature subsets ----------\n",
        "def grid_search_feature_subsets_lstm_holdout(\n",
        "    df: pd.DataFrame,\n",
        "    test_start: str|pd.Timestamp,\n",
        "    CANDIDATE_FEATURES: list[str] | None = None,\n",
        "    target_col: str = \"target\",\n",
        "    lookback: int = 26,\n",
        "    units: int = 64,\n",
        "    dropout: float = 0.2,\n",
        "    lr: float = 1e-3,\n",
        "    patience: int = 5,\n",
        "    batch_size: int = 64,\n",
        "    epochs: int = 50,\n",
        "    class_weight_mode: str | dict | None = \"balanced\",\n",
        "    keep_prefixes: tuple[str,...] = (\"p_state\",),\n",
        "    max_subsets: int | None = None,      # set to e.g. 2000 to cap runtime\n",
        "    top_k_test: int = 5,                 # retrain+test only for top-K by val logloss\n",
        "    random_seed: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict with:\n",
        "      - 'ranked': DataFrame of all tried subsets sorted by val_logloss\n",
        "      - 'best_subset': best feature list (by val_logloss)\n",
        "      - 'best_val': metrics dict for the best subset\n",
        "      - 'topk_test': list of dicts with test metrics for top-K subsets\n",
        "    \"\"\"\n",
        "    df = df.sort_index().copy()\n",
        "    trainval, test = make_static_splits(df, test_start)\n",
        "\n",
        "    # candidate features (auto-detect numeric columns if not provided)\n",
        "    if CANDIDATE_FEATURES is None:\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        CANDIDATE_FEATURES = [c for c in numeric_cols if c != target_col]\n",
        "\n",
        "    # enumerate ALL non-empty subsets\n",
        "    all_subsets = []\n",
        "    for r in range(1, len(CANDIDATE_FEATURES) + 1):\n",
        "        for comb in itertools.combinations(CANDIDATE_FEATURES, r):\n",
        "            all_subsets.append(list(comb))\n",
        "\n",
        "    # optional cap to avoid explosion\n",
        "    random.seed(random_seed)\n",
        "    if max_subsets is not None and len(all_subsets) > max_subsets:\n",
        "        all_subsets = random.sample(all_subsets, max_subsets)\n",
        "\n",
        "    rows = []\n",
        "    cache_best = {\"subset\": None, \"val\": None}\n",
        "\n",
        "    for subset in all_subsets:\n",
        "        res = _train_eval_subset(\n",
        "            trainval.iloc[:-min(52, max(26, len(trainval)//10))],   # Train\n",
        "            trainval.iloc[-min(52, max(26, len(trainval)//10)):],   # Val\n",
        "            feature_cols=subset,\n",
        "            target_col=target_col,\n",
        "            lookback=lookback, units=units, dropout=dropout, lr=lr,\n",
        "            patience=patience, batch_size=batch_size, epochs=epochs,\n",
        "            class_weight_mode=class_weight_mode, keep_prefixes=keep_prefixes\n",
        "        )\n",
        "        if res is None:\n",
        "            continue\n",
        "\n",
        "        row = {\n",
        "            \"subset\": tuple(subset),\n",
        "            \"k\": len(subset),\n",
        "            \"val_logloss\": res[\"val_logloss\"],\n",
        "            \"val_auc\": res[\"val_auc\"],\n",
        "            \"val_acc\": res[\"val_acc\"],\n",
        "            \"val_f1_macro\": res[\"val_f1_macro\"],\n",
        "            \"val_rmse_prob\": res[\"val_rmse_prob\"]\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "        if (cache_best[\"val\"] is None) or (row[\"val_logloss\"] < cache_best[\"val\"][\"val_logloss\"]):\n",
        "            cache_best[\"subset\"] = subset\n",
        "            cache_best[\"val\"] = row\n",
        "\n",
        "    if not rows:\n",
        "        raise RuntimeError(\"No valid subsets could be trained (not enough data per subset/lookback).\")\n",
        "\n",
        "    ranked = pd.DataFrame(rows).sort_values(\"val_logloss\").reset_index(drop=True)\n",
        "    best_subset = list(ranked.loc[0, \"subset\"])\n",
        "    best_val_row = ranked.loc[0].to_dict()\n",
        "\n",
        "    # retrain on Train+Val and evaluate on Test for top-K subsets\n",
        "    topk = min(top_k_test, len(ranked))\n",
        "    topk_test = []\n",
        "    trv = trainval  # full Train+Val\n",
        "\n",
        "    for i in range(topk):\n",
        "        subset_i = list(ranked.loc[i, \"subset\"])\n",
        "        test_res = _retrain_and_test(\n",
        "            trv, test,\n",
        "            feature_cols=subset_i, target_col=target_col,\n",
        "            lookback=lookback, units=units, dropout=dropout, lr=lr,\n",
        "            patience=patience, batch_size=batch_size, epochs=epochs,\n",
        "            class_weight_mode=class_weight_mode, keep_prefixes=keep_prefixes\n",
        "        )\n",
        "        if test_res is not None:\n",
        "            topk_test.append({\"subset\": tuple(subset_i), **test_res})\n",
        "\n",
        "    return {\n",
        "        \"ranked\": ranked,\n",
        "        \"best_subset\": best_subset,\n",
        "        \"best_val\": best_val_row,\n",
        "        \"topk_test\": topk_test\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (uncomment)\n",
        "# ----------------------------\n",
        "data.index = pd.to_datetime(data[\"date\"])\n",
        "data = data.sort_index()\n",
        "#\n",
        "# # Define candidate features explicitly (recommended)\n",
        "CANDIDATE_FEATURES = [\"Last Price\",\"top_state\",\"spi_6\",\n",
        "                      \"ret_1\"]\n",
        "#\n",
        "out = grid_search_feature_subsets_lstm_holdout(\n",
        "    data,\n",
        "    test_start=\"2018-01-01\",\n",
        "    CANDIDATE_FEATURES=CANDIDATE_FEATURES,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    units=64, dropout=0.2, lr=1e-3,\n",
        "    patience=5, batch_size=64, epochs=50,\n",
        "    class_weight_mode=\"balanced\",\n",
        "    keep_prefixes=(\"p_state\",),   # keep p_state* unscaled\n",
        "    max_subsets=1024,             # cap if feature count is large\n",
        "    top_k_test=5\n",
        ")\n",
        "#\n",
        "print(\"Best subset by VAL logloss:\", out[\"best_subset\"])\n",
        "print(\"Best VAL row:\", out[\"best_val\"])\n",
        "print(\"Top-K TEST results:\")\n",
        "for r in out[\"topk_test\"]:\n",
        "    print(r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLzTAhJy94Ch",
        "outputId": "71e7e1ad-be38-4ee6-f8c8-5fee5259ba06"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best subset by VAL logloss: ['spi_6']\n",
            "Best VAL row: {'subset': ('spi_6',), 'k': 1, 'val_logloss': 0.6671536790350419, 'val_auc': 0.625, 'val_acc': 0.6538461538461539, 'val_f1_macro': 0.6067226890756303, 'val_rmse_prob': 0.48685812486001484}\n",
            "Top-K TEST results:\n",
            "{'subset': ('spi_6',), 'test_logloss': 0.6925327106857391, 'test_auc': 0.5387801868499912, 'test_acc': 0.5447154471544715, 'test_f1_macro': 0.5432360742705571, 'test_rmse_prob': 0.4996381821083819}\n",
            "{'subset': ('Last Price', 'top_state'), 'test_logloss': 0.6919179675906035, 'test_auc': 0.5390445972148775, 'test_acc': 0.5447154471544715, 'test_f1_macro': 0.5422647527910687, 'test_rmse_prob': 0.49938506581254377}\n",
            "{'subset': ('top_state', 'spi_6'), 'test_logloss': 0.6921368318806327, 'test_auc': 0.5294670662201069, 'test_acc': 0.5338753387533876, 'test_f1_macro': 0.5323607427055703, 'test_rmse_prob': 0.4994946195616021}\n",
            "{'subset': ('Last Price', 'top_state', 'spi_6'), 'test_logloss': 0.6932942997503005, 'test_auc': 0.5208002820377226, 'test_acc': 0.5121951219512195, 'test_f1_macro': 0.4984898822108125, 'test_rmse_prob': 0.5000706853622587}\n",
            "{'subset': ('top_state',), 'test_logloss': 0.691634567527391, 'test_auc': 0.5417621481873202, 'test_acc': 0.5474254742547425, 'test_f1_macro': 0.5444141326768248, 'test_rmse_prob': 0.49924330460841393}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "rjSjCCEQ3n9E",
        "outputId": "0c377528-82ca-4347-bc7f-ac7f93f9e6f0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  date     spi_6  p_state0  p_state1  p_state2  Last Price  \\\n",
              "date                                                                         \n",
              "1992-01-12  1992-01-12 -0.676211  0.188472  0.778493  0.033035        2.43   \n",
              "1992-01-19  1992-01-19 -0.408163  0.042357  0.918067  0.039576        2.48   \n",
              "1992-01-26  1992-01-26 -0.149762  0.031910  0.927547  0.040544        2.55   \n",
              "1992-02-02  1992-02-02 -0.012210  0.031731  0.926709  0.041559        2.57   \n",
              "1992-02-09  1992-02-09 -0.154111  0.031869  0.927580  0.040551        2.57   \n",
              "...                ...       ...       ...       ...       ...         ...   \n",
              "2025-06-29  2025-06-29 -0.317970  0.047144  0.919293  0.033563        4.00   \n",
              "2025-07-06  2025-07-06 -0.346593  0.053727  0.912975  0.033298        4.10   \n",
              "2025-07-13  2025-07-13 -0.113659  0.039652  0.926234  0.034114        3.87   \n",
              "2025-07-20  2025-07-20  0.110532  0.035815  0.928811  0.035374        3.95   \n",
              "2025-07-27  2025-07-27  0.294891  0.035153  0.926273  0.038575        3.87   \n",
              "\n",
              "           price_date_used  fallback_days     ret_1  top_state  next_ret  \\\n",
              "date                                                                       \n",
              "1992-01-12      1992-01-10            2.0  0.016736          1  0.020576   \n",
              "1992-01-19      1992-01-17            2.0  0.020576          1  0.028226   \n",
              "1992-01-26      1992-01-24            2.0  0.028226          1  0.007843   \n",
              "1992-02-02      1992-01-31            2.0  0.007843          1  0.000000   \n",
              "1992-02-09      1992-02-07            2.0  0.000000          1 -0.007782   \n",
              "...                    ...            ...       ...        ...       ...   \n",
              "2025-06-29      2025-06-27            2.0 -0.014778          1  0.025000   \n",
              "2025-07-06      2025-07-03            3.0  0.025000          1 -0.056098   \n",
              "2025-07-13      2025-07-11            2.0 -0.056098          1  0.020672   \n",
              "2025-07-20      2025-07-18            2.0  0.020672          1 -0.020253   \n",
              "2025-07-27      2025-07-25            2.0 -0.020253          1       NaN   \n",
              "\n",
              "            target  \n",
              "date                \n",
              "1992-01-12       1  \n",
              "1992-01-19       1  \n",
              "1992-01-26       1  \n",
              "1992-02-02       0  \n",
              "1992-02-09       0  \n",
              "...            ...  \n",
              "2025-06-29       1  \n",
              "2025-07-06       0  \n",
              "2025-07-13       1  \n",
              "2025-07-20       0  \n",
              "2025-07-27       0  \n",
              "\n",
              "[1749 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52165487-b187-49bf-928c-cbb2a525f07d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>spi_6</th>\n",
              "      <th>p_state0</th>\n",
              "      <th>p_state1</th>\n",
              "      <th>p_state2</th>\n",
              "      <th>Last Price</th>\n",
              "      <th>price_date_used</th>\n",
              "      <th>fallback_days</th>\n",
              "      <th>ret_1</th>\n",
              "      <th>top_state</th>\n",
              "      <th>next_ret</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1992-01-12</th>\n",
              "      <td>1992-01-12</td>\n",
              "      <td>-0.676211</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.778493</td>\n",
              "      <td>0.033035</td>\n",
              "      <td>2.43</td>\n",
              "      <td>1992-01-10</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.016736</td>\n",
              "      <td>1</td>\n",
              "      <td>0.020576</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992-01-19</th>\n",
              "      <td>1992-01-19</td>\n",
              "      <td>-0.408163</td>\n",
              "      <td>0.042357</td>\n",
              "      <td>0.918067</td>\n",
              "      <td>0.039576</td>\n",
              "      <td>2.48</td>\n",
              "      <td>1992-01-17</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.020576</td>\n",
              "      <td>1</td>\n",
              "      <td>0.028226</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992-01-26</th>\n",
              "      <td>1992-01-26</td>\n",
              "      <td>-0.149762</td>\n",
              "      <td>0.031910</td>\n",
              "      <td>0.927547</td>\n",
              "      <td>0.040544</td>\n",
              "      <td>2.55</td>\n",
              "      <td>1992-01-24</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.028226</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992-02-02</th>\n",
              "      <td>1992-02-02</td>\n",
              "      <td>-0.012210</td>\n",
              "      <td>0.031731</td>\n",
              "      <td>0.926709</td>\n",
              "      <td>0.041559</td>\n",
              "      <td>2.57</td>\n",
              "      <td>1992-01-31</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992-02-09</th>\n",
              "      <td>1992-02-09</td>\n",
              "      <td>-0.154111</td>\n",
              "      <td>0.031869</td>\n",
              "      <td>0.927580</td>\n",
              "      <td>0.040551</td>\n",
              "      <td>2.57</td>\n",
              "      <td>1992-02-07</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.007782</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-06-29</th>\n",
              "      <td>2025-06-29</td>\n",
              "      <td>-0.317970</td>\n",
              "      <td>0.047144</td>\n",
              "      <td>0.919293</td>\n",
              "      <td>0.033563</td>\n",
              "      <td>4.00</td>\n",
              "      <td>2025-06-27</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.014778</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-06</th>\n",
              "      <td>2025-07-06</td>\n",
              "      <td>-0.346593</td>\n",
              "      <td>0.053727</td>\n",
              "      <td>0.912975</td>\n",
              "      <td>0.033298</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2025-07-03</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.056098</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-13</th>\n",
              "      <td>2025-07-13</td>\n",
              "      <td>-0.113659</td>\n",
              "      <td>0.039652</td>\n",
              "      <td>0.926234</td>\n",
              "      <td>0.034114</td>\n",
              "      <td>3.87</td>\n",
              "      <td>2025-07-11</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.056098</td>\n",
              "      <td>1</td>\n",
              "      <td>0.020672</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-20</th>\n",
              "      <td>2025-07-20</td>\n",
              "      <td>0.110532</td>\n",
              "      <td>0.035815</td>\n",
              "      <td>0.928811</td>\n",
              "      <td>0.035374</td>\n",
              "      <td>3.95</td>\n",
              "      <td>2025-07-18</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.020672</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.020253</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-27</th>\n",
              "      <td>2025-07-27</td>\n",
              "      <td>0.294891</td>\n",
              "      <td>0.035153</td>\n",
              "      <td>0.926273</td>\n",
              "      <td>0.038575</td>\n",
              "      <td>3.87</td>\n",
              "      <td>2025-07-25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.020253</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1749 rows Ã— 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52165487-b187-49bf-928c-cbb2a525f07d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-52165487-b187-49bf-928c-cbb2a525f07d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-52165487-b187-49bf-928c-cbb2a525f07d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dadc953f-8b49-4a9b-916a-79f89bf81569\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dadc953f-8b49-4a9b-916a-79f89bf81569')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dadc953f-8b49-4a9b-916a-79f89bf81569 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_2d447f8b-2bd2-4bfc-a15c-8c3b01adb344\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2d447f8b-2bd2-4bfc-a15c-8c3b01adb344 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "repr_error": "cannot insert date, already exists"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}