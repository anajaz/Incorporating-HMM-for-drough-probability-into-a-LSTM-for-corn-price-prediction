{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j5xOs1lINIms"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"data.csv\", dtype={\"date\": \"string\"})\n",
        "data.rename(columns={\"y\": \"target\"}, inplace=True)\n",
        "data.rename(columns={\"p_state0_h1\": \"p_state0\"}, inplace=True)\n",
        "data.rename(columns={\"p_state1_h1\": \"p_state1\"}, inplace=True)\n",
        "data.rename(columns={\"p_state2_h1\": \"p_state2\"}, inplace=True)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3FBlO1gi9h7",
        "outputId": "c2fdb73b-681c-4abd-fc4d-f9f8a4d77089"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            date     spi_6  p_state0  p_state1  p_state2  Last Price  \\\n",
            "0     1992-01-12 -0.676211  0.188472  0.778493  0.033035        2.43   \n",
            "1     1992-01-19 -0.408163  0.042357  0.918067  0.039576        2.48   \n",
            "2     1992-01-26 -0.149762  0.031910  0.927547  0.040544        2.55   \n",
            "3     1992-02-02 -0.012210  0.031731  0.926709  0.041559        2.57   \n",
            "4     1992-02-09 -0.154111  0.031869  0.927580  0.040551        2.57   \n",
            "...          ...       ...       ...       ...       ...         ...   \n",
            "1744  2025-06-29 -0.317970  0.047144  0.919293  0.033563        4.00   \n",
            "1745  2025-07-06 -0.346593  0.053727  0.912975  0.033298        4.10   \n",
            "1746  2025-07-13 -0.113659  0.039652  0.926234  0.034114        3.87   \n",
            "1747  2025-07-20  0.110532  0.035815  0.928811  0.035374        3.95   \n",
            "1748  2025-07-27  0.294891  0.035153  0.926273  0.038575        3.87   \n",
            "\n",
            "     price_date_used  fallback_days     ret_1  top_state  next_ret  target  \n",
            "0         1992-01-10            2.0  0.016736          1  0.020576       1  \n",
            "1         1992-01-17            2.0  0.020576          1  0.028226       1  \n",
            "2         1992-01-24            2.0  0.028226          1  0.007843       1  \n",
            "3         1992-01-31            2.0  0.007843          1  0.000000       0  \n",
            "4         1992-02-07            2.0  0.000000          1 -0.007782       0  \n",
            "...              ...            ...       ...        ...       ...     ...  \n",
            "1744      2025-06-27            2.0 -0.014778          1  0.025000       1  \n",
            "1745      2025-07-03            3.0  0.025000          1 -0.056098       0  \n",
            "1746      2025-07-11            2.0 -0.056098          1  0.020672       1  \n",
            "1747      2025-07-18            2.0  0.020672          1 -0.020253       0  \n",
            "1748      2025-07-25            2.0 -0.020253          1       NaN       0  \n",
            "\n",
            "[1749 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This notebook builds different modesl for predicting corn prices\n",
        "\n",
        "The models are always a simple logistic regression and a LSTM, but the inputs change:\n",
        "First we segment the training, val and test sets. Then we build different models:\n",
        "1.   Only price data\n",
        "2.   Price and prcp data\n",
        "3.   Price and SPI data\n",
        "4.   Price, SPI and HMM data\n"
      ],
      "metadata": {
        "id": "MG48muHWNOsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Only price data"
      ],
      "metadata": {
        "id": "ydaX_QYUNRfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1 Logistic regression"
      ],
      "metadata": {
        "id": "PfvZwxmcNV5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2 LSTM"
      ],
      "metadata": {
        "id": "9hUBppWX79RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "xP2iy0kq9059"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LSTM-only pipeline for binary next-period price direction\n",
        "# Assumes your DataFrame ALREADY contains the binary target column.\n",
        "#\n",
        "# Expected columns at minimum:\n",
        "#   [\"date\",\"return\",\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi\",\"target\"]\n",
        "# Set a datetime index and sort ascending before running:\n",
        "#   df.index = pd.to_datetime(df[\"date\"]); df = df.sort_index()\n",
        "# ============================================================\n",
        "\n",
        "# ----------------------------\n",
        "# Config and splitting helpers\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class CVConfig:\n",
        "    val_span: int        # e.g., 52 (weeks)\n",
        "    step: int            # e.g., 26\n",
        "    min_train_span: int  # e.g., 156\n",
        "\n",
        "FEATURES = [\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi_6\"]\n",
        "\n",
        "def make_static_splits(df: pd.DataFrame, test_start: str | pd.Timestamp):\n",
        "    \"\"\"\n",
        "    Chronological hold-out split: all dates >= test_start go to test.\n",
        "    The remainder will later be split into train/val.\n",
        "    \"\"\"\n",
        "    mask_test = df.index >= pd.to_datetime(test_start)\n",
        "    return df.loc[~mask_test].copy(), df.loc[mask_test].copy()\n",
        "\n",
        "def expanding_cv_folds(trainval_df: pd.DataFrame, cfg: CVConfig):\n",
        "    \"\"\"\n",
        "    Walk-forward (expanding-window) CV generator over train/val pairs.\n",
        "    \"\"\"\n",
        "    n = len(trainval_df); idx = np.arange(n)\n",
        "    train_end = cfg.min_train_span\n",
        "    while True:\n",
        "        val_end = train_end + cfg.val_span\n",
        "        if val_end > n: break\n",
        "        train_mask = idx < train_end\n",
        "        val_mask   = (idx >= train_end) & (idx < val_end)\n",
        "        yield trainval_df.loc[train_mask].copy(), trainval_df.loc[val_mask].copy()\n",
        "        train_end += cfg.step\n",
        "\n",
        "# ----------------------------\n",
        "# Sequences, model, and metrics\n",
        "# ----------------------------\n",
        "def make_sequences(df: pd.DataFrame, lookback: int, feature_cols, target_col: str = \"target\"):\n",
        "    \"\"\"\n",
        "    Build (X, y) where each X[i] is a window of length `lookback` ending at t-1,\n",
        "    and y[i] is the target at time t.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    V = df[feature_cols].values\n",
        "    t = df[target_col].values\n",
        "    for i in range(lookback, len(df)):        # predict y at i using window [i-L, ..., i-1]\n",
        "        X.append(V[i-lookback:i])\n",
        "        y.append(t[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_lstm(input_dim, lookback, units=64, dropout=0.2, lr=1e-3):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(lookback, input_dim)),\n",
        "        layers.LSTM(units, return_sequences=False),\n",
        "        layers.Dropout(dropout),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Root Mean Squared Error. For classification, pass probabilities for √Brier.\"\"\"\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "\n",
        "def fit_eval_lstm(\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    lookback=26,\n",
        "    class_weight: dict | None = None,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Scale on train only, make sequences, train LSTM, evaluate on val.\n",
        "    Returns (model, metrics_dict, (pred, proba), scaler).\n",
        "    \"\"\"\n",
        "    # Clean and scale\n",
        "    train_df = train_df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    val_df   = val_df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    scaler = StandardScaler().fit(train_df[feature_cols])\n",
        "    tr = train_df.copy(); va = val_df.copy()\n",
        "    tr[feature_cols] = scaler.transform(tr[feature_cols])\n",
        "    va[feature_cols] = scaler.transform(va[feature_cols])\n",
        "\n",
        "    # Sequences\n",
        "    X_tr, y_tr = make_sequences(tr, lookback, feature_cols, target_col)\n",
        "    X_va, y_va = make_sequences(va, lookback, feature_cols, target_col)\n",
        "\n",
        "    # Guard: not enough data to form sequences\n",
        "    if len(X_tr) == 0 or len(X_va) == 0:\n",
        "        raise ValueError(\"Not enough rows to form sequences. Reduce lookback or provide more data.\")\n",
        "\n",
        "    # Model\n",
        "    model = build_lstm(input_dim=X_tr.shape[2], lookback=lookback)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=(X_va, y_va),\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=class_weight\n",
        "    )\n",
        "\n",
        "    proba = model.predict(X_va, verbose=0).ravel()\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": float(accuracy_score(y_va, pred)),\n",
        "        \"f1_macro\": float(f1_score(y_va, pred, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_va, proba)) if len(np.unique(y_va)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_va, proba),                 # √Brier on probabilities\n",
        "        \"rmse_hard\": rmse(y_va, pred.astype(float)),    # sqrt(error rate) on hard labels\n",
        "        \"classification_report\": classification_report(y_va, pred, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_va, pred).tolist()\n",
        "    }\n",
        "    return model, metrics, (pred, proba), scaler\n",
        "\n",
        "# ----------------------------\n",
        "# End-to-end runners (LSTM only)\n",
        "# ----------------------------\n",
        "def run_holdout_lstm(\n",
        "    df: pd.DataFrame,\n",
        "    test_start: str | pd.Timestamp,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    lookback=26,\n",
        "    class_weight: dict | None = None,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Split Train/Val/Test chronologically (hold-out; df already has 'target')\n",
        "    2) Train on Train, tune on Val\n",
        "    3) Retrain on Train+Val and evaluate on Test (no peeking at Test)\n",
        "    Returns dict with validation metrics, test metrics, and split boundaries.\n",
        "    \"\"\"\n",
        "    # Sort and basic cleaning (drop rows missing features or target)\n",
        "    df = df.sort_index()\n",
        "    df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    # Train/Val/Test chronological split\n",
        "    trainval, test = make_static_splits(df, test_start)\n",
        "\n",
        "    # carve validation from tail of trainval\n",
        "    VAL_SPAN = min(52, max(26, len(trainval)//10))  # ~10% bounded into weekly range\n",
        "    val = trainval.iloc[-VAL_SPAN:].copy()\n",
        "    train = trainval.iloc[:-VAL_SPAN].copy()\n",
        "\n",
        "    # ---- validation round (select hyperparams externally if needed)\n",
        "    _, val_metrics, _, _ = fit_eval_lstm(\n",
        "        train, val,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        lookback=lookback,\n",
        "        class_weight=class_weight,\n",
        "        epochs=epochs, batch_size=batch_size, patience=patience\n",
        "    )\n",
        "\n",
        "    # ---- final training on Train+Val, evaluate on Test (no test peeking)\n",
        "    # Scale on Train+Val\n",
        "    trv = pd.concat([train, val]).copy()\n",
        "    trv = trv.dropna(subset=feature_cols + [target_col])\n",
        "    test = test.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "    scaler = StandardScaler().fit(trv[feature_cols])\n",
        "    trv_scaled  = trv.copy();  trv_scaled[feature_cols]  = scaler.transform(trv_scaled[feature_cols])\n",
        "    test_scaled = test.copy(); test_scaled[feature_cols] = scaler.transform(test_scaled[feature_cols])\n",
        "\n",
        "    # Make sequences\n",
        "    X_trv, y_trv = make_sequences(trv_scaled,  lookback, feature_cols, target_col)\n",
        "    X_te,  y_te  = make_sequences(test_scaled, lookback, feature_cols, target_col)\n",
        "\n",
        "    if len(X_trv) == 0 or len(X_te) == 0:\n",
        "        raise ValueError(\"Not enough sequence data in Train+Val or Test. Adjust lookback or splits.\")\n",
        "\n",
        "    # Set aside a small tail of Train+Val sequences for early-stopping monitor (still not using Test)\n",
        "    monitor_k = max( min( max(len(X_trv)//10, 32), 256 ), 16 )  # between 16 and 256, ~10% of trv\n",
        "    monitor_k = min(monitor_k, len(X_trv)//5) if len(X_trv) >= 5 else 0\n",
        "    if monitor_k == 0:\n",
        "        X_trv_train, y_trv_train = X_trv, y_trv\n",
        "        val_monitor = None\n",
        "    else:\n",
        "        X_trv_train, y_trv_train = X_trv[:-monitor_k], y_trv[:-monitor_k]\n",
        "        X_trv_valm, y_trv_valm   = X_trv[-monitor_k:], y_trv[-monitor_k:]\n",
        "        val_monitor = (X_trv_valm, y_trv_valm)\n",
        "\n",
        "    model = build_lstm(input_dim=X_trv.shape[2], lookback=lookback)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "\n",
        "    model.fit(\n",
        "        X_trv_train, y_trv_train,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=val_monitor,\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=class_weight\n",
        "    )\n",
        "\n",
        "    # Evaluate on the entire Test set\n",
        "    proba_te = model.predict(X_te, verbose=0).ravel()\n",
        "    pred_te  = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    test_metrics = {\n",
        "        \"acc\": float(accuracy_score(y_te, pred_te)),\n",
        "        \"f1_macro\": float(f1_score(y_te, pred_te, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_te, proba_te)) if len(np.unique(y_te)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_te, proba_te),                 # √Brier on probabilities\n",
        "        \"rmse_hard\": rmse(y_te, pred_te.astype(float)),    # sqrt(error rate) on hard labels\n",
        "        \"classification_report\": classification_report(y_te, pred_te, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_te, pred_te).tolist()\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"splits\": {\n",
        "            \"train\": (train.index.min(), train.index.max()),\n",
        "            \"val\":   (val.index.min(),   val.index.max()),\n",
        "            \"test\":  (test.index.min(),  test.index.max())\n",
        "        },\n",
        "        \"lookback\": lookback\n",
        "    }\n",
        "\n",
        "def run_walkforward_lstm(\n",
        "    df: pd.DataFrame,\n",
        "    cfg: CVConfig = CVConfig(val_span=52, step=26, min_train_span=156),\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    lookback=26,\n",
        "    class_weight: dict | None = None,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Expanding-window CV for LSTM. Assumes df already has 'target'.\n",
        "    Returns average metrics across folds and per-fold metrics list.\n",
        "    \"\"\"\n",
        "    df = df.sort_index()\n",
        "    df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    fold_metrics = []\n",
        "    for df_tr, df_va in expanding_cv_folds(df, cfg):\n",
        "        # Safety: dropna per fold\n",
        "        df_tr = df_tr.dropna(subset=feature_cols + [target_col])\n",
        "        df_va = df_va.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "        _, m, _, _ = fit_eval_lstm(\n",
        "            df_tr, df_va,\n",
        "            feature_cols=feature_cols, target_col=target_col,\n",
        "            lookback=lookback,\n",
        "            class_weight=class_weight,\n",
        "            epochs=epochs, batch_size=batch_size, patience=patience\n",
        "        )\n",
        "        fold_metrics.append(m)\n",
        "\n",
        "    def avg(key):\n",
        "        vals = [fm.get(key, np.nan) for fm in fold_metrics if not np.isnan(fm.get(key, np.nan))]\n",
        "        return float(np.mean(vals)) if vals else np.nan\n",
        "\n",
        "    return {\n",
        "        \"avg_metrics\": {\n",
        "            \"acc\": avg(\"acc\"),\n",
        "            \"f1_macro\": avg(\"f1_macro\"),\n",
        "            \"roc_auc\": avg(\"roc_auc\"),\n",
        "            \"rmse_prob\": avg(\"rmse_prob\"),\n",
        "            \"rmse_hard\": avg(\"rmse_hard\"),\n",
        "        },\n",
        "        \"folds\": fold_metrics,\n",
        "        \"n_folds\": len(fold_metrics),\n",
        "        \"cfg\": cfg,\n",
        "        \"lookback\": lookback\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXIsVSFyrKjE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Example usage\n",
        "# ----------------------------\n",
        "selected_features = [\"Last Price\"]\n",
        "data.index = pd.to_datetime(data[\"date\"])\n",
        "data = data.sort_index()\n",
        "#\n",
        "# # Hold-out\n",
        "result_holdout = run_holdout_lstm(\n",
        "     data, test_start=\"2020-01-01\",\n",
        "     feature_cols=selected_features, target_col=\"target\",\n",
        "     lookback=26, epochs=50, batch_size=64\n",
        " )\n",
        "print(result_holdout[\"val_metrics\"])\n",
        "print(result_holdout[\"test_metrics\"])\n",
        "\n",
        "# # Walk-forward CV\n",
        "result_wf = run_walkforward_lstm(\n",
        "    data, cfg=CVConfig(val_span=52, step=26, min_train_span=156),\n",
        "    feature_cols=selected_features, target_col=\"target\",\n",
        "    lookback=26, epochs=50, batch_size=64\n",
        ")\n",
        "print(result_wf[\"avg_metrics\"], \"folds:\", result_wf[\"n_folds\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7u2n14VrWRI",
        "outputId": "1798cee7-1b8e-4b33-e5e6-7a258beb39b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc': 0.46153846153846156, 'f1_macro': 0.3680555555555556, 'roc_auc': 0.375, 'rmse_prob': 0.5014988206945712, 'rmse_hard': 0.7337993857053428, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.500     0.786     0.611        14\\n           1      0.250     0.083     0.125        12\\n\\n    accuracy                          0.462        26\\n   macro avg      0.375     0.435     0.368        26\\nweighted avg      0.385     0.462     0.387        26\\n', 'confusion_matrix': [[11, 3], [11, 1]]}\n",
            "{'acc': 0.47924528301886793, 'f1_macro': 0.3239795918367347, 'roc_auc': 0.5689832249229716, 'rmse_prob': 0.5012676430906889, 'rmse_hard': 0.721633367424991, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.479     1.000     0.648       127\\n           1      0.000     0.000     0.000       138\\n\\n    accuracy                          0.479       265\\n   macro avg      0.240     0.500     0.324       265\\nweighted avg      0.230     0.479     0.311       265\\n', 'confusion_matrix': [[127, 0], [138, 0]]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2ed0c5d800> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2ed06d3060> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc': 0.5608974358974359, 'f1_macro': 0.41716447685698554, 'roc_auc': 0.5363406793039592, 'rmse_prob': 0.49841853275066, 'rmse_hard': 0.6586742067457287} folds: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_synthetic_lstm_dataset(\n",
        "    n=1500,\n",
        "    seed=7,\n",
        "    start_date=\"1990-01-05\",\n",
        "    freq=\"W-FRI\",\n",
        "    p_stick=0.97,                 # state persistence (higher = easier)\n",
        "    mu=(-0.02, 0.0, 0.02),        # per-state drift applied to NEXT return\n",
        "    sigma=0.004,                  # noise on returns (lower = easier)\n",
        "    prob_noise=0.02               # noise on p_state probabilities (lower = easier)\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with columns:\n",
        "      ['ret_1','Last Price','p_state0','p_state1','p_state2','spi_6','target']\n",
        "    Index is a DatetimeIndex. 'target' = 1 if next-period return > 0, else 0.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    dates = pd.date_range(start_date, periods=n, freq=freq)\n",
        "    K = 3  # dry/neutral/wet\n",
        "\n",
        "    # Highly persistent Markov chain\n",
        "    trans = np.full((K, K), (1 - p_stick) / (K - 1))\n",
        "    np.fill_diagonal(trans, p_stick)\n",
        "\n",
        "    s = np.zeros(n, dtype=int)\n",
        "    for t in range(1, n):\n",
        "        s[t] = rng.choice(K, p=trans[s[t-1]])\n",
        "\n",
        "    # Near one-hot state probabilities with tiny noise\n",
        "    p_states = np.full((n, K), prob_noise / (K - 1))\n",
        "    p_states[np.arange(n), s] = 1 - prob_noise\n",
        "\n",
        "    # SPI correlated with state\n",
        "    spi_means = np.array([-1.0, 0.0, 1.0])\n",
        "    spi = spi_means[s] + rng.normal(0, 0.2, size=n)\n",
        "\n",
        "    # Returns: r_t depends on PREVIOUS state s_{t-1}  -> avoids leakage in your windowing\n",
        "    r = np.zeros(n)\n",
        "    for t in range(1, n):\n",
        "        r[t] = mu[s[t-1]] + rng.normal(0, sigma)\n",
        "\n",
        "    # Price path from log-returns (approx)\n",
        "    price = 100 * np.exp(np.cumsum(r))\n",
        "\n",
        "    # target_t = 1[ r_{t+1} > 0 ]  (binary next-period direction)\n",
        "    target = (pd.Series(r).shift(-1) > 0).astype(\"float\").to_numpy()\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"date\": dates,\n",
        "        \"return\": r,\n",
        "        \"Last Price\": price,\n",
        "        \"p_state0\": p_states[:, 0],\n",
        "        \"p_state1\": p_states[:, 1],\n",
        "        \"p_state2\": p_states[:, 2],\n",
        "        \"spi\": spi,\n",
        "        \"target\": target\n",
        "    }).set_index(\"date\")\n",
        "\n",
        "    # Drop last row where target is NaN (no next period)\n",
        "    df = df.dropna(subset=[\"target\"]).copy()\n",
        "    df[\"target\"] = df[\"target\"].astype(int)\n",
        "    return df\n",
        "\n",
        "# 1) Make data\n",
        "df_syn = generate_synthetic_lstm_dataset()\n",
        "\n",
        "# 2) (Optional) only certain features? e.g., use all default ones:\n",
        "FEATURES_SYN = [\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi\"]\n",
        "\n",
        "# 3) Run your existing functions:\n",
        "# df_syn.index already set; it's sorted\n",
        "res = run_holdout_lstm(\n",
        "    df_syn,\n",
        "    test_start=\"2018-01-05\",      # pick a split date inside the range\n",
        "    feature_cols=FEATURES_SYN,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    epochs=40,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "print(\"VAL:\", res[\"val_metrics\"])\n",
        "print(\"TEST:\", res[\"test_metrics\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buUOoXYbwCUU",
        "outputId": "589d563e-6dcb-4372-da85-0c8e91ca43a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL: {'acc': 0.8846153846153846, 'f1_macro': 0.46938775510204084, 'roc_auc': 0.8405797101449275, 'rmse_prob': 0.2902522992931794, 'rmse_hard': 0.3396831102433787, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.000     0.000     0.000         3\\n           1      0.885     1.000     0.939        23\\n\\n    accuracy                          0.885        26\\n   macro avg      0.442     0.500     0.469        26\\nweighted avg      0.783     0.885     0.830        26\\n', 'confusion_matrix': [[0, 3], [0, 23]]}\n",
            "TEST: {'acc': 0.9230769230769231, 'f1_macro': 0.48, 'roc_auc': 0.6666666666666666, 'rmse_prob': 0.3211704535610317, 'rmse_hard': 0.2773500981126146, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.000     0.000     0.000         1\\n           1      0.923     1.000     0.960        12\\n\\n    accuracy                          0.923        13\\n   macro avg      0.462     0.500     0.480        13\\nweighted avg      0.852     0.923     0.886        13\\n', 'confusion_matrix': [[0, 1], [0, 12]]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Logistic-Regression-only pipeline for binary next-period direction\n",
        "# Assumes your DataFrame ALREADY contains the binary target column.\n",
        "#\n",
        "# Expected columns at minimum:\n",
        "#   [\"date\",\"return\",\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi\",\"target\"]\n",
        "# Set a datetime index and sort ascending before running:\n",
        "#   df.index = pd.to_datetime(df[\"date\"]); df = df.sort_index()\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ----------------------------\n",
        "# Config and splitting helpers\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class CVConfig:\n",
        "    val_span: int        # e.g., 52 (weeks)\n",
        "    step: int            # e.g., 26\n",
        "    min_train_span: int  # e.g., 156\n",
        "\n",
        "FEATURES = [\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi_6\"]\n",
        "\n",
        "def make_static_splits(df: pd.DataFrame, test_start: str | pd.Timestamp):\n",
        "    \"\"\"\n",
        "    Chronological hold-out split: all dates >= test_start go to test.\n",
        "    The remainder will later be split into train/val.\n",
        "    \"\"\"\n",
        "    mask_test = df.index >= pd.to_datetime(test_start)\n",
        "    return df.loc[~mask_test].copy(), df.loc[mask_test].copy()\n",
        "\n",
        "def expanding_cv_folds(trainval_df: pd.DataFrame, cfg: CVConfig):\n",
        "    \"\"\"\n",
        "    Walk-forward (expanding-window) CV generator over train/val pairs.\n",
        "    \"\"\"\n",
        "    n = len(trainval_df); idx = np.arange(n)\n",
        "    train_end = cfg.min_train_span\n",
        "    while True:\n",
        "        val_end = train_end + cfg.val_span\n",
        "        if val_end > n: break\n",
        "        train_mask = idx < train_end\n",
        "        val_mask   = (idx >= train_end) & (idx < val_end)\n",
        "        yield trainval_df.loc[train_mask].copy(), trainval_df.loc[val_mask].copy()\n",
        "        train_end += cfg.step\n",
        "\n",
        "# ----------------------------\n",
        "# Metrics\n",
        "# ----------------------------\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Root Mean Squared Error. For classification, pass probabilities for √Brier.\"\"\"\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "\n",
        "# ----------------------------\n",
        "# Logistic Regression model\n",
        "# ----------------------------\n",
        "def fit_eval_logreg(\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    class_weight=\"balanced\",           # or dict, or None\n",
        "    C=1.0,                             # inverse regularization strength\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Scale on train only, fit Logistic Regression, evaluate on val.\n",
        "    Returns (model, metrics_dict, (pred, proba), scaler).\n",
        "    \"\"\"\n",
        "    # Clean\n",
        "    train_df = train_df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    val_df   = val_df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    y_tr = train_df[target_col].astype(int).values\n",
        "    y_va = val_df[target_col].astype(int).values\n",
        "    if len(np.unique(y_tr)) < 2:\n",
        "        raise ValueError(\"Training split contains only one class; LogisticRegression needs at least 2 classes.\")\n",
        "\n",
        "    # Scale on train only\n",
        "    scaler = StandardScaler().fit(train_df[feature_cols])\n",
        "    X_tr = scaler.transform(train_df[feature_cols])\n",
        "    X_va = scaler.transform(val_df[feature_cols])\n",
        "\n",
        "    # Model\n",
        "    clf = LogisticRegression(\n",
        "        class_weight=class_weight,\n",
        "        C=C,\n",
        "        penalty=penalty,\n",
        "        solver=solver,\n",
        "        max_iter=max_iter\n",
        "    )\n",
        "    clf.fit(X_tr, y_tr)\n",
        "\n",
        "    # Probabilities (index of positive class=1)\n",
        "    if 1 in clf.classes_:\n",
        "        pos_idx = list(clf.classes_).index(1)\n",
        "        proba = clf.predict_proba(X_va)[:, pos_idx]\n",
        "    else:\n",
        "        # Shouldn't happen if y_tr had both classes, but guard anyway\n",
        "        proba = np.zeros_like(y_va, dtype=float)\n",
        "\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": float(accuracy_score(y_va, pred)),\n",
        "        \"f1_macro\": float(f1_score(y_va, pred, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_va, proba)) if len(np.unique(y_va)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_va, proba),                 # √Brier on probabilities\n",
        "        \"rmse_hard\": rmse(y_va, pred.astype(float)),    # sqrt(error rate) on hard labels\n",
        "        \"classification_report\": classification_report(y_va, pred, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_va, pred).tolist()\n",
        "    }\n",
        "    return clf, metrics, (pred, proba), scaler\n",
        "\n",
        "# ----------------------------\n",
        "# End-to-end runners (LogReg only)\n",
        "# ----------------------------\n",
        "def run_holdout_logreg(\n",
        "    df: pd.DataFrame,\n",
        "    test_start: str | pd.Timestamp,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    class_weight=\"balanced\",\n",
        "    C=1.0,\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000,\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Split Train/Val/Test chronologically (hold-out; df already has 'target')\n",
        "    2) Train on Train, tune on Val\n",
        "    3) Retrain on Train+Val and evaluate on Test\n",
        "    Returns dict with validation metrics, test metrics, and split boundaries.\n",
        "    \"\"\"\n",
        "    # Sort and basic cleaning\n",
        "    df = df.sort_index()\n",
        "    df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    # Train/Val/Test split\n",
        "    trainval, test = make_static_splits(df, test_start)\n",
        "\n",
        "    # carve validation from tail of trainval\n",
        "    VAL_SPAN = min(52, max(26, len(trainval)//10))  # ~10% bounded into weekly-ish range\n",
        "    val = trainval.iloc[-VAL_SPAN:].copy()\n",
        "    train = trainval.iloc[:-VAL_SPAN].copy()\n",
        "\n",
        "    # ---- validation round\n",
        "    _, val_metrics, _, _ = fit_eval_logreg(\n",
        "        train, val,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        class_weight=class_weight, C=C, penalty=penalty, solver=solver, max_iter=max_iter\n",
        "    )\n",
        "\n",
        "    # ---- final training on Train+Val, evaluate on Test\n",
        "    trv = pd.concat([train, val]).copy()\n",
        "    trv = trv.dropna(subset=feature_cols + [target_col])\n",
        "    test = test.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "    y_trv = trv[target_col].astype(int).values\n",
        "    if len(np.unique(y_trv)) < 2:\n",
        "        raise ValueError(\"Train+Val contains only one class; cannot fit LogisticRegression.\")\n",
        "\n",
        "    scaler = StandardScaler().fit(trv[feature_cols])\n",
        "    X_trv = scaler.transform(trv[feature_cols]); y_trv = trv[target_col].astype(int).values\n",
        "    X_te  = scaler.transform(test[feature_cols]); y_te  = test[target_col].astype(int).values\n",
        "\n",
        "    clf = LogisticRegression(\n",
        "        class_weight=class_weight, C=C, penalty=penalty, solver=solver, max_iter=max_iter\n",
        "    ).fit(X_trv, y_trv)\n",
        "\n",
        "    if 1 in clf.classes_:\n",
        "        pos_idx = list(clf.classes_).index(1)\n",
        "        proba_te = clf.predict_proba(X_te)[:, pos_idx]\n",
        "    else:\n",
        "        proba_te = np.zeros_like(y_te, dtype=float)\n",
        "\n",
        "    pred_te  = (proba_te >= 0.5).astype(int)\n",
        "\n",
        "    test_metrics = {\n",
        "        \"acc\": float(accuracy_score(y_te, pred_te)),\n",
        "        \"f1_macro\": float(f1_score(y_te, pred_te, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_te, proba_te)) if len(np.unique(y_te)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_te, proba_te),                 # √Brier on probabilities\n",
        "        \"rmse_hard\": rmse(y_te, pred_te.astype(float)),    # sqrt(error rate) on hard labels\n",
        "        \"classification_report\": classification_report(y_te, pred_te, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_te, pred_te).tolist()\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"splits\": {\n",
        "            \"train\": (train.index.min(), train.index.max()),\n",
        "            \"val\":   (val.index.min(),   val.index.max()),\n",
        "            \"test\":  (test.index.min(),  test.index.max())\n",
        "        }\n",
        "    }\n",
        "\n",
        "def run_walkforward_logreg(\n",
        "    df: pd.DataFrame,\n",
        "    cfg: CVConfig = CVConfig(val_span=52, step=26, min_train_span=156),\n",
        "    feature_cols=FEATURES,\n",
        "    target_col: str = \"target\",\n",
        "    class_weight=\"balanced\",\n",
        "    C=1.0,\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=2000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Expanding-window CV for Logistic Regression. Assumes df already has 'target'.\n",
        "    Returns average metrics across folds and per-fold metrics list.\n",
        "    \"\"\"\n",
        "    df = df.sort_index()\n",
        "    df = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    fold_metrics = []\n",
        "    for df_tr, df_va in expanding_cv_folds(df, cfg):\n",
        "        df_tr = df_tr.dropna(subset=feature_cols + [target_col])\n",
        "        df_va = df_va.dropna(subset=feature_cols + [target_col])\n",
        "\n",
        "        # skip fold if training has <2 classes\n",
        "        if len(np.unique(df_tr[target_col].astype(int).values)) < 2:\n",
        "            continue\n",
        "\n",
        "        _, m, _, _ = fit_eval_logreg(\n",
        "            df_tr, df_va,\n",
        "            feature_cols=feature_cols, target_col=target_col,\n",
        "            class_weight=class_weight, C=C, penalty=penalty, solver=solver, max_iter=max_iter\n",
        "        )\n",
        "        fold_metrics.append(m)\n",
        "\n",
        "    def avg(key):\n",
        "        vals = [fm.get(key, np.nan) for fm in fold_metrics if not np.isnan(fm.get(key, np.nan))]\n",
        "        return float(np.mean(vals)) if vals else np.nan\n",
        "\n",
        "    return {\n",
        "        \"avg_metrics\": {\n",
        "            \"acc\": avg(\"acc\"),\n",
        "            \"f1_macro\": avg(\"f1_macro\"),\n",
        "            \"roc_auc\": avg(\"roc_auc\"),\n",
        "            \"rmse_prob\": avg(\"rmse_prob\"),\n",
        "            \"rmse_hard\": avg(\"rmse_hard\"),\n",
        "        },\n",
        "        \"folds\": fold_metrics,\n",
        "        \"n_folds\": len(fold_metrics),\n",
        "        \"cfg\": cfg\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (uncomment)\n",
        "# ----------------------------\n",
        "data.index = pd.to_datetime(data[\"date\"])\n",
        "data = data.sort_index()\n",
        "#\n",
        "selected_features = [\"Last Price\"]\n",
        "# # Hold-out\n",
        "result_holdout = run_holdout_logreg(\n",
        "    data, test_start=\"2020-01-01\",\n",
        "    feature_cols=[\"Last Price\", \"ret_1\", \"spi_6\"],\n",
        "    target_col=\"target\",\n",
        "    class_weight=\"balanced\", C=1.0, penalty=\"l2\", solver=\"lbfgs\"\n",
        ")\n",
        "print(result_holdout[\"val_metrics\"])\n",
        "print(result_holdout[\"test_metrics\"])\n",
        "#\n",
        "# # Walk-forward CV\n",
        "# result_wf = run_walkforward_logreg(\n",
        "#     df, cfg=CVConfig(val_span=52, step=26, min_train_span=156),\n",
        "#     feature_cols=[\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi\"],\n",
        "#     target_col=\"target\",\n",
        "#     class_weight=\"balanced\", C=1.0, penalty=\"l2\", solver=\"lbfgs\"\n",
        "# )\n",
        "# print(result_wf[\"avg_metrics\"], \"folds:\", result_wf[\"n_folds\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnmSFe6Z2ExO",
        "outputId": "c8edbbc6-6f9c-4081-8788-ea0c5eff2edf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc': 0.4807692307692308, 'f1_macro': 0.47592385218365063, 'roc_auc': 0.4851190476190476, 'rmse_prob': 0.5002640895967102, 'rmse_hard': 0.7205766921228921, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.526     0.357     0.426        28\\n           1      0.455     0.625     0.526        24\\n\\n    accuracy                          0.481        52\\n   macro avg      0.490     0.491     0.476        52\\nweighted avg      0.493     0.481     0.472        52\\n', 'confusion_matrix': [[10, 18], [9, 15]]}\n",
            "{'acc': 0.5017182130584192, 'f1_macro': 0.47959447959447954, 'roc_auc': 0.4743904743904744, 'rmse_prob': 0.5007000969151711, 'rmse_hard': 0.7058907755039591, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.495     0.720     0.587       143\\n           1      0.518     0.291     0.372       148\\n\\n    accuracy                          0.502       291\\n   macro avg      0.507     0.505     0.480       291\\nweighted avg      0.507     0.502     0.478       291\\n', 'confusion_matrix': [[103, 40], [105, 43]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LSTM (single hold-out: Train -> Val -> retrain on Train+Val -> Test)\n",
        "# Assumes df already has a binary 'target' column and a datetime index.\n",
        "#\n",
        "# - Train on full Train, validate on Val (choose threshold on Val)\n",
        "# - Retrain on Train+Val (no test peeking), evaluate once on Test\n",
        "# - Includes RMSE metrics (√Brier on probabilities and RMSE on hard labels)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# --------- Configurable features ---------\n",
        "FEATURES = [\"Last Price\",\"p_state0\",\"p_state1\",\"p_state2\",\"spi_6\"]\n",
        "\n",
        "# --------- Split & metrics helpers ---------\n",
        "def make_static_splits(df: pd.DataFrame, test_start: str|pd.Timestamp):\n",
        "    m = df.index >= pd.to_datetime(test_start)\n",
        "    return df.loc[~m].copy(), df.loc[m].copy()\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
        "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "\n",
        "def best_threshold(y_true, proba, metric=\"f1_macro\"):\n",
        "    grid = np.linspace(0.05, 0.95, 181)\n",
        "    best_t, best = 0.5, -1.0\n",
        "    for t in grid:\n",
        "        pred = (proba >= t).astype(int)\n",
        "        score = f1_score(y_true, pred, average=\"macro\") if metric==\"f1_macro\" else f1_score(y_true, pred)\n",
        "        if score > best:\n",
        "            best, best_t = score, t\n",
        "    return float(best_t)\n",
        "\n",
        "def safe_logloss(y_true, proba):\n",
        "    # clip to avoid log(0) under any version\n",
        "    p = np.clip(np.asarray(proba, dtype=float), 1e-7, 1-1e-7)\n",
        "    return float(log_loss(y_true, p, labels=[0, 1]))\n",
        "\n",
        "def _class_weight_from_labels(y):\n",
        "    # inverse-frequency weights normalized s.t. average weight ≈ 1\n",
        "    y = np.asarray(y, int)\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    K = len(classes); total = counts.sum()\n",
        "    return {int(c): float(total/(K*cnt)) for c, cnt in zip(classes, counts)}\n",
        "\n",
        "# --------- Sequences & model ---------\n",
        "def make_sequences(df: pd.DataFrame, lookback: int, feature_cols, target_col=\"target\"):\n",
        "    \"\"\"\n",
        "    Build (X, y) where each X[i] is a window of length `lookback` ending at t-1,\n",
        "    and y[i] is the target at time t (no look-ahead).\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    V = df[feature_cols].values\n",
        "    t = df[target_col].values\n",
        "    for i in range(lookback, len(df)):\n",
        "        X.append(V[i-lookback:i])\n",
        "        y.append(t[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_lstm(input_dim, lookback, units=64, dropout=0.2, lr=1e-3):\n",
        "    m = keras.Sequential([\n",
        "        layers.Input(shape=(lookback, input_dim)),\n",
        "        layers.LSTM(units, return_sequences=False),\n",
        "        layers.Dropout(dropout),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    m.compile(optimizer=keras.optimizers.Adam(lr),\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "    return m\n",
        "\n",
        "# --------- Core training / evaluation blocks ---------\n",
        "def fit_lstm_train_val(\n",
        "    df_train: pd.DataFrame,\n",
        "    df_val: pd.DataFrame,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    class_weight=\"balanced\",  # \"balanced\" -> auto inverse-freq; or pass a dict {0:w0,1:w1}; or None\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Fit scaler on TRAIN, train LSTM on TRAIN sequences with early-stopping on VAL sequences.\n",
        "    Return: model, scaler, y_val_seq, proba_val, val_metrics_at_0p5\n",
        "    \"\"\"\n",
        "    # Clean\n",
        "    df_train = df_train.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    df_val   = df_val.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    # Scale on TRAIN only\n",
        "    scaler = StandardScaler().fit(df_train[feature_cols])\n",
        "    tr = df_train.copy(); va = df_val.copy()\n",
        "    tr[feature_cols] = scaler.transform(tr[feature_cols])\n",
        "    va[feature_cols] = scaler.transform(va[feature_cols])\n",
        "\n",
        "    # Sequences\n",
        "    X_tr, y_tr = make_sequences(tr, lookback, feature_cols, target_col)\n",
        "    X_va, y_va = make_sequences(va, lookback, feature_cols, target_col)\n",
        "    if len(X_tr) == 0 or len(X_va) == 0:\n",
        "        raise ValueError(\"Not enough data to form sequences. Reduce lookback or provide more rows.\")\n",
        "\n",
        "    # Class weights\n",
        "    cw = None\n",
        "    if class_weight == \"balanced\":\n",
        "        cw = _class_weight_from_labels(y_tr)\n",
        "    elif isinstance(class_weight, dict):\n",
        "        cw = class_weight\n",
        "\n",
        "    # Train\n",
        "    model = build_lstm(input_dim=X_tr.shape[2], lookback=lookback)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=(X_va, y_va),\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=cw\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=500,\n",
        "    batch_size=64,\n",
        "    shuffle=False,  # time series\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
        "      ],\n",
        "    )\n",
        "\n",
        "    import pandas as pd, matplotlib.pyplot as plt\n",
        "    pd.DataFrame(history.history)[[\"loss\",\"val_loss\"]].plot()\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training vs Validation Loss\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Validation outputs & metrics at threshold 0.5\n",
        "    proba_va = model.predict(X_va, verbose=0).ravel()\n",
        "    pred_va  = (proba_va >= 0.5).astype(int)\n",
        "    val_metrics_0p5 = {\n",
        "        \"acc\": float(accuracy_score(y_va, pred_va)),\n",
        "        \"f1_macro\": float(f1_score(y_va, pred_va, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_va, proba_va)) if len(np.unique(y_va)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_va, proba_va),\n",
        "        \"rmse_hard\": rmse(y_va, pred_va.astype(float)),\n",
        "        \"classification_report\": classification_report(y_va, pred_va, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_va, pred_va).tolist()\n",
        "    }\n",
        "    return model, scaler, y_va, proba_va, val_metrics_0p5\n",
        "\n",
        "def fit_lstm_final_on_trv(\n",
        "    df_trv: pd.DataFrame,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    class_weight=\"balanced\",\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrain LSTM on TRAIN+VAL with an internal holdout (from the tail of trv sequences) for early-stopping.\n",
        "    \"\"\"\n",
        "    df_trv = df_trv.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "    scaler = StandardScaler().fit(df_trv[feature_cols])\n",
        "    trv = df_trv.copy()\n",
        "    trv[feature_cols] = scaler.transform(trv[feature_cols])\n",
        "\n",
        "    X_trv, y_trv = make_sequences(trv, lookback, feature_cols, target_col)\n",
        "    if len(X_trv) == 0:\n",
        "        raise ValueError(\"Not enough data in Train+Val to form sequences. Reduce lookback.\")\n",
        "\n",
        "    # small internal validation from tail of Train+Val (no Test peeking)\n",
        "    monitor_k = max( min( max(len(X_trv)//10, 32), 256 ), 16 )\n",
        "    monitor_k = min(monitor_k, len(X_trv)//5) if len(X_trv) >= 5 else 0\n",
        "    if monitor_k > 0:\n",
        "        X_trv_train, y_trv_train = X_trv[:-monitor_k], y_trv[:-monitor_k]\n",
        "        X_trv_valm,  y_trv_valm  = X_trv[-monitor_k:], y_trv[-monitor_k:]\n",
        "        val_monitor = (X_trv_valm, y_trv_valm)\n",
        "    else:\n",
        "        X_trv_train, y_trv_train = X_trv, y_trv\n",
        "        val_monitor = None\n",
        "\n",
        "    cw = None\n",
        "    if class_weight == \"balanced\":\n",
        "        cw = _class_weight_from_labels(y_trv_train)\n",
        "    elif isinstance(class_weight, dict):\n",
        "        cw = class_weight\n",
        "\n",
        "    model = build_lstm(input_dim=X_trv.shape[2], lookback=lookback)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    model.fit(\n",
        "        X_trv_train, y_trv_train,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=val_monitor,\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=cw\n",
        "    )\n",
        "    return model, scaler\n",
        "\n",
        "def eval_lstm_on_split(model, scaler, df_split, feature_cols=FEATURES, target_col=\"target\", lookback=26, threshold=0.5):\n",
        "    df_split = df_split.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    df_split[feature_cols] = scaler.transform(df_split[feature_cols])\n",
        "\n",
        "    X, y = make_sequences(df_split, lookback, feature_cols, target_col)\n",
        "    if len(X) == 0:\n",
        "        raise ValueError(\"Not enough data in this split to form sequences at the chosen lookback.\")\n",
        "    proba = model.predict(X, verbose=0).ravel()\n",
        "    pred  = (proba >= threshold).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"acc\": float(accuracy_score(y, pred)),\n",
        "        \"f1_macro\": float(f1_score(y, pred, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y, proba)) if len(np.unique(y)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y, proba),\n",
        "        \"rmse_hard\": rmse(y, pred.astype(float)),\n",
        "        \"classification_report\": classification_report(y, pred, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y, pred).tolist()\n",
        "    }\n",
        "    return metrics, y, proba, pred\n",
        "\n",
        "# --------- End-to-end (no CV; Train -> Val -> retrain -> Test) ---------\n",
        "def run_holdout_lstm_single(\n",
        "    df: pd.DataFrame,\n",
        "    test_start: str|pd.Timestamp,\n",
        "    feature_cols=FEATURES,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    val_span: int|None = None,      # if None -> ~10% bounded in [26, 52]\n",
        "    class_weight=\"balanced\",        # \"balanced\", dict, or None\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    patience=5,\n",
        "    tune_threshold: bool = True,\n",
        "    tune_metric: str = \"f1_macro\"   # which metric to optimize when picking threshold\n",
        "):\n",
        "    # 0) Sort; basic cleaning deferred to child funcs to avoid accidental leakage\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # 1) Train/Val/Test split\n",
        "    trainval, test = make_static_splits(df, test_start)\n",
        "    if val_span is None:\n",
        "        val_span = min(52, max(26, len(trainval)//10))\n",
        "    val   = trainval.iloc[-val_span:].copy()\n",
        "    train = trainval.iloc[:-val_span].copy()\n",
        "\n",
        "    # 2) Train on full Train, validate on Val (get raw proba for threshold tuning)\n",
        "    model_tr, scaler_tr, y_va, proba_va, val_metrics_0p5 = fit_lstm_train_val(\n",
        "        train, val,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        lookback=lookback,\n",
        "        class_weight=class_weight,\n",
        "        epochs=epochs, batch_size=batch_size, patience=patience\n",
        "    )\n",
        "\n",
        "    # 3) Choose threshold on Val (optional)\n",
        "    t_star = 0.5\n",
        "    if tune_threshold:\n",
        "        t_star = best_threshold(y_va, proba_va, metric=tune_metric)\n",
        "\n",
        "    # Recompute Val metrics at tuned threshold for reporting\n",
        "    # (reuse trained model_tr + scaler_tr to avoid re-fitting)\n",
        "    df_val_scaled = val.dropna(subset=feature_cols + [target_col]).copy()\n",
        "    df_val_scaled[feature_cols] = scaler_tr.transform(df_val_scaled[feature_cols])\n",
        "    X_va2, y_va2 = make_sequences(df_val_scaled, lookback, feature_cols, target_col)\n",
        "    proba_va2 = model_tr.predict(X_va2, verbose=0).ravel()\n",
        "    pred_va2  = (proba_va2 >= t_star).astype(int)\n",
        "    val_metrics_tuned = {\n",
        "        \"acc\": float(accuracy_score(y_va2, pred_va2)),\n",
        "        \"f1_macro\": float(f1_score(y_va2, pred_va2, average=\"macro\")),\n",
        "        \"roc_auc\": float(roc_auc_score(y_va2, proba_va2)) if len(np.unique(y_va2)) == 2 else np.nan,\n",
        "        \"rmse_prob\": rmse(y_va2, proba_va2),\n",
        "        \"rmse_hard\": rmse(y_va2, pred_va2.astype(float)),\n",
        "        \"classification_report\": classification_report(y_va2, pred_va2, digits=3),\n",
        "        \"confusion_matrix\": confusion_matrix(y_va2, pred_va2).tolist()\n",
        "    }\n",
        "\n",
        "    # 4) Retrain on Train+Val, evaluate once on Test using t_star (no test peeking)\n",
        "    trv = pd.concat([train, val]).copy()\n",
        "    model_final, scaler_final = fit_lstm_final_on_trv(\n",
        "        trv,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        lookback=lookback,\n",
        "        class_weight=class_weight,\n",
        "        epochs=epochs, batch_size=batch_size, patience=patience\n",
        "    )\n",
        "    test_metrics, *_ = eval_lstm_on_split(\n",
        "        model_final, scaler_final, test,\n",
        "        feature_cols=feature_cols, target_col=target_col,\n",
        "        lookback=lookback, threshold=t_star\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"val_metrics_tuned\": val_metrics_tuned,\n",
        "        \"val_metrics_at_0p5\": val_metrics_0p5,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"threshold_used\": float(t_star),\n",
        "        \"splits\": {\n",
        "            \"train\": (train.index.min(), train.index.max()),\n",
        "            \"val\":   (val.index.min(),   val.index.max()),\n",
        "            \"test\":  (test.index.min(),  test.index.max())\n",
        "        },\n",
        "        \"feature_cols\": list(feature_cols),\n",
        "        \"lookback\": lookback\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (uncomment)\n",
        "# ----------------------------\n",
        "data.index = pd.to_datetime(data[\"date\"])\n",
        "data = data.sort_index()\n",
        "out = run_holdout_lstm_single(\n",
        "    data,\n",
        "    test_start=\"2018-01-01\",\n",
        "    feature_cols=[\"spi_6\"],\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    val_span=None,            # or set e.g., 52\n",
        "    class_weight=\"balanced\",  # or pass dict {0: w0, 1: w1} or None\n",
        "    epochs=500, batch_size=64, patience=5,\n",
        "    tune_threshold=True, tune_metric=\"f1_macro\"\n",
        ")\n",
        "print(\"VAL (tuned):\", out[\"val_metrics_tuned\"])\n",
        "print(\"VAL (0.5):  \", out[\"val_metrics_at_0p5\"])\n",
        "print(\"TEST:       \", out[\"test_metrics\"])\n",
        "print(\"Threshold:  \", out[\"threshold_used\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "36xY7zum69T_",
        "outputId": "cb5ffbc3-ac58-4aeb-e379-b116f4ab9cb4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5451 - loss: 0.6781 - val_accuracy: 0.6538 - val_loss: 0.6685 - learning_rate: 0.0010\n",
            "Epoch 2/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5563 - loss: 0.6808 - val_accuracy: 0.6154 - val_loss: 0.6697 - learning_rate: 0.0010\n",
            "Epoch 3/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5472 - loss: 0.6773 - val_accuracy: 0.6154 - val_loss: 0.6697 - learning_rate: 0.0010\n",
            "Epoch 4/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5473 - loss: 0.6775 - val_accuracy: 0.6154 - val_loss: 0.6692 - learning_rate: 0.0010\n",
            "Epoch 5/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5579 - loss: 0.6775 - val_accuracy: 0.6154 - val_loss: 0.6691 - learning_rate: 0.0010\n",
            "Epoch 6/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5551 - loss: 0.6773 - val_accuracy: 0.6154 - val_loss: 0.6692 - learning_rate: 0.0010\n",
            "Epoch 7/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5639 - loss: 0.6759 - val_accuracy: 0.6154 - val_loss: 0.6684 - learning_rate: 5.0000e-04\n",
            "Epoch 8/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5420 - loss: 0.6796 - val_accuracy: 0.6154 - val_loss: 0.6681 - learning_rate: 5.0000e-04\n",
            "Epoch 9/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.5454 - loss: 0.6761 - val_accuracy: 0.6154 - val_loss: 0.6681 - learning_rate: 5.0000e-04\n",
            "Epoch 10/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.5524 - loss: 0.6779 - val_accuracy: 0.6154 - val_loss: 0.6675 - learning_rate: 5.0000e-04\n",
            "Epoch 11/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.5457 - loss: 0.6772 - val_accuracy: 0.6154 - val_loss: 0.6675 - learning_rate: 5.0000e-04\n",
            "Epoch 12/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5649 - loss: 0.6757 - val_accuracy: 0.6154 - val_loss: 0.6686 - learning_rate: 5.0000e-04\n",
            "Epoch 13/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5677 - loss: 0.6724 - val_accuracy: 0.6154 - val_loss: 0.6685 - learning_rate: 5.0000e-04\n",
            "Epoch 14/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5421 - loss: 0.6774 - val_accuracy: 0.6154 - val_loss: 0.6674 - learning_rate: 5.0000e-04\n",
            "Epoch 15/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5601 - loss: 0.6759 - val_accuracy: 0.6154 - val_loss: 0.6673 - learning_rate: 5.0000e-04\n",
            "Epoch 16/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5594 - loss: 0.6762 - val_accuracy: 0.6154 - val_loss: 0.6665 - learning_rate: 5.0000e-04\n",
            "Epoch 17/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5627 - loss: 0.6735 - val_accuracy: 0.6154 - val_loss: 0.6670 - learning_rate: 5.0000e-04\n",
            "Epoch 18/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5537 - loss: 0.6756 - val_accuracy: 0.6154 - val_loss: 0.6669 - learning_rate: 5.0000e-04\n",
            "Epoch 19/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5578 - loss: 0.6746 - val_accuracy: 0.6154 - val_loss: 0.6669 - learning_rate: 5.0000e-04\n",
            "Epoch 20/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5638 - loss: 0.6753 - val_accuracy: 0.6154 - val_loss: 0.6667 - learning_rate: 5.0000e-04\n",
            "Epoch 21/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5614 - loss: 0.6760 - val_accuracy: 0.6154 - val_loss: 0.6656 - learning_rate: 5.0000e-04\n",
            "Epoch 22/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5534 - loss: 0.6744 - val_accuracy: 0.6154 - val_loss: 0.6660 - learning_rate: 5.0000e-04\n",
            "Epoch 23/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5520 - loss: 0.6747 - val_accuracy: 0.6154 - val_loss: 0.6651 - learning_rate: 5.0000e-04\n",
            "Epoch 24/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5615 - loss: 0.6742 - val_accuracy: 0.6154 - val_loss: 0.6643 - learning_rate: 5.0000e-04\n",
            "Epoch 25/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5611 - loss: 0.6727 - val_accuracy: 0.6154 - val_loss: 0.6650 - learning_rate: 5.0000e-04\n",
            "Epoch 26/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5568 - loss: 0.6756 - val_accuracy: 0.6154 - val_loss: 0.6665 - learning_rate: 5.0000e-04\n",
            "Epoch 27/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5587 - loss: 0.6733 - val_accuracy: 0.6154 - val_loss: 0.6667 - learning_rate: 5.0000e-04\n",
            "Epoch 28/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5596 - loss: 0.6756 - val_accuracy: 0.6154 - val_loss: 0.6659 - learning_rate: 5.0000e-04\n",
            "Epoch 29/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5548 - loss: 0.6753 - val_accuracy: 0.6154 - val_loss: 0.6661 - learning_rate: 5.0000e-04\n",
            "Epoch 30/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5621 - loss: 0.6728 - val_accuracy: 0.6154 - val_loss: 0.6658 - learning_rate: 2.5000e-04\n",
            "Epoch 31/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5663 - loss: 0.6697 - val_accuracy: 0.6154 - val_loss: 0.6657 - learning_rate: 2.5000e-04\n",
            "Epoch 32/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5633 - loss: 0.6695 - val_accuracy: 0.6154 - val_loss: 0.6652 - learning_rate: 2.5000e-04\n",
            "Epoch 33/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5637 - loss: 0.6709 - val_accuracy: 0.6154 - val_loss: 0.6648 - learning_rate: 2.5000e-04\n",
            "Epoch 34/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.5696 - loss: 0.6712 - val_accuracy: 0.6154 - val_loss: 0.6641 - learning_rate: 2.5000e-04\n",
            "Epoch 35/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5714 - loss: 0.6739 - val_accuracy: 0.6154 - val_loss: 0.6627 - learning_rate: 2.5000e-04\n",
            "Epoch 36/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.5533 - loss: 0.6738 - val_accuracy: 0.6154 - val_loss: 0.6621 - learning_rate: 2.5000e-04\n",
            "Epoch 37/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.5638 - loss: 0.6714 - val_accuracy: 0.6154 - val_loss: 0.6615 - learning_rate: 2.5000e-04\n",
            "Epoch 38/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5612 - loss: 0.6679 - val_accuracy: 0.6154 - val_loss: 0.6618 - learning_rate: 2.5000e-04\n",
            "Epoch 39/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5564 - loss: 0.6701 - val_accuracy: 0.6154 - val_loss: 0.6610 - learning_rate: 2.5000e-04\n",
            "Epoch 40/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5784 - loss: 0.6689 - val_accuracy: 0.6154 - val_loss: 0.6606 - learning_rate: 2.5000e-04\n",
            "Epoch 41/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5719 - loss: 0.6691 - val_accuracy: 0.6538 - val_loss: 0.6602 - learning_rate: 2.5000e-04\n",
            "Epoch 42/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5613 - loss: 0.6684 - val_accuracy: 0.6538 - val_loss: 0.6597 - learning_rate: 2.5000e-04\n",
            "Epoch 43/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5707 - loss: 0.6710 - val_accuracy: 0.6538 - val_loss: 0.6602 - learning_rate: 2.5000e-04\n",
            "Epoch 44/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5656 - loss: 0.6759 - val_accuracy: 0.6154 - val_loss: 0.6608 - learning_rate: 2.5000e-04\n",
            "Epoch 45/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5737 - loss: 0.6705 - val_accuracy: 0.6538 - val_loss: 0.6605 - learning_rate: 2.5000e-04\n",
            "Epoch 46/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5573 - loss: 0.6714 - val_accuracy: 0.6538 - val_loss: 0.6600 - learning_rate: 2.5000e-04\n",
            "Epoch 47/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5648 - loss: 0.6696 - val_accuracy: 0.6538 - val_loss: 0.6599 - learning_rate: 2.5000e-04\n",
            "Epoch 48/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5671 - loss: 0.6712 - val_accuracy: 0.6538 - val_loss: 0.6598 - learning_rate: 1.2500e-04\n",
            "Epoch 49/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5691 - loss: 0.6678 - val_accuracy: 0.6538 - val_loss: 0.6591 - learning_rate: 1.2500e-04\n",
            "Epoch 50/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5857 - loss: 0.6686 - val_accuracy: 0.6538 - val_loss: 0.6587 - learning_rate: 1.2500e-04\n",
            "Epoch 51/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5771 - loss: 0.6665 - val_accuracy: 0.6538 - val_loss: 0.6581 - learning_rate: 1.2500e-04\n",
            "Epoch 52/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5710 - loss: 0.6681 - val_accuracy: 0.6538 - val_loss: 0.6579 - learning_rate: 1.2500e-04\n",
            "Epoch 53/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5616 - loss: 0.6720 - val_accuracy: 0.6538 - val_loss: 0.6581 - learning_rate: 1.2500e-04\n",
            "Epoch 54/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5621 - loss: 0.6663 - val_accuracy: 0.6538 - val_loss: 0.6579 - learning_rate: 1.2500e-04\n",
            "Epoch 55/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5609 - loss: 0.6688 - val_accuracy: 0.6538 - val_loss: 0.6574 - learning_rate: 1.2500e-04\n",
            "Epoch 56/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5698 - loss: 0.6722 - val_accuracy: 0.6538 - val_loss: 0.6574 - learning_rate: 1.2500e-04\n",
            "Epoch 57/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5806 - loss: 0.6697 - val_accuracy: 0.6538 - val_loss: 0.6572 - learning_rate: 1.2500e-04\n",
            "Epoch 58/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5894 - loss: 0.6658 - val_accuracy: 0.6538 - val_loss: 0.6572 - learning_rate: 1.2500e-04\n",
            "Epoch 59/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5561 - loss: 0.6724 - val_accuracy: 0.6538 - val_loss: 0.6578 - learning_rate: 1.2500e-04\n",
            "Epoch 60/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.5667 - loss: 0.6710 - val_accuracy: 0.6538 - val_loss: 0.6582 - learning_rate: 1.2500e-04\n",
            "Epoch 61/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5721 - loss: 0.6682 - val_accuracy: 0.6538 - val_loss: 0.6580 - learning_rate: 1.2500e-04\n",
            "Epoch 62/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5629 - loss: 0.6704 - val_accuracy: 0.6538 - val_loss: 0.6575 - learning_rate: 1.2500e-04\n",
            "Epoch 63/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5579 - loss: 0.6687 - val_accuracy: 0.6538 - val_loss: 0.6573 - learning_rate: 6.2500e-05\n",
            "Epoch 64/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5541 - loss: 0.6711 - val_accuracy: 0.6538 - val_loss: 0.6572 - learning_rate: 6.2500e-05\n",
            "Epoch 65/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5742 - loss: 0.6669 - val_accuracy: 0.6538 - val_loss: 0.6572 - learning_rate: 6.2500e-05\n",
            "Epoch 66/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5760 - loss: 0.6694 - val_accuracy: 0.6538 - val_loss: 0.6573 - learning_rate: 6.2500e-05\n",
            "Epoch 67/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5696 - loss: 0.6700 - val_accuracy: 0.6538 - val_loss: 0.6571 - learning_rate: 6.2500e-05\n",
            "Epoch 68/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5645 - loss: 0.6678 - val_accuracy: 0.6538 - val_loss: 0.6572 - learning_rate: 3.1250e-05\n",
            "Epoch 69/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5570 - loss: 0.6691 - val_accuracy: 0.6538 - val_loss: 0.6573 - learning_rate: 3.1250e-05\n",
            "Epoch 70/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5778 - loss: 0.6663 - val_accuracy: 0.6538 - val_loss: 0.6571 - learning_rate: 3.1250e-05\n",
            "Epoch 71/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5851 - loss: 0.6663 - val_accuracy: 0.6538 - val_loss: 0.6569 - learning_rate: 3.1250e-05\n",
            "Epoch 72/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5762 - loss: 0.6675 - val_accuracy: 0.6538 - val_loss: 0.6568 - learning_rate: 3.1250e-05\n",
            "Epoch 73/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5741 - loss: 0.6690 - val_accuracy: 0.6538 - val_loss: 0.6568 - learning_rate: 3.1250e-05\n",
            "Epoch 74/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5721 - loss: 0.6683 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 3.1250e-05\n",
            "Epoch 75/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5715 - loss: 0.6696 - val_accuracy: 0.6538 - val_loss: 0.6571 - learning_rate: 3.1250e-05\n",
            "Epoch 76/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5817 - loss: 0.6679 - val_accuracy: 0.6538 - val_loss: 0.6571 - learning_rate: 3.1250e-05\n",
            "Epoch 77/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5870 - loss: 0.6688 - val_accuracy: 0.6538 - val_loss: 0.6571 - learning_rate: 3.1250e-05\n",
            "Epoch 78/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5786 - loss: 0.6677 - val_accuracy: 0.6538 - val_loss: 0.6569 - learning_rate: 1.5625e-05\n",
            "Epoch 79/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5795 - loss: 0.6694 - val_accuracy: 0.6538 - val_loss: 0.6569 - learning_rate: 1.5625e-05\n",
            "Epoch 80/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5691 - loss: 0.6676 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 1.5625e-05\n",
            "Epoch 81/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5822 - loss: 0.6662 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 1.5625e-05\n",
            "Epoch 82/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5526 - loss: 0.6705 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 1.5625e-05\n",
            "Epoch 83/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5585 - loss: 0.6661 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 7.8125e-06\n",
            "Epoch 84/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5792 - loss: 0.6660 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 7.8125e-06\n",
            "Epoch 85/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.5705 - loss: 0.6686 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 7.8125e-06\n",
            "Epoch 86/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5724 - loss: 0.6643 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 7.8125e-06\n",
            "Epoch 87/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5584 - loss: 0.6687 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 7.8125e-06\n",
            "Epoch 88/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5812 - loss: 0.6666 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 3.9063e-06\n",
            "Epoch 89/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5800 - loss: 0.6661 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 3.9063e-06\n",
            "Epoch 90/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5585 - loss: 0.6684 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 3.9063e-06\n",
            "Epoch 91/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5741 - loss: 0.6671 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 3.9063e-06\n",
            "Epoch 92/500\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5668 - loss: 0.6691 - val_accuracy: 0.6538 - val_loss: 0.6570 - learning_rate: 3.9063e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL (tuned): {'acc': 0.6923076923076923, 'f1_macro': 0.6904761904761905, 'roc_auc': 0.6190476190476191, 'rmse_prob': 0.48163555897165033, 'rmse_hard': 0.5547001962252291, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.667     0.667     0.667        12\\n           1      0.714     0.714     0.714        14\\n\\n    accuracy                          0.692        26\\n   macro avg      0.690     0.690     0.690        26\\nweighted avg      0.692     0.692     0.692        26\\n', 'confusion_matrix': [[8, 4], [4, 10]]}\n",
            "VAL (0.5):   {'acc': 0.6538461538461539, 'f1_macro': 0.6067226890756303, 'roc_auc': 0.6190476190476191, 'rmse_prob': 0.48163555897165033, 'rmse_hard': 0.5883484054145521, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.800     0.333     0.471        12\\n           1      0.619     0.929     0.743        14\\n\\n    accuracy                          0.654        26\\n   macro avg      0.710     0.631     0.607        26\\nweighted avg      0.703     0.654     0.617        26\\n', 'confusion_matrix': [[4, 8], [1, 13]]}\n",
            "TEST:        {'acc': 0.4959349593495935, 'f1_macro': 0.33152173913043476, 'roc_auc': 0.48989364827545684, 'rmse_prob': 0.500635264180198, 'rmse_hard': 0.7099753803128714, 'classification_report': '              precision    recall  f1-score   support\\n\\n           0      0.496     1.000     0.663       183\\n           1      0.000     0.000     0.000       186\\n\\n    accuracy                          0.496       369\\n   macro avg      0.248     0.500     0.332       369\\nweighted avg      0.246     0.496     0.329       369\\n', 'confusion_matrix': [[183, 0], [186, 0]]}\n",
            "Threshold:   0.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LSTM feature-subset search (single hold-out; pick subset by LOG LOSS)\n",
        "# - Enumerates ALL non-empty feature combinations from CANDIDATE_FEATURES\n",
        "# - Trains on Train, validates on Val (choose subset with lowest log loss)\n",
        "# - Optionally retrains on Train+Val and reports Test log loss for top-K subsets\n",
        "#\n",
        "# Assumes:\n",
        "#   - df already has a binary 'target' column\n",
        "#   - df has a datetime index (or set df.index = pd.to_datetime(df[\"date\"]))\n",
        "# ============================================================\n",
        "\n",
        "import itertools, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, f1_score\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ---------- basic helpers ----------\n",
        "def make_static_splits(df: pd.DataFrame, test_start: str|pd.Timestamp):\n",
        "    m = df.index >= pd.to_datetime(test_start)\n",
        "    return df.loc[~m].copy(), df.loc[m].copy()\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
        "    return float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "\n",
        "def _class_weight_from_labels(y):\n",
        "    y = np.asarray(y, int)\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    K = len(classes); total = counts.sum()\n",
        "    return {int(c): float(total/(K*cnt)) for c, cnt in zip(classes, counts)}\n",
        "\n",
        "def best_threshold(y_true, proba, metric=\"f1_macro\"):\n",
        "    # not used for log loss selection, but handy to keep\n",
        "    grid = np.linspace(0.05, 0.95, 181)\n",
        "    best_t, best = 0.5, -1.0\n",
        "    for t in grid:\n",
        "        pred = (proba >= t).astype(int)\n",
        "        score = f1_score(y_true, pred, average=\"macro\") if metric==\"f1_macro\" else f1_score(y_true, pred)\n",
        "        if score > best:\n",
        "            best, best_t = score, t\n",
        "    return float(best_t)\n",
        "\n",
        "# ---------- sequences & model ----------\n",
        "def make_sequences(df: pd.DataFrame, lookback: int, feature_cols, target_col=\"target\"):\n",
        "    X, y = [], []\n",
        "    V = df[feature_cols].values\n",
        "    t = df[target_col].values\n",
        "    for i in range(lookback, len(df)):\n",
        "        X.append(V[i-lookback:i])\n",
        "        y.append(t[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_lstm(input_dim, lookback, units=64, dropout=0.2, lr=1e-3):\n",
        "    m = keras.Sequential([\n",
        "        layers.Input(shape=(lookback, input_dim)),\n",
        "        layers.LSTM(units, return_sequences=False),\n",
        "        layers.Dropout(dropout),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    m.compile(optimizer=keras.optimizers.Adam(lr),\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "    return m\n",
        "\n",
        "# ---------- scaling utilities ----------\n",
        "def _split_scale_cols(feature_cols, keep_prefixes=(\"p_state\",)):\n",
        "    to_scale = [c for c in feature_cols if not any(c.startswith(p) for p in keep_prefixes)]\n",
        "    return to_scale\n",
        "\n",
        "def _fit_transform(train_df, other_df, feature_cols, keep_prefixes=(\"p_state\",)):\n",
        "    to_scale = _split_scale_cols(feature_cols, keep_prefixes)\n",
        "    scaler = StandardScaler().fit(train_df[to_scale]) if len(to_scale) else None\n",
        "    tr = train_df.copy(); ot = other_df.copy()\n",
        "    if scaler is not None:\n",
        "        tr[to_scale] = scaler.transform(tr[to_scale])\n",
        "        ot[to_scale] = scaler.transform(ot[to_scale])\n",
        "    return tr, ot, scaler, to_scale\n",
        "\n",
        "def _transform_with(scaler, df, to_scale):\n",
        "    if scaler is None or not to_scale:\n",
        "        return df.copy()\n",
        "    out = df.copy()\n",
        "    out[to_scale] = scaler.transform(out[to_scale])\n",
        "    return out\n",
        "def safe_logloss(y_true, proba):\n",
        "    \"\"\"Version-agnostic log loss: clips probs and always passes labels=[0,1].\"\"\"\n",
        "    p = np.clip(np.asarray(proba, dtype=float), 1e-7, 1-1e-7)\n",
        "    y = np.asarray(y_true, dtype=int)\n",
        "    return float(log_loss(y, p, labels=[0, 1]))\n",
        "\n",
        "# ---------- core training on a given subset ----------\n",
        "def _train_eval_subset(train_df, val_df, feature_cols, target_col=\"target\",\n",
        "                       lookback=26, units=64, dropout=0.2, lr=1e-3,\n",
        "                       patience=5, batch_size=64, epochs=50,\n",
        "                       class_weight_mode=\"balanced\", keep_prefixes=(\"p_state\",)):\n",
        "    # clean\n",
        "    cols_needed = feature_cols + [target_col]\n",
        "    train_df = train_df.dropna(subset=cols_needed).copy()\n",
        "    val_df   = val_df.dropna(subset=cols_needed).copy()\n",
        "    if train_df.empty or val_df.empty:\n",
        "        return None  # not enough data\n",
        "\n",
        "    # scale (price/SPI by default; keep p_state* raw)\n",
        "    tr, va, scaler, to_scale = _fit_transform(train_df, val_df, feature_cols, keep_prefixes)\n",
        "\n",
        "    # sequences\n",
        "    X_tr, y_tr = make_sequences(tr, lookback, feature_cols, target_col)\n",
        "    X_va, y_va = make_sequences(va, lookback, feature_cols, target_col)\n",
        "    if len(X_tr) == 0 or len(X_va) == 0:\n",
        "        return None\n",
        "\n",
        "    # class weights\n",
        "    cw = None\n",
        "    if class_weight_mode == \"balanced\":\n",
        "        cw = _class_weight_from_labels(y_tr)\n",
        "    elif isinstance(class_weight_mode, dict):\n",
        "        cw = class_weight_mode\n",
        "\n",
        "    # model\n",
        "    model = build_lstm(input_dim=X_tr.shape[2], lookback=lookback, units=units, dropout=dropout, lr=lr)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_data=(X_va, y_va),\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=cw\n",
        "    )\n",
        "    # validation probabilities/metrics\n",
        "    proba_va = model.predict(X_va, verbose=0).ravel()\n",
        "    # robust log loss (uses labels=[0,1] even if one class absent)\n",
        "    val_logloss = safe_logloss(y_va, proba_va)\n",
        "    # some extra metrics for info\n",
        "    val_auc = float(roc_auc_score(y_va, proba_va)) if len(np.unique(y_va)) == 2 else np.nan\n",
        "    val_acc = float(accuracy_score(y_va, (proba_va >= 0.5).astype(int)))\n",
        "    val_f1  = float(f1_score(y_va, (proba_va >= 0.5).astype(int), average=\"macro\"))\n",
        "    val_rmse_prob = rmse(y_va, proba_va)\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"scaler\": scaler,\n",
        "        \"to_scale\": to_scale,\n",
        "        \"y_va\": y_va,\n",
        "        \"proba_va\": proba_va,\n",
        "        \"val_logloss\": val_logloss,\n",
        "        \"val_auc\": val_auc,\n",
        "        \"val_acc\": val_acc,\n",
        "        \"val_f1_macro\": val_f1,\n",
        "        \"val_rmse_prob\": val_rmse_prob\n",
        "    }\n",
        "\n",
        "# ---------- final retrain on Train+Val and test ----------\n",
        "def _retrain_and_test(df_trv, df_test, feature_cols, target_col=\"target\",\n",
        "                      lookback=26, units=64, dropout=0.2, lr=1e-3,\n",
        "                      patience=5, batch_size=64, epochs=50,\n",
        "                      class_weight_mode=\"balanced\", keep_prefixes=(\"p_state\",)):\n",
        "    # clean\n",
        "    cols_needed = feature_cols + [target_col]\n",
        "    df_trv  = df_trv.dropna(subset=cols_needed).copy()\n",
        "    df_test = df_test.dropna(subset=cols_needed).copy()\n",
        "    if df_trv.empty or df_test.empty:\n",
        "        return None\n",
        "\n",
        "    # scale on Train+Val\n",
        "    to_scale = _split_scale_cols(feature_cols, keep_prefixes)\n",
        "    scaler = StandardScaler().fit(df_trv[to_scale]) if len(to_scale) else None\n",
        "    trv  = _transform_with(scaler, df_trv, to_scale)\n",
        "    test = _transform_with(scaler, df_test, to_scale)\n",
        "\n",
        "    # sequences\n",
        "    X_trv, y_trv = make_sequences(trv,  lookback, feature_cols, target_col)\n",
        "    X_te,  y_te  = make_sequences(test, lookback, feature_cols, target_col)\n",
        "    if len(X_trv) == 0 or len(X_te) == 0:\n",
        "        return None\n",
        "\n",
        "    # small internal val from trv tail for early stopping\n",
        "    monitor_k = max(min(max(len(X_trv)//10, 32), 256), 16)\n",
        "    monitor_k = min(monitor_k, len(X_trv)//5) if len(X_trv) >= 5 else 0\n",
        "    if monitor_k > 0:\n",
        "        X_tr, y_tr = X_trv[:-monitor_k], y_trv[:-monitor_k]\n",
        "        X_vm, y_vm = X_trv[-monitor_k:], y_trv[-monitor_k:]\n",
        "        val_monitor = (X_vm, y_vm)\n",
        "    else:\n",
        "        X_tr, y_tr = X_trv, y_trv\n",
        "        val_monitor = None\n",
        "\n",
        "    cw = None\n",
        "    if class_weight_mode == \"balanced\":\n",
        "        cw = _class_weight_from_labels(y_tr)\n",
        "    elif isinstance(class_weight_mode, dict):\n",
        "        cw = class_weight_mode\n",
        "\n",
        "    # model\n",
        "    model = build_lstm(input_dim=X_trv.shape[2], lookback=lookback, units=units, dropout=dropout, lr=lr)\n",
        "    es = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True, monitor=\"val_loss\")\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=epochs, batch_size=64,\n",
        "        validation_data=val_monitor,\n",
        "        callbacks=[es], verbose=0,\n",
        "        class_weight=cw\n",
        "    )\n",
        "\n",
        "    proba_te = model.predict(X_te, verbose=0).ravel()\n",
        "    test_logloss = safe_logloss(y_te, proba_te)\n",
        "    test_auc = float(roc_auc_score(y_te, proba_te)) if len(np.unique(y_te)) == 2 else np.nan\n",
        "    test_acc = float(accuracy_score(y_te, (proba_te >= 0.5).astype(int)))\n",
        "    test_f1  = float(f1_score(y_te, (proba_te >= 0.5).astype(int), average=\"macro\"))\n",
        "    test_rmse_prob = rmse(y_te, proba_te)\n",
        "\n",
        "    return {\n",
        "        \"test_logloss\": test_logloss,\n",
        "        \"test_auc\": test_auc,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"test_f1_macro\": test_f1,\n",
        "        \"test_rmse_prob\": test_rmse_prob\n",
        "    }\n",
        "\n",
        "# ---------- search over ALL feature subsets ----------\n",
        "def grid_search_feature_subsets_lstm_holdout(\n",
        "    df: pd.DataFrame,\n",
        "    test_start: str|pd.Timestamp,\n",
        "    CANDIDATE_FEATURES: list[str] | None = None,\n",
        "    target_col: str = \"target\",\n",
        "    lookback: int = 26,\n",
        "    units: int = 64,\n",
        "    dropout: float = 0.2,\n",
        "    lr: float = 1e-3,\n",
        "    patience: int = 5,\n",
        "    batch_size: int = 64,\n",
        "    epochs: int = 50,\n",
        "    class_weight_mode: str | dict | None = \"balanced\",\n",
        "    keep_prefixes: tuple[str,...] = (\"p_state\",),\n",
        "    max_subsets: int | None = None,      # set to e.g. 2000 to cap runtime\n",
        "    top_k_test: int = 5,                 # retrain+test only for top-K by val logloss\n",
        "    random_seed: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict with:\n",
        "      - 'ranked': DataFrame of all tried subsets sorted by val_logloss\n",
        "      - 'best_subset': best feature list (by val_logloss)\n",
        "      - 'best_val': metrics dict for the best subset\n",
        "      - 'topk_test': list of dicts with test metrics for top-K subsets\n",
        "    \"\"\"\n",
        "    df = df.sort_index().copy()\n",
        "    trainval, test = make_static_splits(df, test_start)\n",
        "\n",
        "    # candidate features (auto-detect numeric columns if not provided)\n",
        "    if CANDIDATE_FEATURES is None:\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        CANDIDATE_FEATURES = [c for c in numeric_cols if c != target_col]\n",
        "\n",
        "    # enumerate ALL non-empty subsets\n",
        "    all_subsets = []\n",
        "    for r in range(1, len(CANDIDATE_FEATURES) + 1):\n",
        "        for comb in itertools.combinations(CANDIDATE_FEATURES, r):\n",
        "            all_subsets.append(list(comb))\n",
        "\n",
        "    # optional cap to avoid explosion\n",
        "    random.seed(random_seed)\n",
        "    if max_subsets is not None and len(all_subsets) > max_subsets:\n",
        "        all_subsets = random.sample(all_subsets, max_subsets)\n",
        "\n",
        "    rows = []\n",
        "    cache_best = {\"subset\": None, \"val\": None}\n",
        "\n",
        "    for subset in all_subsets:\n",
        "        res = _train_eval_subset(\n",
        "            trainval.iloc[:-min(52, max(26, len(trainval)//10))],   # Train\n",
        "            trainval.iloc[-min(52, max(26, len(trainval)//10)):],   # Val\n",
        "            feature_cols=subset,\n",
        "            target_col=target_col,\n",
        "            lookback=lookback, units=units, dropout=dropout, lr=lr,\n",
        "            patience=patience, batch_size=batch_size, epochs=epochs,\n",
        "            class_weight_mode=class_weight_mode, keep_prefixes=keep_prefixes\n",
        "        )\n",
        "        if res is None:\n",
        "            continue\n",
        "\n",
        "        row = {\n",
        "            \"subset\": tuple(subset),\n",
        "            \"k\": len(subset),\n",
        "            \"val_logloss\": res[\"val_logloss\"],\n",
        "            \"val_auc\": res[\"val_auc\"],\n",
        "            \"val_acc\": res[\"val_acc\"],\n",
        "            \"val_f1_macro\": res[\"val_f1_macro\"],\n",
        "            \"val_rmse_prob\": res[\"val_rmse_prob\"]\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "        if (cache_best[\"val\"] is None) or (row[\"val_logloss\"] < cache_best[\"val\"][\"val_logloss\"]):\n",
        "            cache_best[\"subset\"] = subset\n",
        "            cache_best[\"val\"] = row\n",
        "\n",
        "    if not rows:\n",
        "        raise RuntimeError(\"No valid subsets could be trained (not enough data per subset/lookback).\")\n",
        "\n",
        "    ranked = pd.DataFrame(rows).sort_values(\"val_logloss\").reset_index(drop=True)\n",
        "    best_subset = list(ranked.loc[0, \"subset\"])\n",
        "    best_val_row = ranked.loc[0].to_dict()\n",
        "\n",
        "    # retrain on Train+Val and evaluate on Test for top-K subsets\n",
        "    topk = min(top_k_test, len(ranked))\n",
        "    topk_test = []\n",
        "    trv = trainval  # full Train+Val\n",
        "\n",
        "    for i in range(topk):\n",
        "        subset_i = list(ranked.loc[i, \"subset\"])\n",
        "        test_res = _retrain_and_test(\n",
        "            trv, test,\n",
        "            feature_cols=subset_i, target_col=target_col,\n",
        "            lookback=lookback, units=units, dropout=dropout, lr=lr,\n",
        "            patience=patience, batch_size=batch_size, epochs=epochs,\n",
        "            class_weight_mode=class_weight_mode, keep_prefixes=keep_prefixes\n",
        "        )\n",
        "        if test_res is not None:\n",
        "            topk_test.append({\"subset\": tuple(subset_i), **test_res})\n",
        "\n",
        "    return {\n",
        "        \"ranked\": ranked,\n",
        "        \"best_subset\": best_subset,\n",
        "        \"best_val\": best_val_row,\n",
        "        \"topk_test\": topk_test\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# Example usage (uncomment)\n",
        "# ----------------------------\n",
        "data.index = pd.to_datetime(data[\"date\"])\n",
        "data = data.sort_index()\n",
        "#\n",
        "# # Define candidate features explicitly (recommended)\n",
        "CANDIDATE_FEATURES = [\"Last Price\",\"top_state\",\"spi_6\",\n",
        "                      \"ret_1\"]\n",
        "#\n",
        "out = grid_search_feature_subsets_lstm_holdout(\n",
        "    data,\n",
        "    test_start=\"2018-01-01\",\n",
        "    CANDIDATE_FEATURES=CANDIDATE_FEATURES,\n",
        "    target_col=\"target\",\n",
        "    lookback=26,\n",
        "    units=64, dropout=0.2, lr=1e-3,\n",
        "    patience=5, batch_size=64, epochs=50,\n",
        "    class_weight_mode=\"balanced\",\n",
        "    keep_prefixes=(\"p_state\",),   # keep p_state* unscaled\n",
        "    max_subsets=1024,             # cap if feature count is large\n",
        "    top_k_test=5\n",
        ")\n",
        "#\n",
        "print(\"Best subset by VAL logloss:\", out[\"best_subset\"])\n",
        "print(\"Best VAL row:\", out[\"best_val\"])\n",
        "print(\"Top-K TEST results:\")\n",
        "for r in out[\"topk_test\"]:\n",
        "    print(r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLzTAhJy94Ch",
        "outputId": "71e7e1ad-be38-4ee6-f8c8-5fee5259ba06"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best subset by VAL logloss: ['spi_6']\n",
            "Best VAL row: {'subset': ('spi_6',), 'k': 1, 'val_logloss': 0.6671536790350419, 'val_auc': 0.625, 'val_acc': 0.6538461538461539, 'val_f1_macro': 0.6067226890756303, 'val_rmse_prob': 0.48685812486001484}\n",
            "Top-K TEST results:\n",
            "{'subset': ('spi_6',), 'test_logloss': 0.6925327106857391, 'test_auc': 0.5387801868499912, 'test_acc': 0.5447154471544715, 'test_f1_macro': 0.5432360742705571, 'test_rmse_prob': 0.4996381821083819}\n",
            "{'subset': ('Last Price', 'top_state'), 'test_logloss': 0.6919179675906035, 'test_auc': 0.5390445972148775, 'test_acc': 0.5447154471544715, 'test_f1_macro': 0.5422647527910687, 'test_rmse_prob': 0.49938506581254377}\n",
            "{'subset': ('top_state', 'spi_6'), 'test_logloss': 0.6921368318806327, 'test_auc': 0.5294670662201069, 'test_acc': 0.5338753387533876, 'test_f1_macro': 0.5323607427055703, 'test_rmse_prob': 0.4994946195616021}\n",
            "{'subset': ('Last Price', 'top_state', 'spi_6'), 'test_logloss': 0.6932942997503005, 'test_auc': 0.5208002820377226, 'test_acc': 0.5121951219512195, 'test_f1_macro': 0.4984898822108125, 'test_rmse_prob': 0.5000706853622587}\n",
            "{'subset': ('top_state',), 'test_logloss': 0.691634567527391, 'test_auc': 0.5417621481873202, 'test_acc': 0.5474254742547425, 'test_f1_macro': 0.5444141326768248, 'test_rmse_prob': 0.49924330460841393}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "rjSjCCEQ3n9E",
        "outputId": "0c377528-82ca-4347-bc7f-ac7f93f9e6f0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  date     spi_6  p_state0  p_state1  p_state2  Last Price  \\\n",
              "date                                                                         \n",
              "1992-01-12  1992-01-12 -0.676211  0.188472  0.778493  0.033035        2.43   \n",
              "1992-01-19  1992-01-19 -0.408163  0.042357  0.918067  0.039576        2.48   \n",
              "1992-01-26  1992-01-26 -0.149762  0.031910  0.927547  0.040544        2.55   \n",
              "1992-02-02  1992-02-02 -0.012210  0.031731  0.926709  0.041559        2.57   \n",
              "1992-02-09  1992-02-09 -0.154111  0.031869  0.927580  0.040551        2.57   \n",
              "...                ...       ...       ...       ...       ...         ...   \n",
              "2025-06-29  2025-06-29 -0.317970  0.047144  0.919293  0.033563        4.00   \n",
              "2025-07-06  2025-07-06 -0.346593  0.053727  0.912975  0.033298        4.10   \n",
              "2025-07-13  2025-07-13 -0.113659  0.039652  0.926234  0.034114        3.87   \n",
              "2025-07-20  2025-07-20  0.110532  0.035815  0.928811  0.035374        3.95   \n",
              "2025-07-27  2025-07-27  0.294891  0.035153  0.926273  0.038575        3.87   \n",
              "\n",
              "           price_date_used  fallback_days     ret_1  top_state  next_ret  \\\n",
              "date                                                                       \n",
              "1992-01-12      1992-01-10            2.0  0.016736          1  0.020576   \n",
              "1992-01-19      1992-01-17            2.0  0.020576          1  0.028226   \n",
              "1992-01-26      1992-01-24            2.0  0.028226          1  0.007843   \n",
              "1992-02-02      1992-01-31            2.0  0.007843          1  0.000000   \n",
              "1992-02-09      1992-02-07            2.0  0.000000          1 -0.007782   \n",
              "...                    ...            ...       ...        ...       ...   \n",
              "2025-06-29      2025-06-27            2.0 -0.014778          1  0.025000   \n",
              "2025-07-06      2025-07-03            3.0  0.025000          1 -0.056098   \n",
              "2025-07-13      2025-07-11            2.0 -0.056098          1  0.020672   \n",
              "2025-07-20      2025-07-18            2.0  0.020672          1 -0.020253   \n",
              "2025-07-27      2025-07-25            2.0 -0.020253          1       NaN   \n",
              "\n",
              "            target  \n",
              "date                \n",
              "1992-01-12       1  \n",
              "1992-01-19       1  \n",
              "1992-01-26       1  \n",
              "1992-02-02       0  \n",
              "1992-02-09       0  \n",
              "...            ...  \n",
              "2025-06-29       1  \n",
              "2025-07-06       0  \n",
              "2025-07-13       1  \n",
              "2025-07-20       0  \n",
              "2025-07-27       0  \n",
              "\n",
              "[1749 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52165487-b187-49bf-928c-cbb2a525f07d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>spi_6</th>\n",
              "      <th>p_state0</th>\n",
              "      <th>p_state1</th>\n",
              "      <th>p_state2</th>\n",
              "      <th>Last Price</th>\n",
              "      <th>price_date_used</th>\n",
              "      <th>fallback_days</th>\n",
              "      <th>ret_1</th>\n",
              "      <th>top_state</th>\n",
              "      <th>next_ret</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1992-01-12</th>\n",
              "      <td>1992-01-12</td>\n",
              "      <td>-0.676211</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.778493</td>\n",
              "      <td>0.033035</td>\n",
              "      <td>2.43</td>\n",
              "      <td>1992-01-10</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.016736</td>\n",
              "      <td>1</td>\n",
              "      <td>0.020576</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992-01-19</th>\n",
              "      <td>1992-01-19</td>\n",
              "      <td>-0.408163</td>\n",
              "      <td>0.042357</td>\n",
              "      <td>0.918067</td>\n",
              "      <td>0.039576</td>\n",
              "      <td>2.48</td>\n",
              "      <td>1992-01-17</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.020576</td>\n",
              "      <td>1</td>\n",
              "      <td>0.028226</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992-01-26</th>\n",
              "      <td>1992-01-26</td>\n",
              "      <td>-0.149762</td>\n",
              "      <td>0.031910</td>\n",
              "      <td>0.927547</td>\n",
              "      <td>0.040544</td>\n",
              "      <td>2.55</td>\n",
              "      <td>1992-01-24</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.028226</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992-02-02</th>\n",
              "      <td>1992-02-02</td>\n",
              "      <td>-0.012210</td>\n",
              "      <td>0.031731</td>\n",
              "      <td>0.926709</td>\n",
              "      <td>0.041559</td>\n",
              "      <td>2.57</td>\n",
              "      <td>1992-01-31</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992-02-09</th>\n",
              "      <td>1992-02-09</td>\n",
              "      <td>-0.154111</td>\n",
              "      <td>0.031869</td>\n",
              "      <td>0.927580</td>\n",
              "      <td>0.040551</td>\n",
              "      <td>2.57</td>\n",
              "      <td>1992-02-07</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.007782</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-06-29</th>\n",
              "      <td>2025-06-29</td>\n",
              "      <td>-0.317970</td>\n",
              "      <td>0.047144</td>\n",
              "      <td>0.919293</td>\n",
              "      <td>0.033563</td>\n",
              "      <td>4.00</td>\n",
              "      <td>2025-06-27</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.014778</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-06</th>\n",
              "      <td>2025-07-06</td>\n",
              "      <td>-0.346593</td>\n",
              "      <td>0.053727</td>\n",
              "      <td>0.912975</td>\n",
              "      <td>0.033298</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2025-07-03</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.056098</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-13</th>\n",
              "      <td>2025-07-13</td>\n",
              "      <td>-0.113659</td>\n",
              "      <td>0.039652</td>\n",
              "      <td>0.926234</td>\n",
              "      <td>0.034114</td>\n",
              "      <td>3.87</td>\n",
              "      <td>2025-07-11</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.056098</td>\n",
              "      <td>1</td>\n",
              "      <td>0.020672</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-20</th>\n",
              "      <td>2025-07-20</td>\n",
              "      <td>0.110532</td>\n",
              "      <td>0.035815</td>\n",
              "      <td>0.928811</td>\n",
              "      <td>0.035374</td>\n",
              "      <td>3.95</td>\n",
              "      <td>2025-07-18</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.020672</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.020253</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-07-27</th>\n",
              "      <td>2025-07-27</td>\n",
              "      <td>0.294891</td>\n",
              "      <td>0.035153</td>\n",
              "      <td>0.926273</td>\n",
              "      <td>0.038575</td>\n",
              "      <td>3.87</td>\n",
              "      <td>2025-07-25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.020253</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1749 rows × 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52165487-b187-49bf-928c-cbb2a525f07d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-52165487-b187-49bf-928c-cbb2a525f07d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-52165487-b187-49bf-928c-cbb2a525f07d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dadc953f-8b49-4a9b-916a-79f89bf81569\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dadc953f-8b49-4a9b-916a-79f89bf81569')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dadc953f-8b49-4a9b-916a-79f89bf81569 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_2d447f8b-2bd2-4bfc-a15c-8c3b01adb344\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2d447f8b-2bd2-4bfc-a15c-8c3b01adb344 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "repr_error": "cannot insert date, already exists"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}